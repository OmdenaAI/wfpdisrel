{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example Neural Network - Multioutput Linear Regression-v1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CQXAD8TR7Gj",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFzBrCZiRndp",
        "colab_type": "text"
      },
      "source": [
        "Example of Multi Target Regression using Neural Network Regression\n",
        "\n",
        "      @Sijuade Oguntayo\n",
        "\n",
        "\n",
        "Copied from *@XavierTorres* - \n",
        "\n",
        "In this example we are trying to predict 2 continuous variables with 4 input features. Here we are not doing data cleaning, neither EDA, feature engineering nor fine-tuning model features. It's only a preliminary model overview.\n",
        "\n",
        "- Inputs: \n",
        "    - Land_sqf\n",
        "    - Gross_sqf\n",
        "    - Year_built\n",
        "    - NBH_level\t(Neighborhood level: 10= most upper class; 1: most lower class)\n",
        "    \n",
        "- Outputs: \n",
        "    - SALE_PRICE \t\n",
        "    - YEARLY_RENT\n",
        "\n",
        "We are going to try different approaches to predict each output (possible SALE_PRICE and possible YEARLY_RENT for a new property).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMSAyrl7RioM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed-iOgYySA1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('nyc-rolling-sales-clean-1.csv', sep=';')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqNuQsHuSCq8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3ee3ad46-786c-4fa3-9fdf-45dfc2702188"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Land_sqf</th>\n",
              "      <th>Gross_sqf</th>\n",
              "      <th>Year_built</th>\n",
              "      <th>NBH_level</th>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <th>YEARLY_RENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2400</td>\n",
              "      <td>1552</td>\n",
              "      <td>1930</td>\n",
              "      <td>2</td>\n",
              "      <td>220485</td>\n",
              "      <td>10900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2742</td>\n",
              "      <td>1207</td>\n",
              "      <td>1925</td>\n",
              "      <td>2</td>\n",
              "      <td>223372</td>\n",
              "      <td>8100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5610</td>\n",
              "      <td>1320</td>\n",
              "      <td>1910</td>\n",
              "      <td>5</td>\n",
              "      <td>362981</td>\n",
              "      <td>19000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1758</td>\n",
              "      <td>1537</td>\n",
              "      <td>1910</td>\n",
              "      <td>5</td>\n",
              "      <td>245135</td>\n",
              "      <td>9700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1317</td>\n",
              "      <td>1339</td>\n",
              "      <td>1920</td>\n",
              "      <td>4</td>\n",
              "      <td>216477</td>\n",
              "      <td>12800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Land_sqf  Gross_sqf  Year_built  NBH_level  SALE_PRICE  YEARLY_RENT\n",
              "0      2400       1552        1930          2      220485        10900\n",
              "1      2742       1207        1925          2      223372         8100\n",
              "2      5610       1320        1910          5      362981        19000\n",
              "3      1758       1537        1910          5      245135         9700\n",
              "4      1317       1339        1920          4      216477        12800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql0tatV_SEVE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "6f0e91ad-3e46-4b16-cc14-d03f81b7b2df"
      },
      "source": [
        "df.corr()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Land_sqf</th>\n",
              "      <th>Gross_sqf</th>\n",
              "      <th>Year_built</th>\n",
              "      <th>NBH_level</th>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <th>YEARLY_RENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Land_sqf</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.383712</td>\n",
              "      <td>-0.001142</td>\n",
              "      <td>0.030384</td>\n",
              "      <td>0.693293</td>\n",
              "      <td>0.534428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Gross_sqf</th>\n",
              "      <td>0.383712</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.061774</td>\n",
              "      <td>-0.025912</td>\n",
              "      <td>0.782791</td>\n",
              "      <td>0.536878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Year_built</th>\n",
              "      <td>-0.001142</td>\n",
              "      <td>-0.061774</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.030740</td>\n",
              "      <td>-0.004287</td>\n",
              "      <td>0.053032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NBH_level</th>\n",
              "      <td>0.030384</td>\n",
              "      <td>-0.025912</td>\n",
              "      <td>0.030740</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.426108</td>\n",
              "      <td>0.682104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <td>0.693293</td>\n",
              "      <td>0.782791</td>\n",
              "      <td>-0.004287</td>\n",
              "      <td>0.426108</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>YEARLY_RENT</th>\n",
              "      <td>0.534428</td>\n",
              "      <td>0.536878</td>\n",
              "      <td>0.053032</td>\n",
              "      <td>0.682104</td>\n",
              "      <td>0.864152</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Land_sqf  Gross_sqf  ...  SALE_PRICE  YEARLY_RENT\n",
              "Land_sqf     1.000000   0.383712  ...    0.693293     0.534428\n",
              "Gross_sqf    0.383712   1.000000  ...    0.782791     0.536878\n",
              "Year_built  -0.001142  -0.061774  ...   -0.004287     0.053032\n",
              "NBH_level    0.030384  -0.025912  ...    0.426108     0.682104\n",
              "SALE_PRICE   0.693293   0.782791  ...    1.000000     0.864152\n",
              "YEARLY_RENT  0.534428   0.536878  ...    0.864152     1.000000\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7gy7EVISGRT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "485a1973-1d71-42ba-a29a-a76d3913b1a9"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Land_sqf</th>\n",
              "      <th>Gross_sqf</th>\n",
              "      <th>Year_built</th>\n",
              "      <th>NBH_level</th>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <th>YEARLY_RENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>291.000000</td>\n",
              "      <td>291.000000</td>\n",
              "      <td>291.000000</td>\n",
              "      <td>291.000000</td>\n",
              "      <td>291.000000</td>\n",
              "      <td>291.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2645.676976</td>\n",
              "      <td>1639.773196</td>\n",
              "      <td>1938.378007</td>\n",
              "      <td>5.374570</td>\n",
              "      <td>287247.257732</td>\n",
              "      <td>18346.735395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1353.765596</td>\n",
              "      <td>810.233323</td>\n",
              "      <td>27.953929</td>\n",
              "      <td>2.892423</td>\n",
              "      <td>105082.283757</td>\n",
              "      <td>9435.111314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>353.000000</td>\n",
              "      <td>450.000000</td>\n",
              "      <td>1901.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>78522.000000</td>\n",
              "      <td>4300.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1900.000000</td>\n",
              "      <td>1114.000000</td>\n",
              "      <td>1920.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>216482.500000</td>\n",
              "      <td>10750.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2446.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1930.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>281208.000000</td>\n",
              "      <td>17300.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3000.000000</td>\n",
              "      <td>1960.000000</td>\n",
              "      <td>1950.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>332174.500000</td>\n",
              "      <td>24550.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>14384.000000</td>\n",
              "      <td>5303.000000</td>\n",
              "      <td>2015.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>961748.000000</td>\n",
              "      <td>81400.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Land_sqf    Gross_sqf  ...     SALE_PRICE   YEARLY_RENT\n",
              "count    291.000000   291.000000  ...     291.000000    291.000000\n",
              "mean    2645.676976  1639.773196  ...  287247.257732  18346.735395\n",
              "std     1353.765596   810.233323  ...  105082.283757   9435.111314\n",
              "min      353.000000   450.000000  ...   78522.000000   4300.000000\n",
              "25%     1900.000000  1114.000000  ...  216482.500000  10750.000000\n",
              "50%     2446.000000  1400.000000  ...  281208.000000  17300.000000\n",
              "75%     3000.000000  1960.000000  ...  332174.500000  24550.000000\n",
              "max    14384.000000  5303.000000  ...  961748.000000  81400.000000\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vTFjiG3SGTp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "18541ba0-ecc5-4b00-ff91-b458612e5f0d"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Land_sqf       int64\n",
              "Gross_sqf      int64\n",
              "Year_built     int64\n",
              "NBH_level      int64\n",
              "SALE_PRICE     int64\n",
              "YEARLY_RENT    int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evEoONQJSGaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.iloc[:,:4] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgPq5VSuSGfg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72dea830-5086-4be0-dd6c-c7383976e0ee"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(291, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t-pRoVbSLYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assigning last 4 columns to Y, with the 2 dependant variables or 'outputs' or 'targets'\n",
        "Y = df.iloc[:,4:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3ofxG7qSLdM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "81171f1d-3c28-4e0e-80a5-9882de0be89e"
      },
      "source": [
        "Y.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <th>YEARLY_RENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>220485</td>\n",
              "      <td>10900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>223372</td>\n",
              "      <td>8100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>362981</td>\n",
              "      <td>19000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>245135</td>\n",
              "      <td>9700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>216477</td>\n",
              "      <td>12800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SALE_PRICE  YEARLY_RENT\n",
              "0      220485        10900\n",
              "1      223372         8100\n",
              "2      362981        19000\n",
              "3      245135         9700\n",
              "4      216477        12800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GctuFnrMSLic",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "005ea8e4-20de-455b-f0e5-cfc5cf5f0780"
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(291, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkCxnczTSLrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split train and test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,\n",
        "                                                    random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ__qQsmSL0D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cfe50d6e-035b-49ed-f4a7-f335f4f5c093"
      },
      "source": [
        "! pip install --upgrade tensorflow"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 36kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.1)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (45.2.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow-addons~=0.7.0, but you'll have tensorflow-addons 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALRwU0aASL3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn5LPfF1SoTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Custom Loss funtion - R2\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def coeff_determination(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atVa_FuPSLzH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "9e19450f-b231-4700-8b14-af91ded1269d"
      },
      "source": [
        "# Model Architecture\n",
        "\n",
        "input_tensor = Input(shape=(4, ))\n",
        "hidden_layer_one = Dense(64)(input_tensor)\n",
        "hidden_layer_two = Dense(128)(hidden_layer_one)\n",
        "hidden_layer_three = Dense(256)(hidden_layer_two)\n",
        "hidden_layer_four = Dense(512)(hidden_layer_three)\n",
        "output = Dense(2)(hidden_layer_four)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUYXkqx8SLyL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "82dedc5d-a7b5-43cc-bbe2-b8915ba9bc92"
      },
      "source": [
        "model = Model(input_tensor, output)\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D6KAaxPSLwH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b37eb672-0b39-48c6-f850-9b5d4258a031"
      },
      "source": [
        "model.fit(X_train, Y_train, epochs=700)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/700\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "203/203 [==============================] - 0s 2ms/step - loss: 43972858410.8768\n",
            "Epoch 2/700\n",
            "203/203 [==============================] - 0s 167us/step - loss: 24284263320.5911\n",
            "Epoch 3/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 4928994029.0837\n",
            "Epoch 4/700\n",
            "203/203 [==============================] - 0s 166us/step - loss: 3914284835.3103\n",
            "Epoch 5/700\n",
            "203/203 [==============================] - 0s 161us/step - loss: 3337265366.3842\n",
            "Epoch 6/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 1790018581.7537\n",
            "Epoch 7/700\n",
            "203/203 [==============================] - 0s 165us/step - loss: 1968034662.7783\n",
            "Epoch 8/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 1562718987.9803\n",
            "Epoch 9/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 1428625054.8966\n",
            "Epoch 10/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 1303945639.4089\n",
            "Epoch 11/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 1231936911.7635\n",
            "Epoch 12/700\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1203153948.3744\n",
            "Epoch 13/700\n",
            "203/203 [==============================] - 0s 197us/step - loss: 1168889733.6749\n",
            "Epoch 14/700\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1137230846.1084\n",
            "Epoch 15/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1118612215.1724\n",
            "Epoch 16/700\n",
            "203/203 [==============================] - 0s 189us/step - loss: 1136110626.9951\n",
            "Epoch 17/700\n",
            "203/203 [==============================] - 0s 199us/step - loss: 1101263532.7685\n",
            "Epoch 18/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 1112250439.5665\n",
            "Epoch 19/700\n",
            "203/203 [==============================] - 0s 210us/step - loss: 1093134966.5419\n",
            "Epoch 20/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 1142516022.2266\n",
            "Epoch 21/700\n",
            "203/203 [==============================] - 0s 197us/step - loss: 1225705301.7537\n",
            "Epoch 22/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 1086616637.1626\n",
            "Epoch 23/700\n",
            "203/203 [==============================] - 0s 221us/step - loss: 1096655460.2562\n",
            "Epoch 24/700\n",
            "203/203 [==============================] - 0s 225us/step - loss: 1086963982.5025\n",
            "Epoch 25/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1082034550.5419\n",
            "Epoch 26/700\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1098770855.4089\n",
            "Epoch 27/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 1087684056.2759\n",
            "Epoch 28/700\n",
            "203/203 [==============================] - 0s 212us/step - loss: 1103390725.9901\n",
            "Epoch 29/700\n",
            "203/203 [==============================] - 0s 212us/step - loss: 1151308219.7438\n",
            "Epoch 30/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 1084034342.4631\n",
            "Epoch 31/700\n",
            "203/203 [==============================] - 0s 179us/step - loss: 1096651250.1281\n",
            "Epoch 32/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 1086637979.1133\n",
            "Epoch 33/700\n",
            "203/203 [==============================] - 0s 197us/step - loss: 1092739013.9901\n",
            "Epoch 34/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 1085900306.2857\n",
            "Epoch 35/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1131851460.7291\n",
            "Epoch 36/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 1111771731.8621\n",
            "Epoch 37/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 1094004271.6059\n",
            "Epoch 38/700\n",
            "203/203 [==============================] - 0s 198us/step - loss: 1090404295.8818\n",
            "Epoch 39/700\n",
            "203/203 [==============================] - 0s 207us/step - loss: 1127441421.0837\n",
            "Epoch 40/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 1087683967.3695\n",
            "Epoch 41/700\n",
            "203/203 [==============================] - 0s 200us/step - loss: 1093114794.4039\n",
            "Epoch 42/700\n",
            "203/203 [==============================] - 0s 200us/step - loss: 1114905349.0443\n",
            "Epoch 43/700\n",
            "203/203 [==============================] - 0s 191us/step - loss: 1209732376.5911\n",
            "Epoch 44/700\n",
            "203/203 [==============================] - 0s 191us/step - loss: 1178475584.3153\n",
            "Epoch 45/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 1194235535.1330\n",
            "Epoch 46/700\n",
            "203/203 [==============================] - 0s 213us/step - loss: 1097820185.5369\n",
            "Epoch 47/700\n",
            "203/203 [==============================] - 0s 200us/step - loss: 1075207726.0296\n",
            "Epoch 48/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 1128537671.8818\n",
            "Epoch 49/700\n",
            "203/203 [==============================] - 0s 204us/step - loss: 1160896850.2857\n",
            "Epoch 50/700\n",
            "203/203 [==============================] - 0s 191us/step - loss: 1130864736.1576\n",
            "Epoch 51/700\n",
            "203/203 [==============================] - 0s 191us/step - loss: 1107275333.0443\n",
            "Epoch 52/700\n",
            "203/203 [==============================] - 0s 216us/step - loss: 1136897284.4138\n",
            "Epoch 53/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 1107177189.8325\n",
            "Epoch 54/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 1070171053.3990\n",
            "Epoch 55/700\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1093844544.6305\n",
            "Epoch 56/700\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1074044493.8719\n",
            "Epoch 57/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1075560892.5320\n",
            "Epoch 58/700\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1075489989.3596\n",
            "Epoch 59/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 1088426717.9507\n",
            "Epoch 60/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 1091389223.7241\n",
            "Epoch 61/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 1159465524.3350\n",
            "Epoch 62/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1078299209.1429\n",
            "Epoch 63/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 1094320641.8916\n",
            "Epoch 64/700\n",
            "203/203 [==============================] - 0s 217us/step - loss: 1082564850.1281\n",
            "Epoch 65/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 1072254609.6552\n",
            "Epoch 66/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 1064520412.0591\n",
            "Epoch 67/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1063892423.5665\n",
            "Epoch 68/700\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1094798417.9704\n",
            "Epoch 69/700\n",
            "203/203 [==============================] - 0s 196us/step - loss: 1105886211.7833\n",
            "Epoch 70/700\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1144357388.6108\n",
            "Epoch 71/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 1082277863.2512\n",
            "Epoch 72/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 1106969680.7094\n",
            "Epoch 73/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 1088686039.0148\n",
            "Epoch 74/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1073682110.7389\n",
            "Epoch 75/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1065284365.8719\n",
            "Epoch 76/700\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1079636538.0099\n",
            "Epoch 77/700\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1100035867.1133\n",
            "Epoch 78/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1127841378.3645\n",
            "Epoch 79/700\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1164483400.5123\n",
            "Epoch 80/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 1149505089.5764\n",
            "Epoch 81/700\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1181295873.8916\n",
            "Epoch 82/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 1123058650.7980\n",
            "Epoch 83/700\n",
            "203/203 [==============================] - 0s 162us/step - loss: 1062019774.4236\n",
            "Epoch 84/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 1108252663.1724\n",
            "Epoch 85/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1077935358.7389\n",
            "Epoch 86/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1102613119.3695\n",
            "Epoch 87/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 1122227668.8079\n",
            "Epoch 88/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 1126343271.4089\n",
            "Epoch 89/700\n",
            "203/203 [==============================] - 0s 193us/step - loss: 1099903405.7143\n",
            "Epoch 90/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1087972458.8768\n",
            "Epoch 91/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 1143448287.8424\n",
            "Epoch 92/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 1120758050.6798\n",
            "Epoch 93/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1116366947.6256\n",
            "Epoch 94/700\n",
            "203/203 [==============================] - 0s 211us/step - loss: 1188634245.0443\n",
            "Epoch 95/700\n",
            "203/203 [==============================] - 0s 199us/step - loss: 1140564608.3153\n",
            "Epoch 96/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 1067419208.1970\n",
            "Epoch 97/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 1190735385.2217\n",
            "Epoch 98/700\n",
            "203/203 [==============================] - 0s 193us/step - loss: 1081781014.6995\n",
            "Epoch 99/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 1050041546.4039\n",
            "Epoch 100/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 1082515334.9360\n",
            "Epoch 101/700\n",
            "203/203 [==============================] - 0s 204us/step - loss: 1065721697.7340\n",
            "Epoch 102/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 1068904341.4384\n",
            "Epoch 103/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1088628366.5025\n",
            "Epoch 104/700\n",
            "203/203 [==============================] - 0s 200us/step - loss: 1057519607.1724\n",
            "Epoch 105/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1046929546.0887\n",
            "Epoch 106/700\n",
            "203/203 [==============================] - 0s 197us/step - loss: 1074744389.3596\n",
            "Epoch 107/700\n",
            "203/203 [==============================] - 0s 189us/step - loss: 1054106119.2512\n",
            "Epoch 108/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 1080809686.3842\n",
            "Epoch 109/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 1070240734.2660\n",
            "Epoch 110/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 1159597725.6355\n",
            "Epoch 111/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1100263835.1133\n",
            "Epoch 112/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 1110937672.5123\n",
            "Epoch 113/700\n",
            "203/203 [==============================] - 0s 199us/step - loss: 1071843617.4187\n",
            "Epoch 114/700\n",
            "203/203 [==============================] - 0s 215us/step - loss: 1091984679.0936\n",
            "Epoch 115/700\n",
            "203/203 [==============================] - 0s 214us/step - loss: 1119161499.7438\n",
            "Epoch 116/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 1163910270.4236\n",
            "Epoch 117/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 1139862922.4039\n",
            "Epoch 118/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 1207343998.7389\n",
            "Epoch 119/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 1189105344.3153\n",
            "Epoch 120/700\n",
            "203/203 [==============================] - 0s 226us/step - loss: 1110332643.9409\n",
            "Epoch 121/700\n",
            "203/203 [==============================] - 0s 209us/step - loss: 1070056435.3892\n",
            "Epoch 122/700\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1082939152.3941\n",
            "Epoch 123/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 1057238636.4532\n",
            "Epoch 124/700\n",
            "203/203 [==============================] - 0s 200us/step - loss: 1057809391.9212\n",
            "Epoch 125/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 1046636395.8227\n",
            "Epoch 126/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1048129310.2660\n",
            "Epoch 127/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 1048575819.3498\n",
            "Epoch 128/700\n",
            "203/203 [==============================] - 0s 198us/step - loss: 1059198259.7044\n",
            "Epoch 129/700\n",
            "203/203 [==============================] - 0s 203us/step - loss: 1068749838.5025\n",
            "Epoch 130/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 1115337320.6700\n",
            "Epoch 131/700\n",
            "203/203 [==============================] - 0s 222us/step - loss: 1081571064.7488\n",
            "Epoch 132/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 1056967225.6946\n",
            "Epoch 133/700\n",
            "203/203 [==============================] - 0s 196us/step - loss: 1072506871.8030\n",
            "Epoch 134/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1134163473.3399\n",
            "Epoch 135/700\n",
            "203/203 [==============================] - 0s 200us/step - loss: 1250648437.2808\n",
            "Epoch 136/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 1071708134.7783\n",
            "Epoch 137/700\n",
            "203/203 [==============================] - 0s 191us/step - loss: 1151811198.7389\n",
            "Epoch 138/700\n",
            "203/203 [==============================] - 0s 202us/step - loss: 1208431028.9655\n",
            "Epoch 139/700\n",
            "203/203 [==============================] - 0s 220us/step - loss: 1057004852.3350\n",
            "Epoch 140/700\n",
            "203/203 [==============================] - 0s 202us/step - loss: 1039766309.2020\n",
            "Epoch 141/700\n",
            "203/203 [==============================] - 0s 197us/step - loss: 1070986493.1626\n",
            "Epoch 142/700\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1087156481.2611\n",
            "Epoch 143/700\n",
            "203/203 [==============================] - 0s 193us/step - loss: 1055242248.8276\n",
            "Epoch 144/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 1083482635.9803\n",
            "Epoch 145/700\n",
            "203/203 [==============================] - 0s 198us/step - loss: 1104853074.2857\n",
            "Epoch 146/700\n",
            "203/203 [==============================] - 0s 202us/step - loss: 1097169259.8227\n",
            "Epoch 147/700\n",
            "203/203 [==============================] - 0s 198us/step - loss: 1047841927.5665\n",
            "Epoch 148/700\n",
            "203/203 [==============================] - 0s 201us/step - loss: 1046918763.5074\n",
            "Epoch 149/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 1065322551.9606\n",
            "Epoch 150/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 1058909251.4680\n",
            "Epoch 151/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1159925287.7241\n",
            "Epoch 152/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 1101392679.0936\n",
            "Epoch 153/700\n",
            "203/203 [==============================] - 0s 197us/step - loss: 1083796283.9015\n",
            "Epoch 154/700\n",
            "203/203 [==============================] - 0s 196us/step - loss: 1147462199.4877\n",
            "Epoch 155/700\n",
            "203/203 [==============================] - 0s 191us/step - loss: 1075532332.1379\n",
            "Epoch 156/700\n",
            "203/203 [==============================] - 0s 203us/step - loss: 1104990831.6059\n",
            "Epoch 157/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 1166136819.0739\n",
            "Epoch 158/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 1026300539.2709\n",
            "Epoch 159/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 1056822225.9704\n",
            "Epoch 160/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 1070149964.2956\n",
            "Epoch 161/700\n",
            "203/203 [==============================] - 0s 196us/step - loss: 1146302250.5616\n",
            "Epoch 162/700\n",
            "203/203 [==============================] - 0s 195us/step - loss: 1047129747.2315\n",
            "Epoch 163/700\n",
            "203/203 [==============================] - 0s 208us/step - loss: 1099884089.3793\n",
            "Epoch 164/700\n",
            "203/203 [==============================] - 0s 226us/step - loss: 1067464205.2414\n",
            "Epoch 165/700\n",
            "203/203 [==============================] - 0s 202us/step - loss: 1045325452.6108\n",
            "Epoch 166/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 1025962046.1084\n",
            "Epoch 167/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 1229126268.2167\n",
            "Epoch 168/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 1111210390.8571\n",
            "Epoch 169/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 1054676856.4335\n",
            "Epoch 170/700\n",
            "203/203 [==============================] - 0s 196us/step - loss: 1055357691.2709\n",
            "Epoch 171/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 1129869825.2611\n",
            "Epoch 172/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 1066862633.3005\n",
            "Epoch 173/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 1040879746.5222\n",
            "Epoch 174/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 1096469051.2709\n",
            "Epoch 175/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 1102793983.6847\n",
            "Epoch 176/700\n",
            "203/203 [==============================] - 0s 195us/step - loss: 1046473246.2660\n",
            "Epoch 177/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 1080544754.7586\n",
            "Epoch 178/700\n",
            "203/203 [==============================] - 0s 221us/step - loss: 1121027636.0197\n",
            "Epoch 179/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1040606466.2069\n",
            "Epoch 180/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 1201650329.2217\n",
            "Epoch 181/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 1070523879.7241\n",
            "Epoch 182/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 1073065115.4286\n",
            "Epoch 183/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 1108435961.6946\n",
            "Epoch 184/700\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1080903027.3892\n",
            "Epoch 185/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1101627161.8522\n",
            "Epoch 186/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 1177532113.3399\n",
            "Epoch 187/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 1079468964.5714\n",
            "Epoch 188/700\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1022563707.5862\n",
            "Epoch 189/700\n",
            "203/203 [==============================] - 0s 191us/step - loss: 1062128104.0394\n",
            "Epoch 190/700\n",
            "203/203 [==============================] - 0s 218us/step - loss: 1136579580.8473\n",
            "Epoch 191/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 1030101387.0345\n",
            "Epoch 192/700\n",
            "203/203 [==============================] - 0s 198us/step - loss: 1045850805.5961\n",
            "Epoch 193/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 1090386063.1330\n",
            "Epoch 194/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1116992643.1527\n",
            "Epoch 195/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 1002635299.3103\n",
            "Epoch 196/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 1022703627.3498\n",
            "Epoch 197/700\n",
            "203/203 [==============================] - 0s 211us/step - loss: 1007782434.6798\n",
            "Epoch 198/700\n",
            "203/203 [==============================] - 0s 237us/step - loss: 1012925855.5271\n",
            "Epoch 199/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 1027099226.1675\n",
            "Epoch 200/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 1028075623.4089\n",
            "Epoch 201/700\n",
            "203/203 [==============================] - 0s 193us/step - loss: 1039693329.0246\n",
            "Epoch 202/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1126283908.4138\n",
            "Epoch 203/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1047905175.3300\n",
            "Epoch 204/700\n",
            "203/203 [==============================] - 0s 173us/step - loss: 994081795.4680\n",
            "Epoch 205/700\n",
            "203/203 [==============================] - 0s 179us/step - loss: 1060251118.9754\n",
            "Epoch 206/700\n",
            "203/203 [==============================] - 0s 212us/step - loss: 1043786059.3498\n",
            "Epoch 207/700\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1025119447.0148\n",
            "Epoch 208/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1017426911.2118\n",
            "Epoch 209/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1015901512.1970\n",
            "Epoch 210/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 999529460.0197\n",
            "Epoch 211/700\n",
            "203/203 [==============================] - 0s 160us/step - loss: 993792173.2414\n",
            "Epoch 212/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1038024261.3596\n",
            "Epoch 213/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1043833004.7685\n",
            "Epoch 214/700\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1021471383.9606\n",
            "Epoch 215/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 999973377.5764\n",
            "Epoch 216/700\n",
            "203/203 [==============================] - 0s 167us/step - loss: 999524809.4581\n",
            "Epoch 217/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 1086885525.4384\n",
            "Epoch 218/700\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1010019407.7635\n",
            "Epoch 219/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 983519375.1330\n",
            "Epoch 220/700\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1120734049.7340\n",
            "Epoch 221/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 1002013001.7734\n",
            "Epoch 222/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 1008319279.2906\n",
            "Epoch 223/700\n",
            "203/203 [==============================] - 0s 179us/step - loss: 997753254.7783\n",
            "Epoch 224/700\n",
            "203/203 [==============================] - 0s 198us/step - loss: 994762352.8670\n",
            "Epoch 225/700\n",
            "203/203 [==============================] - 0s 196us/step - loss: 1069069697.8916\n",
            "Epoch 226/700\n",
            "203/203 [==============================] - 0s 238us/step - loss: 1024921655.1724\n",
            "Epoch 227/700\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1010343871.6847\n",
            "Epoch 228/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 999253422.0296\n",
            "Epoch 229/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 1034062992.0788\n",
            "Epoch 230/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 984924223.6847\n",
            "Epoch 231/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 1079016593.3399\n",
            "Epoch 232/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 1215400686.0296\n",
            "Epoch 233/700\n",
            "203/203 [==============================] - 0s 217us/step - loss: 953607112.9852\n",
            "Epoch 234/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 986222524.5320\n",
            "Epoch 235/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 990742285.8719\n",
            "Epoch 236/700\n",
            "203/203 [==============================] - 0s 208us/step - loss: 996633929.7734\n",
            "Epoch 237/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 1044945795.7833\n",
            "Epoch 238/700\n",
            "203/203 [==============================] - 0s 165us/step - loss: 1010268956.6897\n",
            "Epoch 239/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 1074479172.7291\n",
            "Epoch 240/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 1189820735.6847\n",
            "Epoch 241/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1167541122.5222\n",
            "Epoch 242/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 1267977245.0049\n",
            "Epoch 243/700\n",
            "203/203 [==============================] - 0s 231us/step - loss: 1114301472.1576\n",
            "Epoch 244/700\n",
            "203/203 [==============================] - 0s 198us/step - loss: 1027169121.4187\n",
            "Epoch 245/700\n",
            "203/203 [==============================] - 0s 195us/step - loss: 973896887.4877\n",
            "Epoch 246/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 984152173.0837\n",
            "Epoch 247/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 996203536.7094\n",
            "Epoch 248/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1021076883.8621\n",
            "Epoch 249/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 1129255792.8670\n",
            "Epoch 250/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 997755897.3793\n",
            "Epoch 251/700\n",
            "203/203 [==============================] - 0s 203us/step - loss: 988430733.5567\n",
            "Epoch 252/700\n",
            "203/203 [==============================] - 0s 201us/step - loss: 1017314191.7635\n",
            "Epoch 253/700\n",
            "203/203 [==============================] - 0s 200us/step - loss: 1037186173.1626\n",
            "Epoch 254/700\n",
            "203/203 [==============================] - 0s 200us/step - loss: 1029780595.7044\n",
            "Epoch 255/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1113375073.7340\n",
            "Epoch 256/700\n",
            "203/203 [==============================] - 0s 168us/step - loss: 976316624.3941\n",
            "Epoch 257/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 970103131.7438\n",
            "Epoch 258/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 974462189.3990\n",
            "Epoch 259/700\n",
            "203/203 [==============================] - 0s 237us/step - loss: 955230883.9409\n",
            "Epoch 260/700\n",
            "203/203 [==============================] - 0s 212us/step - loss: 1079566458.3251\n",
            "Epoch 261/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1215296413.0049\n",
            "Epoch 262/700\n",
            "203/203 [==============================] - 0s 216us/step - loss: 1017730835.0739\n",
            "Epoch 263/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 1029497171.8621\n",
            "Epoch 264/700\n",
            "203/203 [==============================] - 0s 168us/step - loss: 960396050.9163\n",
            "Epoch 265/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 986768561.1823\n",
            "Epoch 266/700\n",
            "203/203 [==============================] - 0s 160us/step - loss: 988284680.1970\n",
            "Epoch 267/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 934680967.5665\n",
            "Epoch 268/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 1031695042.8374\n",
            "Epoch 269/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 951952217.0640\n",
            "Epoch 270/700\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1022187578.6404\n",
            "Epoch 271/700\n",
            "203/203 [==============================] - 0s 203us/step - loss: 1014405212.6897\n",
            "Epoch 272/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 961687121.3399\n",
            "Epoch 273/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 994687620.4138\n",
            "Epoch 274/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 985757110.5419\n",
            "Epoch 275/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1000310033.0246\n",
            "Epoch 276/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 961115583.3695\n",
            "Epoch 277/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 1024779134.4236\n",
            "Epoch 278/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 1029459829.1232\n",
            "Epoch 279/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 939789040.5517\n",
            "Epoch 280/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 927385572.2562\n",
            "Epoch 281/700\n",
            "203/203 [==============================] - 0s 164us/step - loss: 988415441.9704\n",
            "Epoch 282/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 934841603.1527\n",
            "Epoch 283/700\n",
            "203/203 [==============================] - 0s 189us/step - loss: 925084781.7143\n",
            "Epoch 284/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 925484593.4975\n",
            "Epoch 285/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 943905751.6453\n",
            "Epoch 286/700\n",
            "203/203 [==============================] - 0s 162us/step - loss: 902510974.1084\n",
            "Epoch 287/700\n",
            "203/203 [==============================] - 0s 149us/step - loss: 948125442.8374\n",
            "Epoch 288/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 915307898.3251\n",
            "Epoch 289/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 999091993.8522\n",
            "Epoch 290/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 943463361.2611\n",
            "Epoch 291/700\n",
            "203/203 [==============================] - 0s 161us/step - loss: 922342051.3103\n",
            "Epoch 292/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 908709817.0640\n",
            "Epoch 293/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 957485521.3399\n",
            "Epoch 294/700\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1081306952.8276\n",
            "Epoch 295/700\n",
            "203/203 [==============================] - 0s 165us/step - loss: 1067111998.4236\n",
            "Epoch 296/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 935466320.7094\n",
            "Epoch 297/700\n",
            "203/203 [==============================] - 0s 164us/step - loss: 906234036.3350\n",
            "Epoch 298/700\n",
            "203/203 [==============================] - 0s 166us/step - loss: 922751647.0542\n",
            "Epoch 299/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 883361873.9704\n",
            "Epoch 300/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 895952817.8128\n",
            "Epoch 301/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 905493134.5025\n",
            "Epoch 302/700\n",
            "203/203 [==============================] - 0s 173us/step - loss: 886257399.8030\n",
            "Epoch 303/700\n",
            "203/203 [==============================] - 0s 179us/step - loss: 1001130577.6552\n",
            "Epoch 304/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 943996953.5369\n",
            "Epoch 305/700\n",
            "203/203 [==============================] - 0s 169us/step - loss: 875428172.9261\n",
            "Epoch 306/700\n",
            "203/203 [==============================] - 0s 164us/step - loss: 825377019.5862\n",
            "Epoch 307/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1035424586.0887\n",
            "Epoch 308/700\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1033872921.8522\n",
            "Epoch 309/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 883022279.5665\n",
            "Epoch 310/700\n",
            "203/203 [==============================] - 0s 191us/step - loss: 881329563.7438\n",
            "Epoch 311/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 813783803.2709\n",
            "Epoch 312/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 874018918.4631\n",
            "Epoch 313/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 857863988.6502\n",
            "Epoch 314/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 913621947.4286\n",
            "Epoch 315/700\n",
            "203/203 [==============================] - 0s 169us/step - loss: 837514395.2709\n",
            "Epoch 316/700\n",
            "203/203 [==============================] - 0s 167us/step - loss: 861997932.4532\n",
            "Epoch 317/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 803941435.9015\n",
            "Epoch 318/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 802124306.2857\n",
            "Epoch 319/700\n",
            "203/203 [==============================] - 0s 165us/step - loss: 803140581.2020\n",
            "Epoch 320/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 794921388.7685\n",
            "Epoch 321/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 785308972.7685\n",
            "Epoch 322/700\n",
            "203/203 [==============================] - 0s 168us/step - loss: 866177352.1970\n",
            "Epoch 323/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 847112645.5172\n",
            "Epoch 324/700\n",
            "203/203 [==============================] - 0s 191us/step - loss: 807882205.0049\n",
            "Epoch 325/700\n",
            "203/203 [==============================] - 0s 164us/step - loss: 840127819.3498\n",
            "Epoch 326/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 814510816.4729\n",
            "Epoch 327/700\n",
            "203/203 [==============================] - 0s 198us/step - loss: 730372786.7586\n",
            "Epoch 328/700\n",
            "203/203 [==============================] - 0s 212us/step - loss: 918069295.9212\n",
            "Epoch 329/700\n",
            "203/203 [==============================] - 0s 224us/step - loss: 806957440.9458\n",
            "Epoch 330/700\n",
            "203/203 [==============================] - 0s 167us/step - loss: 767697870.8177\n",
            "Epoch 331/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 742472517.0443\n",
            "Epoch 332/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 712317235.7044\n",
            "Epoch 333/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 701195123.3892\n",
            "Epoch 334/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 766460782.3448\n",
            "Epoch 335/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 703952235.1921\n",
            "Epoch 336/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 762503536.8670\n",
            "Epoch 337/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 694211752.1970\n",
            "Epoch 338/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 654318132.0197\n",
            "Epoch 339/700\n",
            "203/203 [==============================] - 0s 161us/step - loss: 712163888.0788\n",
            "Epoch 340/700\n",
            "203/203 [==============================] - 0s 179us/step - loss: 657514635.6650\n",
            "Epoch 341/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 620677170.2857\n",
            "Epoch 342/700\n",
            "203/203 [==============================] - 0s 169us/step - loss: 624097185.1034\n",
            "Epoch 343/700\n",
            "203/203 [==============================] - 0s 161us/step - loss: 585427266.6798\n",
            "Epoch 344/700\n",
            "203/203 [==============================] - 0s 162us/step - loss: 735175121.9704\n",
            "Epoch 345/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 728750992.7094\n",
            "Epoch 346/700\n",
            "203/203 [==============================] - 0s 157us/step - loss: 632218009.6946\n",
            "Epoch 347/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 546921108.8079\n",
            "Epoch 348/700\n",
            "203/203 [==============================] - 0s 162us/step - loss: 503867965.1626\n",
            "Epoch 349/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 503447285.4384\n",
            "Epoch 350/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 449375886.0296\n",
            "Epoch 351/700\n",
            "203/203 [==============================] - 0s 169us/step - loss: 579753842.7586\n",
            "Epoch 352/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 535491383.1724\n",
            "Epoch 353/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 452598592.3153\n",
            "Epoch 354/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 415084937.7734\n",
            "Epoch 355/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 438871207.4089\n",
            "Epoch 356/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 370176383.1724\n",
            "Epoch 357/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 379792759.9606\n",
            "Epoch 358/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 409019701.0837\n",
            "Epoch 359/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 347082611.2315\n",
            "Epoch 360/700\n",
            "203/203 [==============================] - 0s 179us/step - loss: 322198785.8916\n",
            "Epoch 361/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 305995001.0640\n",
            "Epoch 362/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 249101482.0887\n",
            "Epoch 363/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 304506137.4581\n",
            "Epoch 364/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 906479956.1773\n",
            "Epoch 365/700\n",
            "203/203 [==============================] - 0s 162us/step - loss: 441594603.5074\n",
            "Epoch 366/700\n",
            "203/203 [==============================] - 0s 162us/step - loss: 271010840.8276\n",
            "Epoch 367/700\n",
            "203/203 [==============================] - 0s 208us/step - loss: 244568930.5222\n",
            "Epoch 368/700\n",
            "203/203 [==============================] - 0s 169us/step - loss: 274193802.8768\n",
            "Epoch 369/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 232134073.5369\n",
            "Epoch 370/700\n",
            "203/203 [==============================] - 0s 168us/step - loss: 211266763.9015\n",
            "Epoch 371/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 259773002.2463\n",
            "Epoch 372/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 261782202.5616\n",
            "Epoch 373/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 190370684.6108\n",
            "Epoch 374/700\n",
            "203/203 [==============================] - 0s 173us/step - loss: 171675461.1232\n",
            "Epoch 375/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 195887421.2808\n",
            "Epoch 376/700\n",
            "203/203 [==============================] - 0s 213us/step - loss: 211918298.4039\n",
            "Epoch 377/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 305877034.8768\n",
            "Epoch 378/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 228103763.8227\n",
            "Epoch 379/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 471032920.7488\n",
            "Epoch 380/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 444612608.0788\n",
            "Epoch 381/700\n",
            "203/203 [==============================] - 0s 204us/step - loss: 392392412.9261\n",
            "Epoch 382/700\n",
            "203/203 [==============================] - 0s 198us/step - loss: 243922771.0739\n",
            "Epoch 383/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 296229903.2906\n",
            "Epoch 384/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 282033969.0246\n",
            "Epoch 385/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 262421832.1970\n",
            "Epoch 386/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 216256515.0739\n",
            "Epoch 387/700\n",
            "203/203 [==============================] - 0s 197us/step - loss: 192767369.0640\n",
            "Epoch 388/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 171072035.5468\n",
            "Epoch 389/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 190090073.9310\n",
            "Epoch 390/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 195461393.6946\n",
            "Epoch 391/700\n",
            "203/203 [==============================] - 0s 205us/step - loss: 214646112.9458\n",
            "Epoch 392/700\n",
            "203/203 [==============================] - 0s 200us/step - loss: 275688968.6700\n",
            "Epoch 393/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 232997306.9557\n",
            "Epoch 394/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 150621007.1330\n",
            "Epoch 395/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 181953357.7931\n",
            "Epoch 396/700\n",
            "203/203 [==============================] - 0s 162us/step - loss: 236194461.1626\n",
            "Epoch 397/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 189252455.8030\n",
            "Epoch 398/700\n",
            "203/203 [==============================] - 0s 158us/step - loss: 210327573.3596\n",
            "Epoch 399/700\n",
            "203/203 [==============================] - 0s 157us/step - loss: 293112146.9163\n",
            "Epoch 400/700\n",
            "203/203 [==============================] - 0s 167us/step - loss: 271016306.9163\n",
            "Epoch 401/700\n",
            "203/203 [==============================] - 0s 169us/step - loss: 205041521.4975\n",
            "Epoch 402/700\n",
            "203/203 [==============================] - 0s 191us/step - loss: 162023490.5813\n",
            "Epoch 403/700\n",
            "203/203 [==============================] - 0s 166us/step - loss: 139084731.7438\n",
            "Epoch 404/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 138982192.7882\n",
            "Epoch 405/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 154240190.5813\n",
            "Epoch 406/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 185986967.0148\n",
            "Epoch 407/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 170717575.6059\n",
            "Epoch 408/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 404234874.6404\n",
            "Epoch 409/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 300157980.5320\n",
            "Epoch 410/700\n",
            "203/203 [==============================] - 0s 168us/step - loss: 283293291.4286\n",
            "Epoch 411/700\n",
            "203/203 [==============================] - 0s 204us/step - loss: 347843007.0542\n",
            "Epoch 412/700\n",
            "203/203 [==============================] - 0s 166us/step - loss: 552633946.7980\n",
            "Epoch 413/700\n",
            "203/203 [==============================] - 0s 196us/step - loss: 2555042792.0394\n",
            "Epoch 414/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 1368386975.2118\n",
            "Epoch 415/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 679513325.0837\n",
            "Epoch 416/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 431071781.8325\n",
            "Epoch 417/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 351470519.4877\n",
            "Epoch 418/700\n",
            "203/203 [==============================] - 0s 193us/step - loss: 317785092.0985\n",
            "Epoch 419/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 302673917.0837\n",
            "Epoch 420/700\n",
            "203/203 [==============================] - 0s 189us/step - loss: 282431506.9163\n",
            "Epoch 421/700\n",
            "203/203 [==============================] - 0s 191us/step - loss: 274495501.0049\n",
            "Epoch 422/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 275440614.7783\n",
            "Epoch 423/700\n",
            "203/203 [==============================] - 0s 197us/step - loss: 241693467.2709\n",
            "Epoch 424/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 225490930.5222\n",
            "Epoch 425/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 235426306.3645\n",
            "Epoch 426/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 195016558.8966\n",
            "Epoch 427/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 192252044.0591\n",
            "Epoch 428/700\n",
            "203/203 [==============================] - 0s 179us/step - loss: 200967896.0394\n",
            "Epoch 429/700\n",
            "203/203 [==============================] - 0s 239us/step - loss: 201683512.3547\n",
            "Epoch 430/700\n",
            "203/203 [==============================] - 0s 210us/step - loss: 167864132.8079\n",
            "Epoch 431/700\n",
            "203/203 [==============================] - 0s 201us/step - loss: 152318721.0246\n",
            "Epoch 432/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 183150850.2857\n",
            "Epoch 433/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 198717675.1921\n",
            "Epoch 434/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 219798863.2906\n",
            "Epoch 435/700\n",
            "203/203 [==============================] - 0s 298us/step - loss: 162259551.4483\n",
            "Epoch 436/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 144191559.5665\n",
            "Epoch 437/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 136828506.7586\n",
            "Epoch 438/700\n",
            "203/203 [==============================] - 0s 173us/step - loss: 141882679.0936\n",
            "Epoch 439/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 140711624.0394\n",
            "Epoch 440/700\n",
            "203/203 [==============================] - 0s 214us/step - loss: 129046069.9901\n",
            "Epoch 441/700\n",
            "203/203 [==============================] - 0s 191us/step - loss: 136446376.6700\n",
            "Epoch 442/700\n",
            "203/203 [==============================] - 0s 220us/step - loss: 138516603.2709\n",
            "Epoch 443/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 140187431.7241\n",
            "Epoch 444/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 140611234.4433\n",
            "Epoch 445/700\n",
            "203/203 [==============================] - 0s 179us/step - loss: 158130986.2857\n",
            "Epoch 446/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 154763352.2759\n",
            "Epoch 447/700\n",
            "203/203 [==============================] - 0s 195us/step - loss: 159974237.7537\n",
            "Epoch 448/700\n",
            "203/203 [==============================] - 0s 193us/step - loss: 178189237.6749\n",
            "Epoch 449/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 143898980.4926\n",
            "Epoch 450/700\n",
            "203/203 [==============================] - 0s 169us/step - loss: 143064659.0739\n",
            "Epoch 451/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 130151469.3202\n",
            "Epoch 452/700\n",
            "203/203 [==============================] - 0s 206us/step - loss: 145575826.6010\n",
            "Epoch 453/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 147889934.5025\n",
            "Epoch 454/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 150126821.8325\n",
            "Epoch 455/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 153643964.9261\n",
            "Epoch 456/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 189215991.2512\n",
            "Epoch 457/700\n",
            "203/203 [==============================] - 0s 199us/step - loss: 181858278.5419\n",
            "Epoch 458/700\n",
            "203/203 [==============================] - 0s 215us/step - loss: 244407050.9557\n",
            "Epoch 459/700\n",
            "203/203 [==============================] - 0s 200us/step - loss: 158456778.6404\n",
            "Epoch 460/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 193858008.0394\n",
            "Epoch 461/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 147694234.0493\n",
            "Epoch 462/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 151137103.8227\n",
            "Epoch 463/700\n",
            "203/203 [==============================] - 0s 189us/step - loss: 195617724.8473\n",
            "Epoch 464/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 262284318.4236\n",
            "Epoch 465/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 217830820.2562\n",
            "Epoch 466/700\n",
            "203/203 [==============================] - 0s 189us/step - loss: 325821050.3251\n",
            "Epoch 467/700\n",
            "203/203 [==============================] - 0s 189us/step - loss: 256854890.9557\n",
            "Epoch 468/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 222900241.3399\n",
            "Epoch 469/700\n",
            "203/203 [==============================] - 0s 154us/step - loss: 165918988.2167\n",
            "Epoch 470/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 158891495.4089\n",
            "Epoch 471/700\n",
            "203/203 [==============================] - 0s 168us/step - loss: 141560727.5271\n",
            "Epoch 472/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 161875122.4236\n",
            "Epoch 473/700\n",
            "203/203 [==============================] - 0s 163us/step - loss: 232272407.0148\n",
            "Epoch 474/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 257416846.0296\n",
            "Epoch 475/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 301412045.7143\n",
            "Epoch 476/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 251573875.3892\n",
            "Epoch 477/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 144103539.0739\n",
            "Epoch 478/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 186738299.8227\n",
            "Epoch 479/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 142013743.4483\n",
            "Epoch 480/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 171664017.8128\n",
            "Epoch 481/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 149950962.8374\n",
            "Epoch 482/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 140493163.0345\n",
            "Epoch 483/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 136093458.5222\n",
            "Epoch 484/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 133204785.2611\n",
            "Epoch 485/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 146754338.5222\n",
            "Epoch 486/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 307747875.3892\n",
            "Epoch 487/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 348433789.7143\n",
            "Epoch 488/700\n",
            "203/203 [==============================] - 0s 169us/step - loss: 213745219.4680\n",
            "Epoch 489/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 153955138.6798\n",
            "Epoch 490/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 171527515.1133\n",
            "Epoch 491/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 168833910.4631\n",
            "Epoch 492/700\n",
            "203/203 [==============================] - 0s 161us/step - loss: 189194696.4335\n",
            "Epoch 493/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 158589849.8522\n",
            "Epoch 494/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 153134915.8621\n",
            "Epoch 495/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 159702962.7192\n",
            "Epoch 496/700\n",
            "203/203 [==============================] - 0s 207us/step - loss: 150485458.1281\n",
            "Epoch 497/700\n",
            "203/203 [==============================] - 0s 199us/step - loss: 138810081.0443\n",
            "Epoch 498/700\n",
            "203/203 [==============================] - 0s 198us/step - loss: 130538399.3695\n",
            "Epoch 499/700\n",
            "203/203 [==============================] - 0s 179us/step - loss: 135400141.8719\n",
            "Epoch 500/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 163530109.3202\n",
            "Epoch 501/700\n",
            "203/203 [==============================] - 0s 220us/step - loss: 152569089.1034\n",
            "Epoch 502/700\n",
            "203/203 [==============================] - 0s 179us/step - loss: 136758394.0887\n",
            "Epoch 503/700\n",
            "203/203 [==============================] - 0s 210us/step - loss: 144601442.0493\n",
            "Epoch 504/700\n",
            "203/203 [==============================] - 0s 233us/step - loss: 131562461.9507\n",
            "Epoch 505/700\n",
            "203/203 [==============================] - 0s 204us/step - loss: 127977652.7291\n",
            "Epoch 506/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 152879699.3103\n",
            "Epoch 507/700\n",
            "203/203 [==============================] - 0s 173us/step - loss: 200858224.6305\n",
            "Epoch 508/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 199859576.1182\n",
            "Epoch 509/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 162591337.2217\n",
            "Epoch 510/700\n",
            "203/203 [==============================] - 0s 197us/step - loss: 168442774.1478\n",
            "Epoch 511/700\n",
            "203/203 [==============================] - 0s 167us/step - loss: 148733468.6108\n",
            "Epoch 512/700\n",
            "203/203 [==============================] - 0s 168us/step - loss: 192417433.0640\n",
            "Epoch 513/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 531894593.5764\n",
            "Epoch 514/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 231710795.8227\n",
            "Epoch 515/700\n",
            "203/203 [==============================] - 0s 168us/step - loss: 182742206.1478\n",
            "Epoch 516/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 191993708.2956\n",
            "Epoch 517/700\n",
            "203/203 [==============================] - 0s 160us/step - loss: 206973312.6305\n",
            "Epoch 518/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 206777494.4631\n",
            "Epoch 519/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 191726202.7192\n",
            "Epoch 520/700\n",
            "203/203 [==============================] - 0s 179us/step - loss: 169708516.6502\n",
            "Epoch 521/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 176770362.6404\n",
            "Epoch 522/700\n",
            "203/203 [==============================] - 0s 162us/step - loss: 154529041.0640\n",
            "Epoch 523/700\n",
            "203/203 [==============================] - 0s 151us/step - loss: 137743948.6897\n",
            "Epoch 524/700\n",
            "203/203 [==============================] - 0s 164us/step - loss: 166157484.3744\n",
            "Epoch 525/700\n",
            "203/203 [==============================] - 0s 157us/step - loss: 183153498.0099\n",
            "Epoch 526/700\n",
            "203/203 [==============================] - 0s 161us/step - loss: 176892340.6502\n",
            "Epoch 527/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 212610335.6059\n",
            "Epoch 528/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 146375717.8325\n",
            "Epoch 529/700\n",
            "203/203 [==============================] - 0s 166us/step - loss: 164108052.6502\n",
            "Epoch 530/700\n",
            "203/203 [==============================] - 0s 161us/step - loss: 215259170.7192\n",
            "Epoch 531/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 166075961.9704\n",
            "Epoch 532/700\n",
            "203/203 [==============================] - 0s 192us/step - loss: 145577539.1527\n",
            "Epoch 533/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 141110352.2365\n",
            "Epoch 534/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 140584573.5172\n",
            "Epoch 535/700\n",
            "203/203 [==============================] - 0s 189us/step - loss: 129125955.1527\n",
            "Epoch 536/700\n",
            "203/203 [==============================] - 0s 200us/step - loss: 156355023.6847\n",
            "Epoch 537/700\n",
            "203/203 [==============================] - 0s 197us/step - loss: 158007276.2167\n",
            "Epoch 538/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 133007284.2562\n",
            "Epoch 539/700\n",
            "203/203 [==============================] - 0s 195us/step - loss: 130064882.2463\n",
            "Epoch 540/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 138773874.0099\n",
            "Epoch 541/700\n",
            "203/203 [==============================] - 0s 221us/step - loss: 145896814.3448\n",
            "Epoch 542/700\n",
            "203/203 [==============================] - 0s 197us/step - loss: 165097822.1084\n",
            "Epoch 543/700\n",
            "203/203 [==============================] - 0s 197us/step - loss: 153545136.9458\n",
            "Epoch 544/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 188072556.1379\n",
            "Epoch 545/700\n",
            "203/203 [==============================] - 0s 165us/step - loss: 175873368.2759\n",
            "Epoch 546/700\n",
            "203/203 [==============================] - 0s 191us/step - loss: 170679366.3842\n",
            "Epoch 547/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 175352699.9803\n",
            "Epoch 548/700\n",
            "203/203 [==============================] - 0s 197us/step - loss: 169302162.5222\n",
            "Epoch 549/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 170962204.1379\n",
            "Epoch 550/700\n",
            "203/203 [==============================] - 0s 195us/step - loss: 195453346.0493\n",
            "Epoch 551/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 229281357.9507\n",
            "Epoch 552/700\n",
            "203/203 [==============================] - 0s 157us/step - loss: 217067796.1773\n",
            "Epoch 553/700\n",
            "203/203 [==============================] - 0s 174us/step - loss: 181445426.2463\n",
            "Epoch 554/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 144029100.5714\n",
            "Epoch 555/700\n",
            "203/203 [==============================] - 0s 179us/step - loss: 139418713.1429\n",
            "Epoch 556/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 134978227.5862\n",
            "Epoch 557/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 150277801.5764\n",
            "Epoch 558/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 175576927.6059\n",
            "Epoch 559/700\n",
            "203/203 [==============================] - 0s 186us/step - loss: 203608197.3596\n",
            "Epoch 560/700\n",
            "203/203 [==============================] - 0s 169us/step - loss: 154433117.4778\n",
            "Epoch 561/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 133940938.4039\n",
            "Epoch 562/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 148293394.9557\n",
            "Epoch 563/700\n",
            "203/203 [==============================] - 0s 191us/step - loss: 143219041.0246\n",
            "Epoch 564/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 131349535.5271\n",
            "Epoch 565/700\n",
            "203/203 [==============================] - 0s 173us/step - loss: 189091133.0049\n",
            "Epoch 566/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 243704134.3842\n",
            "Epoch 567/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 170913273.2217\n",
            "Epoch 568/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 141779171.3892\n",
            "Epoch 569/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 133368824.3153\n",
            "Epoch 570/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 135109290.9163\n",
            "Epoch 571/700\n",
            "203/203 [==============================] - 0s 166us/step - loss: 133751051.1133\n",
            "Epoch 572/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 140241832.8276\n",
            "Epoch 573/700\n",
            "203/203 [==============================] - 0s 159us/step - loss: 167249605.2020\n",
            "Epoch 574/700\n",
            "203/203 [==============================] - 0s 166us/step - loss: 153007576.0394\n",
            "Epoch 575/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 179867941.5172\n",
            "Epoch 576/700\n",
            "203/203 [==============================] - 0s 168us/step - loss: 177985926.6207\n",
            "Epoch 577/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 146722886.5419\n",
            "Epoch 578/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 130716911.6453\n",
            "Epoch 579/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 134669064.9852\n",
            "Epoch 580/700\n",
            "203/203 [==============================] - 0s 169us/step - loss: 141957703.3695\n",
            "Epoch 581/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 165127872.6305\n",
            "Epoch 582/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 138534398.2660\n",
            "Epoch 583/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 137190210.6010\n",
            "Epoch 584/700\n",
            "203/203 [==============================] - 0s 179us/step - loss: 206535749.5764\n",
            "Epoch 585/700\n",
            "203/203 [==============================] - 0s 166us/step - loss: 296903654.5419\n",
            "Epoch 586/700\n",
            "203/203 [==============================] - 0s 167us/step - loss: 351005087.5271\n",
            "Epoch 587/700\n",
            "203/203 [==============================] - 0s 169us/step - loss: 329953979.4286\n",
            "Epoch 588/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 410884561.0246\n",
            "Epoch 589/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 445807671.0936\n",
            "Epoch 590/700\n",
            "203/203 [==============================] - 0s 214us/step - loss: 294410055.6453\n",
            "Epoch 591/700\n",
            "203/203 [==============================] - 0s 169us/step - loss: 239301369.6946\n",
            "Epoch 592/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 232722995.5074\n",
            "Epoch 593/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 289106370.7192\n",
            "Epoch 594/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 286993348.7291\n",
            "Epoch 595/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 251499175.9606\n",
            "Epoch 596/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 218349424.5517\n",
            "Epoch 597/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 217968644.1773\n",
            "Epoch 598/700\n",
            "203/203 [==============================] - 0s 166us/step - loss: 317253725.7931\n",
            "Epoch 599/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 351985795.1527\n",
            "Epoch 600/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 333576949.4384\n",
            "Epoch 601/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 229238112.3941\n",
            "Epoch 602/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 152689515.2709\n",
            "Epoch 603/700\n",
            "203/203 [==============================] - 0s 160us/step - loss: 145212708.5714\n",
            "Epoch 604/700\n",
            "203/203 [==============================] - 0s 179us/step - loss: 164335880.5123\n",
            "Epoch 605/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 144475647.5271\n",
            "Epoch 606/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 148178274.9951\n",
            "Epoch 607/700\n",
            "203/203 [==============================] - 0s 173us/step - loss: 134609395.8621\n",
            "Epoch 608/700\n",
            "203/203 [==============================] - 0s 165us/step - loss: 130584316.1773\n",
            "Epoch 609/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 138488426.3251\n",
            "Epoch 610/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 147325661.3596\n",
            "Epoch 611/700\n",
            "203/203 [==============================] - 0s 179us/step - loss: 155703540.3350\n",
            "Epoch 612/700\n",
            "203/203 [==============================] - 0s 169us/step - loss: 130722375.6453\n",
            "Epoch 613/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 147846892.8473\n",
            "Epoch 614/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 138786093.2020\n",
            "Epoch 615/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 141050326.3842\n",
            "Epoch 616/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 154238501.6749\n",
            "Epoch 617/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 156000403.0345\n",
            "Epoch 618/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 148618623.8424\n",
            "Epoch 619/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 131238904.6305\n",
            "Epoch 620/700\n",
            "203/203 [==============================] - 0s 208us/step - loss: 132543584.7882\n",
            "Epoch 621/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 182584019.4286\n",
            "Epoch 622/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 141546127.9212\n",
            "Epoch 623/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 150547753.6158\n",
            "Epoch 624/700\n",
            "203/203 [==============================] - 0s 191us/step - loss: 185442306.6798\n",
            "Epoch 625/700\n",
            "203/203 [==============================] - 0s 167us/step - loss: 130642710.7783\n",
            "Epoch 626/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 136051648.8670\n",
            "Epoch 627/700\n",
            "203/203 [==============================] - 0s 205us/step - loss: 146343628.3744\n",
            "Epoch 628/700\n",
            "203/203 [==============================] - 0s 206us/step - loss: 144606093.3202\n",
            "Epoch 629/700\n",
            "203/203 [==============================] - 0s 202us/step - loss: 132674940.2167\n",
            "Epoch 630/700\n",
            "203/203 [==============================] - 0s 165us/step - loss: 144574689.9704\n",
            "Epoch 631/700\n",
            "203/203 [==============================] - 0s 195us/step - loss: 143482253.3990\n",
            "Epoch 632/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 148888793.7734\n",
            "Epoch 633/700\n",
            "203/203 [==============================] - 0s 205us/step - loss: 143707903.1724\n",
            "Epoch 634/700\n",
            "203/203 [==============================] - 0s 197us/step - loss: 135609153.7340\n",
            "Epoch 635/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 150433783.1724\n",
            "Epoch 636/700\n",
            "203/203 [==============================] - 0s 199us/step - loss: 157563452.9261\n",
            "Epoch 637/700\n",
            "203/203 [==============================] - 0s 235us/step - loss: 149237634.9163\n",
            "Epoch 638/700\n",
            "203/203 [==============================] - 0s 197us/step - loss: 140188877.8719\n",
            "Epoch 639/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 184724926.3054\n",
            "Epoch 640/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 293933491.8621\n",
            "Epoch 641/700\n",
            "203/203 [==============================] - 0s 162us/step - loss: 351443695.7241\n",
            "Epoch 642/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 307778195.7044\n",
            "Epoch 643/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 208752742.1478\n",
            "Epoch 644/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 185488736.9458\n",
            "Epoch 645/700\n",
            "203/203 [==============================] - 0s 180us/step - loss: 159734463.3695\n",
            "Epoch 646/700\n",
            "203/203 [==============================] - 0s 220us/step - loss: 158269562.3251\n",
            "Epoch 647/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 133348808.9458\n",
            "Epoch 648/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 140126837.4384\n",
            "Epoch 649/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 132861101.7143\n",
            "Epoch 650/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 191118055.5665\n",
            "Epoch 651/700\n",
            "203/203 [==============================] - 0s 169us/step - loss: 141184364.2956\n",
            "Epoch 652/700\n",
            "203/203 [==============================] - 0s 178us/step - loss: 137103716.3350\n",
            "Epoch 653/700\n",
            "203/203 [==============================] - 0s 211us/step - loss: 130479867.1133\n",
            "Epoch 654/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 137677993.7340\n",
            "Epoch 655/700\n",
            "203/203 [==============================] - 0s 162us/step - loss: 162717536.0788\n",
            "Epoch 656/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 187976591.3695\n",
            "Epoch 657/700\n",
            "203/203 [==============================] - 0s 203us/step - loss: 142064544.6305\n",
            "Epoch 658/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 149992890.8768\n",
            "Epoch 659/700\n",
            "203/203 [==============================] - 0s 177us/step - loss: 169002137.3793\n",
            "Epoch 660/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 155424621.7143\n",
            "Epoch 661/700\n",
            "203/203 [==============================] - 0s 182us/step - loss: 135551561.5764\n",
            "Epoch 662/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 136103792.5517\n",
            "Epoch 663/700\n",
            "203/203 [==============================] - 0s 199us/step - loss: 133385983.3300\n",
            "Epoch 664/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 149878296.5123\n",
            "Epoch 665/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 147401536.1970\n",
            "Epoch 666/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 172385561.0640\n",
            "Epoch 667/700\n",
            "203/203 [==============================] - 0s 194us/step - loss: 187921863.8818\n",
            "Epoch 668/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 147343336.4335\n",
            "Epoch 669/700\n",
            "203/203 [==============================] - 0s 195us/step - loss: 181478221.1626\n",
            "Epoch 670/700\n",
            "203/203 [==============================] - 0s 200us/step - loss: 188035546.0493\n",
            "Epoch 671/700\n",
            "203/203 [==============================] - 0s 195us/step - loss: 198725807.9212\n",
            "Epoch 672/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 221814671.0542\n",
            "Epoch 673/700\n",
            "203/203 [==============================] - 0s 170us/step - loss: 153206861.9507\n",
            "Epoch 674/700\n",
            "203/203 [==============================] - 0s 196us/step - loss: 153153429.9901\n",
            "Epoch 675/700\n",
            "203/203 [==============================] - 0s 214us/step - loss: 158746078.0296\n",
            "Epoch 676/700\n",
            "203/203 [==============================] - 0s 190us/step - loss: 143018048.2365\n",
            "Epoch 677/700\n",
            "203/203 [==============================] - 0s 201us/step - loss: 127780330.6798\n",
            "Epoch 678/700\n",
            "203/203 [==============================] - 0s 187us/step - loss: 136919490.0493\n",
            "Epoch 679/700\n",
            "203/203 [==============================] - 0s 176us/step - loss: 127682098.2463\n",
            "Epoch 680/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 135611258.4039\n",
            "Epoch 681/700\n",
            "203/203 [==============================] - 0s 164us/step - loss: 141653211.3103\n",
            "Epoch 682/700\n",
            "203/203 [==============================] - 0s 203us/step - loss: 160513666.3251\n",
            "Epoch 683/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 155420223.1921\n",
            "Epoch 684/700\n",
            "203/203 [==============================] - 0s 158us/step - loss: 181935546.2463\n",
            "Epoch 685/700\n",
            "203/203 [==============================] - 0s 151us/step - loss: 213792161.7340\n",
            "Epoch 686/700\n",
            "203/203 [==============================] - 0s 165us/step - loss: 188925625.3793\n",
            "Epoch 687/700\n",
            "203/203 [==============================] - 0s 173us/step - loss: 147777669.1232\n",
            "Epoch 688/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 136604505.8522\n",
            "Epoch 689/700\n",
            "203/203 [==============================] - 0s 183us/step - loss: 148302208.6305\n",
            "Epoch 690/700\n",
            "203/203 [==============================] - 0s 175us/step - loss: 185885609.0640\n",
            "Epoch 691/700\n",
            "203/203 [==============================] - 0s 167us/step - loss: 145115975.0936\n",
            "Epoch 692/700\n",
            "203/203 [==============================] - 0s 185us/step - loss: 161167197.7931\n",
            "Epoch 693/700\n",
            "203/203 [==============================] - 0s 184us/step - loss: 134615322.2463\n",
            "Epoch 694/700\n",
            "203/203 [==============================] - 0s 171us/step - loss: 160968564.0985\n",
            "Epoch 695/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 164078826.2463\n",
            "Epoch 696/700\n",
            "203/203 [==============================] - 0s 165us/step - loss: 156282773.7537\n",
            "Epoch 697/700\n",
            "203/203 [==============================] - 0s 172us/step - loss: 164834151.4089\n",
            "Epoch 698/700\n",
            "203/203 [==============================] - 0s 181us/step - loss: 185423775.7635\n",
            "Epoch 699/700\n",
            "203/203 [==============================] - 0s 189us/step - loss: 228819280.3941\n",
            "Epoch 700/700\n",
            "203/203 [==============================] - 0s 188us/step - loss: 238526081.5764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0d8a785dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGmLWdIISLpb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bd68c488-a6ac-4fac-c4ce-01f2dbcb6992"
      },
      "source": [
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88/88 [==============================] - 0s 549us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "249956978.9090909"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DslY6pntSLm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXxjELTWSLlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbUfkJt0SLgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvliG_MFSLa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMRHAdCtSGdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKR6ohidSGZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG3-2ly6SGWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}