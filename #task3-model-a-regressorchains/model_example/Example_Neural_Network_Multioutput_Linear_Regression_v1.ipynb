{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example Neural Network - Multioutput Linear Regression-v1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CQXAD8TR7Gj",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFzBrCZiRndp",
        "colab_type": "text"
      },
      "source": [
        "Example of Multi Target Regression using Neural Network Regression\n",
        "\n",
        "      @Sijuade Oguntayo\n",
        "\n",
        "\n",
        "Copied from *@XavierTorres* - \n",
        "\n",
        "In this example we are trying to predict 2 continuous variables with 4 input features. Here we are not doing data cleaning, neither EDA, feature engineering nor fine-tuning model features. It's only a preliminary model overview.\n",
        "\n",
        "- Inputs: \n",
        "    - Land_sqf\n",
        "    - Gross_sqf\n",
        "    - Year_built\n",
        "    - NBH_level\t(Neighborhood level: 10= most upper class; 1: most lower class)\n",
        "    \n",
        "- Outputs: \n",
        "    - SALE_PRICE \t\n",
        "    - YEARLY_RENT\n",
        "\n",
        "We are going to try different approaches to predict each output (possible SALE_PRICE and possible YEARLY_RENT for a new property).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMSAyrl7RioM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed-iOgYySA1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('nyc-rolling-sales-clean-1.csv', sep=';')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqNuQsHuSCq8",
        "colab_type": "code",
        "outputId": "af66c060-7de1-4002-a6a7-a91e85092b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Land_sqf</th>\n",
              "      <th>Gross_sqf</th>\n",
              "      <th>Year_built</th>\n",
              "      <th>NBH_level</th>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <th>YEARLY_RENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2400</td>\n",
              "      <td>1552</td>\n",
              "      <td>1930</td>\n",
              "      <td>2</td>\n",
              "      <td>220485</td>\n",
              "      <td>10900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2742</td>\n",
              "      <td>1207</td>\n",
              "      <td>1925</td>\n",
              "      <td>2</td>\n",
              "      <td>223372</td>\n",
              "      <td>8100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5610</td>\n",
              "      <td>1320</td>\n",
              "      <td>1910</td>\n",
              "      <td>5</td>\n",
              "      <td>362981</td>\n",
              "      <td>19000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1758</td>\n",
              "      <td>1537</td>\n",
              "      <td>1910</td>\n",
              "      <td>5</td>\n",
              "      <td>245135</td>\n",
              "      <td>9700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1317</td>\n",
              "      <td>1339</td>\n",
              "      <td>1920</td>\n",
              "      <td>4</td>\n",
              "      <td>216477</td>\n",
              "      <td>12800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Land_sqf  Gross_sqf  Year_built  NBH_level  SALE_PRICE  YEARLY_RENT\n",
              "0      2400       1552        1930          2      220485        10900\n",
              "1      2742       1207        1925          2      223372         8100\n",
              "2      5610       1320        1910          5      362981        19000\n",
              "3      1758       1537        1910          5      245135         9700\n",
              "4      1317       1339        1920          4      216477        12800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql0tatV_SEVE",
        "colab_type": "code",
        "outputId": "6f0e91ad-3e46-4b16-cc14-d03f81b7b2df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "df.corr()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Land_sqf</th>\n",
              "      <th>Gross_sqf</th>\n",
              "      <th>Year_built</th>\n",
              "      <th>NBH_level</th>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <th>YEARLY_RENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Land_sqf</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.383712</td>\n",
              "      <td>-0.001142</td>\n",
              "      <td>0.030384</td>\n",
              "      <td>0.693293</td>\n",
              "      <td>0.534428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Gross_sqf</th>\n",
              "      <td>0.383712</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.061774</td>\n",
              "      <td>-0.025912</td>\n",
              "      <td>0.782791</td>\n",
              "      <td>0.536878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Year_built</th>\n",
              "      <td>-0.001142</td>\n",
              "      <td>-0.061774</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.030740</td>\n",
              "      <td>-0.004287</td>\n",
              "      <td>0.053032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NBH_level</th>\n",
              "      <td>0.030384</td>\n",
              "      <td>-0.025912</td>\n",
              "      <td>0.030740</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.426108</td>\n",
              "      <td>0.682104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <td>0.693293</td>\n",
              "      <td>0.782791</td>\n",
              "      <td>-0.004287</td>\n",
              "      <td>0.426108</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>YEARLY_RENT</th>\n",
              "      <td>0.534428</td>\n",
              "      <td>0.536878</td>\n",
              "      <td>0.053032</td>\n",
              "      <td>0.682104</td>\n",
              "      <td>0.864152</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Land_sqf  Gross_sqf  ...  SALE_PRICE  YEARLY_RENT\n",
              "Land_sqf     1.000000   0.383712  ...    0.693293     0.534428\n",
              "Gross_sqf    0.383712   1.000000  ...    0.782791     0.536878\n",
              "Year_built  -0.001142  -0.061774  ...   -0.004287     0.053032\n",
              "NBH_level    0.030384  -0.025912  ...    0.426108     0.682104\n",
              "SALE_PRICE   0.693293   0.782791  ...    1.000000     0.864152\n",
              "YEARLY_RENT  0.534428   0.536878  ...    0.864152     1.000000\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7gy7EVISGRT",
        "colab_type": "code",
        "outputId": "485a1973-1d71-42ba-a29a-a76d3913b1a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Land_sqf</th>\n",
              "      <th>Gross_sqf</th>\n",
              "      <th>Year_built</th>\n",
              "      <th>NBH_level</th>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <th>YEARLY_RENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>291.000000</td>\n",
              "      <td>291.000000</td>\n",
              "      <td>291.000000</td>\n",
              "      <td>291.000000</td>\n",
              "      <td>291.000000</td>\n",
              "      <td>291.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2645.676976</td>\n",
              "      <td>1639.773196</td>\n",
              "      <td>1938.378007</td>\n",
              "      <td>5.374570</td>\n",
              "      <td>287247.257732</td>\n",
              "      <td>18346.735395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1353.765596</td>\n",
              "      <td>810.233323</td>\n",
              "      <td>27.953929</td>\n",
              "      <td>2.892423</td>\n",
              "      <td>105082.283757</td>\n",
              "      <td>9435.111314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>353.000000</td>\n",
              "      <td>450.000000</td>\n",
              "      <td>1901.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>78522.000000</td>\n",
              "      <td>4300.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1900.000000</td>\n",
              "      <td>1114.000000</td>\n",
              "      <td>1920.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>216482.500000</td>\n",
              "      <td>10750.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2446.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1930.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>281208.000000</td>\n",
              "      <td>17300.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3000.000000</td>\n",
              "      <td>1960.000000</td>\n",
              "      <td>1950.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>332174.500000</td>\n",
              "      <td>24550.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>14384.000000</td>\n",
              "      <td>5303.000000</td>\n",
              "      <td>2015.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>961748.000000</td>\n",
              "      <td>81400.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Land_sqf    Gross_sqf  ...     SALE_PRICE   YEARLY_RENT\n",
              "count    291.000000   291.000000  ...     291.000000    291.000000\n",
              "mean    2645.676976  1639.773196  ...  287247.257732  18346.735395\n",
              "std     1353.765596   810.233323  ...  105082.283757   9435.111314\n",
              "min      353.000000   450.000000  ...   78522.000000   4300.000000\n",
              "25%     1900.000000  1114.000000  ...  216482.500000  10750.000000\n",
              "50%     2446.000000  1400.000000  ...  281208.000000  17300.000000\n",
              "75%     3000.000000  1960.000000  ...  332174.500000  24550.000000\n",
              "max    14384.000000  5303.000000  ...  961748.000000  81400.000000\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vTFjiG3SGTp",
        "colab_type": "code",
        "outputId": "18541ba0-ecc5-4b00-ff91-b458612e5f0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Land_sqf       int64\n",
              "Gross_sqf      int64\n",
              "Year_built     int64\n",
              "NBH_level      int64\n",
              "SALE_PRICE     int64\n",
              "YEARLY_RENT    int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evEoONQJSGaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.iloc[:,:4] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgPq5VSuSGfg",
        "colab_type": "code",
        "outputId": "72dea830-5086-4be0-dd6c-c7383976e0ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(291, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t-pRoVbSLYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assigning last 4 columns to Y, with the 2 dependant variables or 'outputs' or 'targets'\n",
        "Y = df.iloc[:,4:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3ofxG7qSLdM",
        "colab_type": "code",
        "outputId": "81171f1d-3c28-4e0e-80a5-9882de0be89e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "Y.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <th>YEARLY_RENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>220485</td>\n",
              "      <td>10900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>223372</td>\n",
              "      <td>8100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>362981</td>\n",
              "      <td>19000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>245135</td>\n",
              "      <td>9700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>216477</td>\n",
              "      <td>12800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SALE_PRICE  YEARLY_RENT\n",
              "0      220485        10900\n",
              "1      223372         8100\n",
              "2      362981        19000\n",
              "3      245135         9700\n",
              "4      216477        12800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GctuFnrMSLic",
        "colab_type": "code",
        "outputId": "005ea8e4-20de-455b-f0e5-cfc5cf5f0780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(291, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkCxnczTSLrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split train and test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,\n",
        "                                                    random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ__qQsmSL0D",
        "colab_type": "code",
        "outputId": "7caa850b-ffe5-4b36-8f7b-6c8f73038633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! pip install --upgrade tensorflow"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 42.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 58.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (45.2.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow-addons~=0.7.0, but you'll have tensorflow-addons 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALRwU0aASL3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn5LPfF1SoTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Custom Loss funtion - R2\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def coeff_determination(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRUG6PQttbfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atVa_FuPSLzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Architecture\n",
        "\n",
        "input_tensor = Input(shape=(4, ))\n",
        "hidden_layer_one = Dense(128)(input_tensor)\n",
        "hidden_layer_two = Dense(256)(hidden_layer_one)\n",
        "dropout_layer = Dropout(rate=0.3)(hidden_layer_two)\n",
        "hidden_layer_three = Dense(256)(dropout_layer)\n",
        "hidden_layer_four = Dense(256)(hidden_layer_three)\n",
        "output = Dense(2)(hidden_layer_four)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUYXkqx8SLyL",
        "colab_type": "code",
        "outputId": "6a03df02-b273-4616-ec3e-79b4353c1768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Model(input_tensor, output)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, Y_train, epochs=1030)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1030\n",
            "203/203 [==============================] - 3s 13ms/step - loss: 42959918871.9606\n",
            "Epoch 2/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 17826088305.4975\n",
            "Epoch 3/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 7865146456.2759\n",
            "Epoch 4/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 2730346579.2315\n",
            "Epoch 5/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 3338210403.6256\n",
            "Epoch 6/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1793927917.7143\n",
            "Epoch 7/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1614681292.9261\n",
            "Epoch 8/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 1483411154.6010\n",
            "Epoch 9/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 1403905169.0246\n",
            "Epoch 10/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1241991693.2414\n",
            "Epoch 11/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1294447146.8768\n",
            "Epoch 12/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 1249883667.2315\n",
            "Epoch 13/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1194527274.8768\n",
            "Epoch 14/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 1182915560.0394\n",
            "Epoch 15/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1147229988.5714\n",
            "Epoch 16/1030\n",
            "203/203 [==============================] - 0s 192us/step - loss: 1200289792.6305\n",
            "Epoch 17/1030\n",
            "203/203 [==============================] - 0s 183us/step - loss: 1303975075.9409\n",
            "Epoch 18/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 1229379235.9409\n",
            "Epoch 19/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1186149173.5961\n",
            "Epoch 20/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 1262242155.1921\n",
            "Epoch 21/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1107500676.4138\n",
            "Epoch 22/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 1302673425.6552\n",
            "Epoch 23/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 1207404359.2512\n",
            "Epoch 24/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 1226481221.6749\n",
            "Epoch 25/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1201249489.3399\n",
            "Epoch 26/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1143037910.3842\n",
            "Epoch 27/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1172017596.2167\n",
            "Epoch 28/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 1152803882.2463\n",
            "Epoch 29/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 1284295421.7931\n",
            "Epoch 30/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1224379700.9655\n",
            "Epoch 31/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1100136766.1084\n",
            "Epoch 32/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1133809337.3793\n",
            "Epoch 33/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 1152248311.8030\n",
            "Epoch 34/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 1300388142.6601\n",
            "Epoch 35/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1287989892.4138\n",
            "Epoch 36/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1294497015.8030\n",
            "Epoch 37/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 1084690403.6256\n",
            "Epoch 38/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1390651327.0542\n",
            "Epoch 39/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1236925458.2857\n",
            "Epoch 40/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1242572000.4729\n",
            "Epoch 41/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1357544400.0788\n",
            "Epoch 42/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 1194776850.6010\n",
            "Epoch 43/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 1279993067.8227\n",
            "Epoch 44/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1258996088.1182\n",
            "Epoch 45/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1192888060.8473\n",
            "Epoch 46/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 1210715438.8177\n",
            "Epoch 47/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1270889890.6798\n",
            "Epoch 48/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1269129753.8522\n",
            "Epoch 49/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1194459031.9606\n",
            "Epoch 50/1030\n",
            "203/203 [==============================] - 0s 191us/step - loss: 1124188349.9507\n",
            "Epoch 51/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1295707363.3103\n",
            "Epoch 52/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1195564238.8177\n",
            "Epoch 53/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1147604841.3005\n",
            "Epoch 54/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1180342994.7586\n",
            "Epoch 55/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1210677744.8670\n",
            "Epoch 56/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1150348762.1675\n",
            "Epoch 57/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 1144684703.8424\n",
            "Epoch 58/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 1237462592.6305\n",
            "Epoch 59/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1192228158.4236\n",
            "Epoch 60/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1218138726.1478\n",
            "Epoch 61/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1177400185.0640\n",
            "Epoch 62/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1251285677.0837\n",
            "Epoch 63/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1247283087.1330\n",
            "Epoch 64/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1257065235.2315\n",
            "Epoch 65/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1228722400.4729\n",
            "Epoch 66/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1251535155.0739\n",
            "Epoch 67/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 1164412111.4483\n",
            "Epoch 68/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 1256092453.2020\n",
            "Epoch 69/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1254781928.6700\n",
            "Epoch 70/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1263150196.6502\n",
            "Epoch 71/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 1254488914.6010\n",
            "Epoch 72/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1191106492.5320\n",
            "Epoch 73/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1239538394.1675\n",
            "Epoch 74/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1269609561.5369\n",
            "Epoch 75/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1193422866.9163\n",
            "Epoch 76/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1218311777.1034\n",
            "Epoch 77/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1163455396.8867\n",
            "Epoch 78/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1188704645.0443\n",
            "Epoch 79/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1140433169.9704\n",
            "Epoch 80/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1190527650.9951\n",
            "Epoch 81/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1197769126.1478\n",
            "Epoch 82/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1126793530.0099\n",
            "Epoch 83/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 1221174114.3645\n",
            "Epoch 84/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1175247338.2463\n",
            "Epoch 85/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 1191098901.4384\n",
            "Epoch 86/1030\n",
            "203/203 [==============================] - 0s 147us/step - loss: 1214532055.6453\n",
            "Epoch 87/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 1210745983.6847\n",
            "Epoch 88/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 1285173408.4729\n",
            "Epoch 89/1030\n",
            "203/203 [==============================] - 0s 184us/step - loss: 1227007864.7488\n",
            "Epoch 90/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 1143121737.7734\n",
            "Epoch 91/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1203241203.3892\n",
            "Epoch 92/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1323700517.8325\n",
            "Epoch 93/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1306878039.6453\n",
            "Epoch 94/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 1234567296.6305\n",
            "Epoch 95/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1111519080.6700\n",
            "Epoch 96/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1255995782.3054\n",
            "Epoch 97/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1228378865.4975\n",
            "Epoch 98/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1215997561.0640\n",
            "Epoch 99/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1126760508.8473\n",
            "Epoch 100/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1271267321.6946\n",
            "Epoch 101/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1183949023.5271\n",
            "Epoch 102/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1115266115.4680\n",
            "Epoch 103/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 1194893251.7833\n",
            "Epoch 104/1030\n",
            "203/203 [==============================] - 0s 181us/step - loss: 1099963316.9655\n",
            "Epoch 105/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1178964060.0591\n",
            "Epoch 106/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 1218728947.7044\n",
            "Epoch 107/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 1223566622.8966\n",
            "Epoch 108/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1200824261.3596\n",
            "Epoch 109/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1245252245.7537\n",
            "Epoch 110/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1218938656.1576\n",
            "Epoch 111/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 1308594196.1773\n",
            "Epoch 112/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 1197622937.2217\n",
            "Epoch 113/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1422095566.1872\n",
            "Epoch 114/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1306480853.7537\n",
            "Epoch 115/1030\n",
            "203/203 [==============================] - 0s 181us/step - loss: 1175143377.9704\n",
            "Epoch 116/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 1172579169.1034\n",
            "Epoch 117/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1313013245.4778\n",
            "Epoch 118/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1131342873.8522\n",
            "Epoch 119/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1193738754.5222\n",
            "Epoch 120/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1144367395.9409\n",
            "Epoch 121/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1184135780.8867\n",
            "Epoch 122/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1198291736.2759\n",
            "Epoch 123/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 1251978841.8522\n",
            "Epoch 124/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1158351256.5911\n",
            "Epoch 125/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 1132301208.5911\n",
            "Epoch 126/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1190765726.2660\n",
            "Epoch 127/1030\n",
            "203/203 [==============================] - 0s 188us/step - loss: 1128728839.5665\n",
            "Epoch 128/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1270957010.6010\n",
            "Epoch 129/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 1128085956.7291\n",
            "Epoch 130/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1202518812.3744\n",
            "Epoch 131/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1213987389.7931\n",
            "Epoch 132/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1131443338.7192\n",
            "Epoch 133/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1229886164.4926\n",
            "Epoch 134/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1178446494.8966\n",
            "Epoch 135/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 1342788914.4433\n",
            "Epoch 136/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1412619560.6700\n",
            "Epoch 137/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1287322912.7882\n",
            "Epoch 138/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1173965541.2020\n",
            "Epoch 139/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1134412128.4729\n",
            "Epoch 140/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1110509630.1084\n",
            "Epoch 141/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1261299902.7389\n",
            "Epoch 142/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1116721747.2315\n",
            "Epoch 143/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1213861455.4483\n",
            "Epoch 144/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 1259978288.8670\n",
            "Epoch 145/1030\n",
            "203/203 [==============================] - 0s 145us/step - loss: 1198209377.1034\n",
            "Epoch 146/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1227747674.7980\n",
            "Epoch 147/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1220466059.6650\n",
            "Epoch 148/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1100494679.6453\n",
            "Epoch 149/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1310256233.3005\n",
            "Epoch 150/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1457380662.8571\n",
            "Epoch 151/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1157388369.9704\n",
            "Epoch 152/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1162433261.0837\n",
            "Epoch 153/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1170798615.9606\n",
            "Epoch 154/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1093805837.2414\n",
            "Epoch 155/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 1144085260.2956\n",
            "Epoch 156/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 1164958417.9704\n",
            "Epoch 157/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 1253230548.3350\n",
            "Epoch 158/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1183880980.1773\n",
            "Epoch 159/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1157808394.0887\n",
            "Epoch 160/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1196589394.6010\n",
            "Epoch 161/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1328958822.1478\n",
            "Epoch 162/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1175525405.6355\n",
            "Epoch 163/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 1329098270.8966\n",
            "Epoch 164/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 1262964592.2365\n",
            "Epoch 165/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1184987472.7094\n",
            "Epoch 166/1030\n",
            "203/203 [==============================] - 0s 194us/step - loss: 1103193209.3793\n",
            "Epoch 167/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1150337812.8079\n",
            "Epoch 168/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1122081542.3054\n",
            "Epoch 169/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1200667397.9901\n",
            "Epoch 170/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 1239571782.6207\n",
            "Epoch 171/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1188630375.4089\n",
            "Epoch 172/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1100066393.5369\n",
            "Epoch 173/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1100654270.4236\n",
            "Epoch 174/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1126020929.5764\n",
            "Epoch 175/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1140286031.7635\n",
            "Epoch 176/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1278806769.4975\n",
            "Epoch 177/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1314031774.2660\n",
            "Epoch 178/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1140581733.5172\n",
            "Epoch 179/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1188907047.0936\n",
            "Epoch 180/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 1075772237.8719\n",
            "Epoch 181/1030\n",
            "203/203 [==============================] - 0s 225us/step - loss: 1119427086.5025\n",
            "Epoch 182/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1296782314.5616\n",
            "Epoch 183/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 1302369743.4483\n",
            "Epoch 184/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 1052648967.8818\n",
            "Epoch 185/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 1312017616.0788\n",
            "Epoch 186/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 1244357463.6453\n",
            "Epoch 187/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1164076662.8571\n",
            "Epoch 188/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1136804736.3153\n",
            "Epoch 189/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1088352931.9409\n",
            "Epoch 190/1030\n",
            "203/203 [==============================] - 0s 184us/step - loss: 1215467042.0493\n",
            "Epoch 191/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1039467962.6404\n",
            "Epoch 192/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1097003245.3990\n",
            "Epoch 193/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1091933797.5172\n",
            "Epoch 194/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 1114540651.8227\n",
            "Epoch 195/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1080708227.7833\n",
            "Epoch 196/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1139134441.3005\n",
            "Epoch 197/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 1045819429.2020\n",
            "Epoch 198/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1116836847.6059\n",
            "Epoch 199/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1029857856.9458\n",
            "Epoch 200/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1062611926.3842\n",
            "Epoch 201/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1130960449.5764\n",
            "Epoch 202/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1030903897.8522\n",
            "Epoch 203/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 1047228223.0542\n",
            "Epoch 204/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 1069953093.9901\n",
            "Epoch 205/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 1134971426.3645\n",
            "Epoch 206/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1010867829.9113\n",
            "Epoch 207/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1156143547.2709\n",
            "Epoch 208/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1236755884.7685\n",
            "Epoch 209/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 1211096394.4039\n",
            "Epoch 210/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1098490972.6897\n",
            "Epoch 211/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 1059361446.4631\n",
            "Epoch 212/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1228258589.6355\n",
            "Epoch 213/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 1277307709.7931\n",
            "Epoch 214/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 1100999494.6207\n",
            "Epoch 215/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1107944294.7783\n",
            "Epoch 216/1030\n",
            "203/203 [==============================] - 0s 147us/step - loss: 1156743690.0887\n",
            "Epoch 217/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 1167803807.8424\n",
            "Epoch 218/1030\n",
            "203/203 [==============================] - 0s 145us/step - loss: 1172209457.4975\n",
            "Epoch 219/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1054925729.4187\n",
            "Epoch 220/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1006752520.5123\n",
            "Epoch 221/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1054667364.2562\n",
            "Epoch 222/1030\n",
            "203/203 [==============================] - 0s 191us/step - loss: 1153562303.0542\n",
            "Epoch 223/1030\n",
            "203/203 [==============================] - 0s 191us/step - loss: 1154399260.3744\n",
            "Epoch 224/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 1110730661.8325\n",
            "Epoch 225/1030\n",
            "203/203 [==============================] - 0s 185us/step - loss: 1094050780.6897\n",
            "Epoch 226/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1081764240.3941\n",
            "Epoch 227/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 1127295670.2266\n",
            "Epoch 228/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 1194587009.5764\n",
            "Epoch 229/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1073683379.7044\n",
            "Epoch 230/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1086238436.5714\n",
            "Epoch 231/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1039422765.0837\n",
            "Epoch 232/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 1257930838.3842\n",
            "Epoch 233/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1052865452.7685\n",
            "Epoch 234/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 1035131857.6552\n",
            "Epoch 235/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 921916933.9901\n",
            "Epoch 236/1030\n",
            "203/203 [==============================] - 0s 182us/step - loss: 1233404628.8079\n",
            "Epoch 237/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1083572689.3399\n",
            "Epoch 238/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1050877636.0985\n",
            "Epoch 239/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 1112906797.3990\n",
            "Epoch 240/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1161896551.4089\n",
            "Epoch 241/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 999673180.6897\n",
            "Epoch 242/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1008705376.4729\n",
            "Epoch 243/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1032656087.0148\n",
            "Epoch 244/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1058521232.3941\n",
            "Epoch 245/1030\n",
            "203/203 [==============================] - 0s 148us/step - loss: 1082697711.6059\n",
            "Epoch 246/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1015610927.2906\n",
            "Epoch 247/1030\n",
            "203/203 [==============================] - 0s 181us/step - loss: 991787165.9507\n",
            "Epoch 248/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1068145275.5862\n",
            "Epoch 249/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1084643925.1232\n",
            "Epoch 250/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1002059649.8916\n",
            "Epoch 251/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 934256625.1823\n",
            "Epoch 252/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 979962413.5567\n",
            "Epoch 253/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 953079476.3350\n",
            "Epoch 254/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1067821151.3695\n",
            "Epoch 255/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 942631314.9163\n",
            "Epoch 256/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 902072617.4581\n",
            "Epoch 257/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 1066230607.4483\n",
            "Epoch 258/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 995753434.1675\n",
            "Epoch 259/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 855511190.0690\n",
            "Epoch 260/1030\n",
            "203/203 [==============================] - 0s 187us/step - loss: 1005173866.5616\n",
            "Epoch 261/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 896432870.9360\n",
            "Epoch 262/1030\n",
            "203/203 [==============================] - 0s 185us/step - loss: 885557137.3399\n",
            "Epoch 263/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1001009839.9212\n",
            "Epoch 264/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 848801684.8079\n",
            "Epoch 265/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 914215214.6601\n",
            "Epoch 266/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 866454985.1429\n",
            "Epoch 267/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1119852443.7438\n",
            "Epoch 268/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 939256864.7882\n",
            "Epoch 269/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 999822195.7044\n",
            "Epoch 270/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 874549247.0542\n",
            "Epoch 271/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 782244736.9458\n",
            "Epoch 272/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 843533610.2463\n",
            "Epoch 273/1030\n",
            "203/203 [==============================] - 0s 185us/step - loss: 886474070.6995\n",
            "Epoch 274/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 882533356.7685\n",
            "Epoch 275/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 885513125.5172\n",
            "Epoch 276/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 770158657.2611\n",
            "Epoch 277/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 748648547.9409\n",
            "Epoch 278/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1132467760.5517\n",
            "Epoch 279/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1020494431.8424\n",
            "Epoch 280/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 931028573.9507\n",
            "Epoch 281/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 983308730.6404\n",
            "Epoch 282/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 811819813.5172\n",
            "Epoch 283/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 846082087.0936\n",
            "Epoch 284/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 871141474.6798\n",
            "Epoch 285/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 704493892.7291\n",
            "Epoch 286/1030\n",
            "203/203 [==============================] - 0s 148us/step - loss: 735238895.6059\n",
            "Epoch 287/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 822957518.1872\n",
            "Epoch 288/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1025992230.4631\n",
            "Epoch 289/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 856449434.4828\n",
            "Epoch 290/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 986584053.9113\n",
            "Epoch 291/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 891835328.0000\n",
            "Epoch 292/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 760278555.7438\n",
            "Epoch 293/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 810971595.9803\n",
            "Epoch 294/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 693574196.9655\n",
            "Epoch 295/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 680287101.4778\n",
            "Epoch 296/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 713344847.7635\n",
            "Epoch 297/1030\n",
            "203/203 [==============================] - 0s 184us/step - loss: 694044746.0887\n",
            "Epoch 298/1030\n",
            "203/203 [==============================] - 0s 226us/step - loss: 755581759.3695\n",
            "Epoch 299/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 667797162.5616\n",
            "Epoch 300/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 538010497.7340\n",
            "Epoch 301/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 577744763.5862\n",
            "Epoch 302/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 492985898.7192\n",
            "Epoch 303/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 513855292.0591\n",
            "Epoch 304/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 519475176.5123\n",
            "Epoch 305/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 604622710.5419\n",
            "Epoch 306/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 576732943.1330\n",
            "Epoch 307/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 457969575.2512\n",
            "Epoch 308/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 434877038.5813\n",
            "Epoch 309/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 565744629.9113\n",
            "Epoch 310/1030\n",
            "203/203 [==============================] - 0s 145us/step - loss: 478811546.1675\n",
            "Epoch 311/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 699223763.8621\n",
            "Epoch 312/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 707590216.1970\n",
            "Epoch 313/1030\n",
            "203/203 [==============================] - 0s 183us/step - loss: 465914449.1034\n",
            "Epoch 314/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 736812766.1084\n",
            "Epoch 315/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 788063957.9113\n",
            "Epoch 316/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 438262390.5419\n",
            "Epoch 317/1030\n",
            "203/203 [==============================] - 0s 146us/step - loss: 461313333.2808\n",
            "Epoch 318/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 582437154.6798\n",
            "Epoch 319/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 951742397.1626\n",
            "Epoch 320/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 622124154.0099\n",
            "Epoch 321/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 539517027.3103\n",
            "Epoch 322/1030\n",
            "203/203 [==============================] - 0s 145us/step - loss: 666845462.6995\n",
            "Epoch 323/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 704697342.1084\n",
            "Epoch 324/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 634341805.0837\n",
            "Epoch 325/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 635695931.9015\n",
            "Epoch 326/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 695560002.2069\n",
            "Epoch 327/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 425660847.9212\n",
            "Epoch 328/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 352189808.2365\n",
            "Epoch 329/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 367415948.4532\n",
            "Epoch 330/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 359727434.0887\n",
            "Epoch 331/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 546305669.6749\n",
            "Epoch 332/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 402487953.6552\n",
            "Epoch 333/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 310185883.9803\n",
            "Epoch 334/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 381789415.4877\n",
            "Epoch 335/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 296893699.4680\n",
            "Epoch 336/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 288535423.2906\n",
            "Epoch 337/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 297317946.4828\n",
            "Epoch 338/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 292078776.1970\n",
            "Epoch 339/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 349700999.7241\n",
            "Epoch 340/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 404262158.0296\n",
            "Epoch 341/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 392056788.3350\n",
            "Epoch 342/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 463536317.4778\n",
            "Epoch 343/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 303386710.3842\n",
            "Epoch 344/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 396847227.4286\n",
            "Epoch 345/1030\n",
            "203/203 [==============================] - 0s 206us/step - loss: 386531033.2217\n",
            "Epoch 346/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 361459370.2463\n",
            "Epoch 347/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 388225771.3498\n",
            "Epoch 348/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 295108141.0837\n",
            "Epoch 349/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 345273775.1330\n",
            "Epoch 350/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 343037212.8473\n",
            "Epoch 351/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 387975624.3547\n",
            "Epoch 352/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 272058171.0345\n",
            "Epoch 353/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 264828523.4286\n",
            "Epoch 354/1030\n",
            "203/203 [==============================] - 0s 181us/step - loss: 394350141.1626\n",
            "Epoch 355/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 343962571.0345\n",
            "Epoch 356/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 323522984.1970\n",
            "Epoch 357/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 355600947.0739\n",
            "Epoch 358/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 348517297.5764\n",
            "Epoch 359/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 490550801.6552\n",
            "Epoch 360/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 453507744.1576\n",
            "Epoch 361/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 311091667.7833\n",
            "Epoch 362/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 232774088.9852\n",
            "Epoch 363/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 301397060.0985\n",
            "Epoch 364/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 262615520.1576\n",
            "Epoch 365/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 335483218.9163\n",
            "Epoch 366/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 303279749.9113\n",
            "Epoch 367/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 282560859.7438\n",
            "Epoch 368/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 322424198.3054\n",
            "Epoch 369/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 300776486.8571\n",
            "Epoch 370/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 243069499.6650\n",
            "Epoch 371/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 265205716.5714\n",
            "Epoch 372/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 248741133.3202\n",
            "Epoch 373/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 231374595.5468\n",
            "Epoch 374/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 250710190.5025\n",
            "Epoch 375/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 305466965.4384\n",
            "Epoch 376/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 303454978.5222\n",
            "Epoch 377/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 377099635.3892\n",
            "Epoch 378/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 397501542.3054\n",
            "Epoch 379/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 350561912.9064\n",
            "Epoch 380/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 232061231.9212\n",
            "Epoch 381/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 291785254.8571\n",
            "Epoch 382/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 342038952.0394\n",
            "Epoch 383/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 330177652.3350\n",
            "Epoch 384/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 278398053.5172\n",
            "Epoch 385/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 290863187.3103\n",
            "Epoch 386/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 254176813.8719\n",
            "Epoch 387/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 277155853.2414\n",
            "Epoch 388/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 341003819.3498\n",
            "Epoch 389/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 318362290.2857\n",
            "Epoch 390/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 431458937.0640\n",
            "Epoch 391/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 288899875.2315\n",
            "Epoch 392/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 290458924.3744\n",
            "Epoch 393/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 280892407.4089\n",
            "Epoch 394/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 211500029.8719\n",
            "Epoch 395/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 264025778.7586\n",
            "Epoch 396/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 322740295.8030\n",
            "Epoch 397/1030\n",
            "203/203 [==============================] - 0s 184us/step - loss: 292902686.7389\n",
            "Epoch 398/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 300501027.6256\n",
            "Epoch 399/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 272189904.4729\n",
            "Epoch 400/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 234402247.5665\n",
            "Epoch 401/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 252389505.5764\n",
            "Epoch 402/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 263401361.1034\n",
            "Epoch 403/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 306979712.8670\n",
            "Epoch 404/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 278182995.8621\n",
            "Epoch 405/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 374110675.5468\n",
            "Epoch 406/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 334219671.5665\n",
            "Epoch 407/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 438726153.2217\n",
            "Epoch 408/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 407621631.4483\n",
            "Epoch 409/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 377574043.1133\n",
            "Epoch 410/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 368460450.9951\n",
            "Epoch 411/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 449062556.1379\n",
            "Epoch 412/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 442134684.4532\n",
            "Epoch 413/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 479998542.3448\n",
            "Epoch 414/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 356584010.2463\n",
            "Epoch 415/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 325021841.4975\n",
            "Epoch 416/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 353841140.3350\n",
            "Epoch 417/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 304876510.1872\n",
            "Epoch 418/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 272233440.1576\n",
            "Epoch 419/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 329571059.0739\n",
            "Epoch 420/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 288197295.5271\n",
            "Epoch 421/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 281748796.0591\n",
            "Epoch 422/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 265311350.1478\n",
            "Epoch 423/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 340542523.5862\n",
            "Epoch 424/1030\n",
            "203/203 [==============================] - 0s 188us/step - loss: 222196297.6158\n",
            "Epoch 425/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 337949273.8522\n",
            "Epoch 426/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 526390667.8227\n",
            "Epoch 427/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 484315891.8621\n",
            "Epoch 428/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 426964336.8670\n",
            "Epoch 429/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 500108842.2463\n",
            "Epoch 430/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 529744505.6946\n",
            "Epoch 431/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 408435843.1527\n",
            "Epoch 432/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 378542793.5369\n",
            "Epoch 433/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 292047043.9409\n",
            "Epoch 434/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 363660653.5567\n",
            "Epoch 435/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 305776313.1429\n",
            "Epoch 436/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 243571756.7685\n",
            "Epoch 437/1030\n",
            "203/203 [==============================] - 0s 188us/step - loss: 244233346.2069\n",
            "Epoch 438/1030\n",
            "203/203 [==============================] - 0s 187us/step - loss: 285500196.7291\n",
            "Epoch 439/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 376333189.8325\n",
            "Epoch 440/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 254399462.9360\n",
            "Epoch 441/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 256614568.1182\n",
            "Epoch 442/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 248139776.3941\n",
            "Epoch 443/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 241629731.3103\n",
            "Epoch 444/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 214712323.9803\n",
            "Epoch 445/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 210548836.3350\n",
            "Epoch 446/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 239428750.2660\n",
            "Epoch 447/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 268353854.4236\n",
            "Epoch 448/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 277313071.2512\n",
            "Epoch 449/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 354204670.1872\n",
            "Epoch 450/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 269694978.2069\n",
            "Epoch 451/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 232932691.9409\n",
            "Epoch 452/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 247868410.4236\n",
            "Epoch 453/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 295166499.6256\n",
            "Epoch 454/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 223617704.7488\n",
            "Epoch 455/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 231242833.5369\n",
            "Epoch 456/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 261918400.6305\n",
            "Epoch 457/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 364944601.0640\n",
            "Epoch 458/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 536182463.5271\n",
            "Epoch 459/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 430837060.0197\n",
            "Epoch 460/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 395448944.2365\n",
            "Epoch 461/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 491870152.5123\n",
            "Epoch 462/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 572254682.1675\n",
            "Epoch 463/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 295984060.7685\n",
            "Epoch 464/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 328545787.4286\n",
            "Epoch 465/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 249319531.6650\n",
            "Epoch 466/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 243080287.0542\n",
            "Epoch 467/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 358254401.9704\n",
            "Epoch 468/1030\n",
            "203/203 [==============================] - 0s 148us/step - loss: 279217805.6355\n",
            "Epoch 469/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 277637659.7438\n",
            "Epoch 470/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 270735163.7438\n",
            "Epoch 471/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 270349495.1724\n",
            "Epoch 472/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 250453892.5714\n",
            "Epoch 473/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 245636038.8571\n",
            "Epoch 474/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 261959854.6601\n",
            "Epoch 475/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 358290583.3300\n",
            "Epoch 476/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 256139904.7882\n",
            "Epoch 477/1030\n",
            "203/203 [==============================] - 0s 148us/step - loss: 249407669.5961\n",
            "Epoch 478/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 278576454.6207\n",
            "Epoch 479/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 315647399.2512\n",
            "Epoch 480/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 271943990.3054\n",
            "Epoch 481/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 341834345.6158\n",
            "Epoch 482/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 343963405.7143\n",
            "Epoch 483/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 277840208.3941\n",
            "Epoch 484/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 272778519.8030\n",
            "Epoch 485/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 344373013.1232\n",
            "Epoch 486/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 291857460.6502\n",
            "Epoch 487/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 224003759.6256\n",
            "Epoch 488/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 300608310.2266\n",
            "Epoch 489/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 374585098.1675\n",
            "Epoch 490/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 324473998.0296\n",
            "Epoch 491/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 226157754.9557\n",
            "Epoch 492/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 250578002.0887\n",
            "Epoch 493/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 253383942.3054\n",
            "Epoch 494/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 249596505.6552\n",
            "Epoch 495/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 294500091.1133\n",
            "Epoch 496/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 307311388.6108\n",
            "Epoch 497/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 252628585.6158\n",
            "Epoch 498/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 329159812.7291\n",
            "Epoch 499/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 488342453.1232\n",
            "Epoch 500/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 513239531.1921\n",
            "Epoch 501/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 349851153.6552\n",
            "Epoch 502/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 249842064.5517\n",
            "Epoch 503/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 222068379.4286\n",
            "Epoch 504/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 245464256.1576\n",
            "Epoch 505/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 238735209.8522\n",
            "Epoch 506/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 339875819.8227\n",
            "Epoch 507/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 362559469.3202\n",
            "Epoch 508/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 347765013.5961\n",
            "Epoch 509/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 317900275.3103\n",
            "Epoch 510/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 293193884.7685\n",
            "Epoch 511/1030\n",
            "203/203 [==============================] - 0s 183us/step - loss: 255255805.0837\n",
            "Epoch 512/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 225796317.1626\n",
            "Epoch 513/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 260028537.3793\n",
            "Epoch 514/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 249555217.4975\n",
            "Epoch 515/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 305089179.5862\n",
            "Epoch 516/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 226695671.7241\n",
            "Epoch 517/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 242856974.1084\n",
            "Epoch 518/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 233700403.0739\n",
            "Epoch 519/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 278767809.8916\n",
            "Epoch 520/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 280331730.6010\n",
            "Epoch 521/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 230868298.4828\n",
            "Epoch 522/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 252149013.2020\n",
            "Epoch 523/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 272357261.7143\n",
            "Epoch 524/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 244495102.7389\n",
            "Epoch 525/1030\n",
            "203/203 [==============================] - 0s 183us/step - loss: 242095394.6798\n",
            "Epoch 526/1030\n",
            "203/203 [==============================] - 0s 181us/step - loss: 265178277.0443\n",
            "Epoch 527/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 231130856.3547\n",
            "Epoch 528/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 235942544.7094\n",
            "Epoch 529/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 284211023.7635\n",
            "Epoch 530/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 349503980.2167\n",
            "Epoch 531/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 345110157.3202\n",
            "Epoch 532/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 364146726.6207\n",
            "Epoch 533/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 232553666.6010\n",
            "Epoch 534/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 228911725.3990\n",
            "Epoch 535/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 216918022.9360\n",
            "Epoch 536/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 279435759.5271\n",
            "Epoch 537/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 250461391.1330\n",
            "Epoch 538/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 250791842.6798\n",
            "Epoch 539/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 461174422.3842\n",
            "Epoch 540/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 412202683.6650\n",
            "Epoch 541/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 258951748.0197\n",
            "Epoch 542/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 274654079.0542\n",
            "Epoch 543/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 299718171.2709\n",
            "Epoch 544/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 200864111.0542\n",
            "Epoch 545/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 282077123.6256\n",
            "Epoch 546/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 408124156.3744\n",
            "Epoch 547/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 272065054.4236\n",
            "Epoch 548/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 266894216.5911\n",
            "Epoch 549/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 247880605.0049\n",
            "Epoch 550/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 280896435.3892\n",
            "Epoch 551/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 467322620.0591\n",
            "Epoch 552/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 442437031.7241\n",
            "Epoch 553/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 470310025.3793\n",
            "Epoch 554/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 360433816.2759\n",
            "Epoch 555/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 678172715.8227\n",
            "Epoch 556/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 364337054.4236\n",
            "Epoch 557/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 374912634.9557\n",
            "Epoch 558/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 603596204.4532\n",
            "Epoch 559/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 416376078.5025\n",
            "Epoch 560/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 553403249.6552\n",
            "Epoch 561/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 395358956.7685\n",
            "Epoch 562/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 372760852.6502\n",
            "Epoch 563/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 362687457.7340\n",
            "Epoch 564/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 353149515.1921\n",
            "Epoch 565/1030\n",
            "203/203 [==============================] - 0s 185us/step - loss: 316125134.5025\n",
            "Epoch 566/1030\n",
            "203/203 [==============================] - 0s 183us/step - loss: 322598621.4778\n",
            "Epoch 567/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 327982606.8177\n",
            "Epoch 568/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 308287376.7882\n",
            "Epoch 569/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 334455991.4877\n",
            "Epoch 570/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 414358360.9064\n",
            "Epoch 571/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 253574304.0000\n",
            "Epoch 572/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 225207709.8719\n",
            "Epoch 573/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 210186953.3793\n",
            "Epoch 574/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 234351141.6749\n",
            "Epoch 575/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 315825355.2709\n",
            "Epoch 576/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 293922077.7931\n",
            "Epoch 577/1030\n",
            "203/203 [==============================] - 0s 183us/step - loss: 302059782.5419\n",
            "Epoch 578/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 242772676.7291\n",
            "Epoch 579/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 203732214.0690\n",
            "Epoch 580/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 251514231.3300\n",
            "Epoch 581/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 292335804.8473\n",
            "Epoch 582/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 358447910.7783\n",
            "Epoch 583/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 346987866.5616\n",
            "Epoch 584/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 313657586.0099\n",
            "Epoch 585/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 279117286.1478\n",
            "Epoch 586/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 262921394.6010\n",
            "Epoch 587/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 233648940.2956\n",
            "Epoch 588/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 237344091.7438\n",
            "Epoch 589/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 259822655.3695\n",
            "Epoch 590/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 259670732.9261\n",
            "Epoch 591/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 260257183.2118\n",
            "Epoch 592/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 357933634.2069\n",
            "Epoch 593/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 457351031.7241\n",
            "Epoch 594/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 472772576.5517\n",
            "Epoch 595/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 335548474.1675\n",
            "Epoch 596/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 389391210.2463\n",
            "Epoch 597/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 367514064.2365\n",
            "Epoch 598/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 317586962.2857\n",
            "Epoch 599/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 293449122.3645\n",
            "Epoch 600/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 298697860.4138\n",
            "Epoch 601/1030\n",
            "203/203 [==============================] - 0s 202us/step - loss: 258544681.2217\n",
            "Epoch 602/1030\n",
            "203/203 [==============================] - 0s 240us/step - loss: 257871524.3350\n",
            "Epoch 603/1030\n",
            "203/203 [==============================] - 0s 191us/step - loss: 256955181.0837\n",
            "Epoch 604/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 277064974.1872\n",
            "Epoch 605/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 211836541.2414\n",
            "Epoch 606/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 239402152.1970\n",
            "Epoch 607/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 272828915.3892\n",
            "Epoch 608/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 271061363.7044\n",
            "Epoch 609/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 291454984.3547\n",
            "Epoch 610/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 251811587.3103\n",
            "Epoch 611/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 277717468.6108\n",
            "Epoch 612/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 331412399.1330\n",
            "Epoch 613/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 231047226.6404\n",
            "Epoch 614/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 227233108.3744\n",
            "Epoch 615/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 250101324.4532\n",
            "Epoch 616/1030\n",
            "203/203 [==============================] - 0s 185us/step - loss: 230766728.3547\n",
            "Epoch 617/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 199196240.7882\n",
            "Epoch 618/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 290397806.1872\n",
            "Epoch 619/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 357358802.9163\n",
            "Epoch 620/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 256853660.4532\n",
            "Epoch 621/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 236928988.5320\n",
            "Epoch 622/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 236317557.5961\n",
            "Epoch 623/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 225179580.9261\n",
            "Epoch 624/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 279345226.6010\n",
            "Epoch 625/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 250403197.9507\n",
            "Epoch 626/1030\n",
            "203/203 [==============================] - 0s 145us/step - loss: 243543438.8177\n",
            "Epoch 627/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 256033472.9064\n",
            "Epoch 628/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 262841814.0690\n",
            "Epoch 629/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 225989422.4236\n",
            "Epoch 630/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 234433219.5468\n",
            "Epoch 631/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 194756280.0394\n",
            "Epoch 632/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 241674038.6995\n",
            "Epoch 633/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 248700705.8916\n",
            "Epoch 634/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 250956307.3892\n",
            "Epoch 635/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 222726379.0345\n",
            "Epoch 636/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 251076949.6749\n",
            "Epoch 637/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 198480266.6404\n",
            "Epoch 638/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 291611435.3498\n",
            "Epoch 639/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 310577330.1281\n",
            "Epoch 640/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 318256687.7635\n",
            "Epoch 641/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 327800932.8867\n",
            "Epoch 642/1030\n",
            "203/203 [==============================] - 0s 193us/step - loss: 314906082.9163\n",
            "Epoch 643/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 659945313.8916\n",
            "Epoch 644/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 402736612.4926\n",
            "Epoch 645/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 299265534.7389\n",
            "Epoch 646/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 294358596.4138\n",
            "Epoch 647/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 226790148.0197\n",
            "Epoch 648/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 224712788.1773\n",
            "Epoch 649/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 293660591.9212\n",
            "Epoch 650/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 312984088.5911\n",
            "Epoch 651/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 296124736.9458\n",
            "Epoch 652/1030\n",
            "203/203 [==============================] - 0s 184us/step - loss: 276464657.4975\n",
            "Epoch 653/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 288375913.5369\n",
            "Epoch 654/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 259234948.8079\n",
            "Epoch 655/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 251920264.9064\n",
            "Epoch 656/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 233907182.9754\n",
            "Epoch 657/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 248907520.2365\n",
            "Epoch 658/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 276952198.4631\n",
            "Epoch 659/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 199757392.3153\n",
            "Epoch 660/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 248747962.7980\n",
            "Epoch 661/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 225716395.8227\n",
            "Epoch 662/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 294400004.5320\n",
            "Epoch 663/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 341543030.0690\n",
            "Epoch 664/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 251627958.7783\n",
            "Epoch 665/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 238286506.7783\n",
            "Epoch 666/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 215214164.0985\n",
            "Epoch 667/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 278691644.8473\n",
            "Epoch 668/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 263065629.4778\n",
            "Epoch 669/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 362089866.0887\n",
            "Epoch 670/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 275789555.1527\n",
            "Epoch 671/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 311277222.6207\n",
            "Epoch 672/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 344094068.0985\n",
            "Epoch 673/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 284123469.0837\n",
            "Epoch 674/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 266181525.9113\n",
            "Epoch 675/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 274218675.0739\n",
            "Epoch 676/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 239818677.5172\n",
            "Epoch 677/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 234233229.6355\n",
            "Epoch 678/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 211133089.2611\n",
            "Epoch 679/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 260804119.3300\n",
            "Epoch 680/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 388734608.9458\n",
            "Epoch 681/1030\n",
            "203/203 [==============================] - 0s 196us/step - loss: 368825573.5172\n",
            "Epoch 682/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 353725182.7389\n",
            "Epoch 683/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 331920967.0148\n",
            "Epoch 684/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 249163209.6552\n",
            "Epoch 685/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 234166590.8966\n",
            "Epoch 686/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 211994208.4729\n",
            "Epoch 687/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 255084278.2266\n",
            "Epoch 688/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 266133362.4433\n",
            "Epoch 689/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 447569543.8030\n",
            "Epoch 690/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 503193052.6897\n",
            "Epoch 691/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 425608774.5025\n",
            "Epoch 692/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 290542107.9015\n",
            "Epoch 693/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 227587231.2118\n",
            "Epoch 694/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 250757137.9704\n",
            "Epoch 695/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 196119474.9163\n",
            "Epoch 696/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 228392327.8818\n",
            "Epoch 697/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 268400608.1576\n",
            "Epoch 698/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 409328195.7833\n",
            "Epoch 699/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 306059055.1330\n",
            "Epoch 700/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 259089910.3842\n",
            "Epoch 701/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 374186745.6946\n",
            "Epoch 702/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 244200771.9409\n",
            "Epoch 703/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 235696469.3202\n",
            "Epoch 704/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 358151850.1675\n",
            "Epoch 705/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 270085895.9606\n",
            "Epoch 706/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 231513893.2020\n",
            "Epoch 707/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 230898907.9015\n",
            "Epoch 708/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 234729518.9754\n",
            "Epoch 709/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 232901649.9704\n",
            "Epoch 710/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 226885798.6995\n",
            "Epoch 711/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 243960770.4433\n",
            "Epoch 712/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 267167109.2020\n",
            "Epoch 713/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 200466212.9655\n",
            "Epoch 714/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 253440116.3350\n",
            "Epoch 715/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 230070852.7291\n",
            "Epoch 716/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 320519980.2167\n",
            "Epoch 717/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 256874719.2118\n",
            "Epoch 718/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 223745679.3695\n",
            "Epoch 719/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 249038996.4138\n",
            "Epoch 720/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 262399506.2069\n",
            "Epoch 721/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 271310073.5369\n",
            "Epoch 722/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 406780963.6256\n",
            "Epoch 723/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 345151287.2512\n",
            "Epoch 724/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 229387781.0443\n",
            "Epoch 725/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 294960356.4926\n",
            "Epoch 726/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 287438503.2512\n",
            "Epoch 727/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 287601305.3793\n",
            "Epoch 728/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 247061153.8916\n",
            "Epoch 729/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 274878234.1675\n",
            "Epoch 730/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 268919083.8227\n",
            "Epoch 731/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 266762336.8670\n",
            "Epoch 732/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 221074852.8867\n",
            "Epoch 733/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 234047502.5025\n",
            "Epoch 734/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 269168194.3251\n",
            "Epoch 735/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 311930622.8966\n",
            "Epoch 736/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 259736162.2857\n",
            "Epoch 737/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 194292065.9704\n",
            "Epoch 738/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 234707538.9163\n",
            "Epoch 739/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 240813953.0246\n",
            "Epoch 740/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 232175021.6355\n",
            "Epoch 741/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 237223844.5714\n",
            "Epoch 742/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 235234618.9557\n",
            "Epoch 743/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 250804825.2217\n",
            "Epoch 744/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 308645464.2759\n",
            "Epoch 745/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 416698882.1281\n",
            "Epoch 746/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 281767001.3793\n",
            "Epoch 747/1030\n",
            "203/203 [==============================] - 0s 181us/step - loss: 250671270.6207\n",
            "Epoch 748/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 309571541.9113\n",
            "Epoch 749/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 348563856.0788\n",
            "Epoch 750/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 288324638.3448\n",
            "Epoch 751/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 266985634.6798\n",
            "Epoch 752/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 239495718.3054\n",
            "Epoch 753/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 281488716.9261\n",
            "Epoch 754/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 301236184.5123\n",
            "Epoch 755/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 244304268.1379\n",
            "Epoch 756/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 258085137.1823\n",
            "Epoch 757/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 258429203.3892\n",
            "Epoch 758/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 271482660.1773\n",
            "Epoch 759/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 200320993.1034\n",
            "Epoch 760/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 239287306.7192\n",
            "Epoch 761/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 227596618.3251\n",
            "Epoch 762/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 220807927.0148\n",
            "Epoch 763/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 224094547.2315\n",
            "Epoch 764/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 218991094.3054\n",
            "Epoch 765/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 242414502.5419\n",
            "Epoch 766/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 272379287.1724\n",
            "Epoch 767/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 322919988.1773\n",
            "Epoch 768/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 380315774.1084\n",
            "Epoch 769/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 352061269.4384\n",
            "Epoch 770/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 313280674.7586\n",
            "Epoch 771/1030\n",
            "203/203 [==============================] - 0s 189us/step - loss: 330516277.2808\n",
            "Epoch 772/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 438724969.4581\n",
            "Epoch 773/1030\n",
            "203/203 [==============================] - 0s 187us/step - loss: 297833921.5764\n",
            "Epoch 774/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 253699041.5764\n",
            "Epoch 775/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 219896360.8276\n",
            "Epoch 776/1030\n",
            "203/203 [==============================] - 0s 186us/step - loss: 273504917.4384\n",
            "Epoch 777/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 236633595.7438\n",
            "Epoch 778/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 244050824.8276\n",
            "Epoch 779/1030\n",
            "203/203 [==============================] - 0s 147us/step - loss: 307846762.0887\n",
            "Epoch 780/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 241337401.2611\n",
            "Epoch 781/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 264436167.2512\n",
            "Epoch 782/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 275473188.2562\n",
            "Epoch 783/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 266169820.9261\n",
            "Epoch 784/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 308922611.0739\n",
            "Epoch 785/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 376248060.5320\n",
            "Epoch 786/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 285605341.5567\n",
            "Epoch 787/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 387065898.2463\n",
            "Epoch 788/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 285494842.2463\n",
            "Epoch 789/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 244508451.1527\n",
            "Epoch 790/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 241939171.6256\n",
            "Epoch 791/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 259668721.8128\n",
            "Epoch 792/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 294124517.3596\n",
            "Epoch 793/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 230525655.8818\n",
            "Epoch 794/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 228896530.4039\n",
            "Epoch 795/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 222180379.9015\n",
            "Epoch 796/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 252529930.7192\n",
            "Epoch 797/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 238937959.0148\n",
            "Epoch 798/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 288101214.8966\n",
            "Epoch 799/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 263117184.2759\n",
            "Epoch 800/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 244521658.4828\n",
            "Epoch 801/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 325855504.6305\n",
            "Epoch 802/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 278621855.4483\n",
            "Epoch 803/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 284784204.9261\n",
            "Epoch 804/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 263594408.6700\n",
            "Epoch 805/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 291163638.3842\n",
            "Epoch 806/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 276380958.7389\n",
            "Epoch 807/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 247189082.6404\n",
            "Epoch 808/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 431426187.6650\n",
            "Epoch 809/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 301672920.6700\n",
            "Epoch 810/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 258146790.3054\n",
            "Epoch 811/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 261113074.4433\n",
            "Epoch 812/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 284398968.8276\n",
            "Epoch 813/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 289855667.2315\n",
            "Epoch 814/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 347576876.6108\n",
            "Epoch 815/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 305944752.2365\n",
            "Epoch 816/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 245566471.8818\n",
            "Epoch 817/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 312713044.2562\n",
            "Epoch 818/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 305365091.5468\n",
            "Epoch 819/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 212013038.5025\n",
            "Epoch 820/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 239024202.0099\n",
            "Epoch 821/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 225872818.6798\n",
            "Epoch 822/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 278432932.8079\n",
            "Epoch 823/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 227274193.7340\n",
            "Epoch 824/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 224784271.2906\n",
            "Epoch 825/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 236050221.3202\n",
            "Epoch 826/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 246407162.7980\n",
            "Epoch 827/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 221567068.3744\n",
            "Epoch 828/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 186973582.5813\n",
            "Epoch 829/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 220700904.7488\n",
            "Epoch 830/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 306772701.0837\n",
            "Epoch 831/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 266392987.7438\n",
            "Epoch 832/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 276377416.1970\n",
            "Epoch 833/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 292697164.4532\n",
            "Epoch 834/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 212268441.0640\n",
            "Epoch 835/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 237270345.7734\n",
            "Epoch 836/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 276251543.8030\n",
            "Epoch 837/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 250990012.2167\n",
            "Epoch 838/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 224991918.0296\n",
            "Epoch 839/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 204014703.2906\n",
            "Epoch 840/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 228570168.2365\n",
            "Epoch 841/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 259503415.6453\n",
            "Epoch 842/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 238291960.2759\n",
            "Epoch 843/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 298850877.1626\n",
            "Epoch 844/1030\n",
            "203/203 [==============================] - 0s 146us/step - loss: 224926145.3399\n",
            "Epoch 845/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 246653343.7635\n",
            "Epoch 846/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 276618702.0296\n",
            "Epoch 847/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 210878372.2562\n",
            "Epoch 848/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 252148814.1872\n",
            "Epoch 849/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 290765378.5222\n",
            "Epoch 850/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 230941524.4138\n",
            "Epoch 851/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 285366634.7980\n",
            "Epoch 852/1030\n",
            "203/203 [==============================] - 0s 185us/step - loss: 258394624.3153\n",
            "Epoch 853/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 275132047.6059\n",
            "Epoch 854/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 272176592.7488\n",
            "Epoch 855/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 220702478.5813\n",
            "Epoch 856/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 242188432.0788\n",
            "Epoch 857/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 260194460.2167\n",
            "Epoch 858/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 249310867.8621\n",
            "Epoch 859/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 268303374.4236\n",
            "Epoch 860/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 237964015.2118\n",
            "Epoch 861/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 252780328.6700\n",
            "Epoch 862/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 305040717.7931\n",
            "Epoch 863/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 246292956.6897\n",
            "Epoch 864/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 278220973.5567\n",
            "Epoch 865/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 231384034.3645\n",
            "Epoch 866/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 314657837.8719\n",
            "Epoch 867/1030\n",
            "203/203 [==============================] - 0s 202us/step - loss: 312448352.4729\n",
            "Epoch 868/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 256680991.7635\n",
            "Epoch 869/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 284865592.2759\n",
            "Epoch 870/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 272739851.2709\n",
            "Epoch 871/1030\n",
            "203/203 [==============================] - 0s 147us/step - loss: 258820729.6946\n",
            "Epoch 872/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 252953095.4877\n",
            "Epoch 873/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 217660159.2118\n",
            "Epoch 874/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 269787880.0394\n",
            "Epoch 875/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 250218848.9458\n",
            "Epoch 876/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 277600226.9951\n",
            "Epoch 877/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 367884151.8030\n",
            "Epoch 878/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 356432944.5517\n",
            "Epoch 879/1030\n",
            "203/203 [==============================] - 0s 147us/step - loss: 330531785.1429\n",
            "Epoch 880/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 309649316.8867\n",
            "Epoch 881/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 375076411.7438\n",
            "Epoch 882/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 223989108.3350\n",
            "Epoch 883/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 239799006.9754\n",
            "Epoch 884/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 233329410.2069\n",
            "Epoch 885/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 238186289.0246\n",
            "Epoch 886/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 193784720.7094\n",
            "Epoch 887/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 201719215.2906\n",
            "Epoch 888/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 387380693.2020\n",
            "Epoch 889/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 241478193.8128\n",
            "Epoch 890/1030\n",
            "203/203 [==============================] - 0s 145us/step - loss: 243772511.8424\n",
            "Epoch 891/1030\n",
            "203/203 [==============================] - 0s 146us/step - loss: 252940238.8177\n",
            "Epoch 892/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 269948654.9754\n",
            "Epoch 893/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 210126770.1281\n",
            "Epoch 894/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 222910740.9655\n",
            "Epoch 895/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 210208005.6749\n",
            "Epoch 896/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 207298813.7931\n",
            "Epoch 897/1030\n",
            "203/203 [==============================] - 0s 148us/step - loss: 244689538.3645\n",
            "Epoch 898/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 312789255.3300\n",
            "Epoch 899/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 261797505.4187\n",
            "Epoch 900/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 287312039.7241\n",
            "Epoch 901/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 559476000.0000\n",
            "Epoch 902/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 555640638.2660\n",
            "Epoch 903/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 387202796.2956\n",
            "Epoch 904/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 496180041.4581\n",
            "Epoch 905/1030\n",
            "203/203 [==============================] - 0s 193us/step - loss: 399057198.9754\n",
            "Epoch 906/1030\n",
            "203/203 [==============================] - 0s 193us/step - loss: 363492002.2069\n",
            "Epoch 907/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 273084548.0985\n",
            "Epoch 908/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 218433831.2512\n",
            "Epoch 909/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 229193182.0296\n",
            "Epoch 910/1030\n",
            "203/203 [==============================] - 0s 184us/step - loss: 273215879.5665\n",
            "Epoch 911/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 250754321.7340\n",
            "Epoch 912/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 238559884.6108\n",
            "Epoch 913/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 223117817.3005\n",
            "Epoch 914/1030\n",
            "203/203 [==============================] - 0s 147us/step - loss: 258250540.5320\n",
            "Epoch 915/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 212620016.9852\n",
            "Epoch 916/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 291571408.7094\n",
            "Epoch 917/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 231146017.8128\n",
            "Epoch 918/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 204858411.8227\n",
            "Epoch 919/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 205401781.7537\n",
            "Epoch 920/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 274209038.3448\n",
            "Epoch 921/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 361527220.3350\n",
            "Epoch 922/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 329262552.4335\n",
            "Epoch 923/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 361700231.5665\n",
            "Epoch 924/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 586647492.4138\n",
            "Epoch 925/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 402654242.0493\n",
            "Epoch 926/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 362765329.0246\n",
            "Epoch 927/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 299400534.1478\n",
            "Epoch 928/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 264080090.4039\n",
            "Epoch 929/1030\n",
            "203/203 [==============================] - 0s 148us/step - loss: 204973880.9064\n",
            "Epoch 930/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 323000710.0690\n",
            "Epoch 931/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 309608090.0099\n",
            "Epoch 932/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 464503904.4729\n",
            "Epoch 933/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 287126736.3941\n",
            "Epoch 934/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 367875707.2709\n",
            "Epoch 935/1030\n",
            "203/203 [==============================] - 0s 147us/step - loss: 395867906.2069\n",
            "Epoch 936/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 300967541.5961\n",
            "Epoch 937/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 200901570.9163\n",
            "Epoch 938/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 231346060.8473\n",
            "Epoch 939/1030\n",
            "203/203 [==============================] - 0s 200us/step - loss: 237020629.2808\n",
            "Epoch 940/1030\n",
            "203/203 [==============================] - 0s 183us/step - loss: 235020779.0345\n",
            "Epoch 941/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 215157571.9015\n",
            "Epoch 942/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 233886064.3153\n",
            "Epoch 943/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 234785767.8818\n",
            "Epoch 944/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 305810281.1429\n",
            "Epoch 945/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 315747293.4778\n",
            "Epoch 946/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 235804359.5665\n",
            "Epoch 947/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 260951762.6010\n",
            "Epoch 948/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 282004395.0345\n",
            "Epoch 949/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 384209646.1478\n",
            "Epoch 950/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 268199068.3744\n",
            "Epoch 951/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 289060681.6158\n",
            "Epoch 952/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 275343620.5714\n",
            "Epoch 953/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 249068229.0443\n",
            "Epoch 954/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 298846159.6453\n",
            "Epoch 955/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 366621478.1478\n",
            "Epoch 956/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 258616031.2906\n",
            "Epoch 957/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 261851671.8818\n",
            "Epoch 958/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 253878938.3251\n",
            "Epoch 959/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 294987672.7488\n",
            "Epoch 960/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 283358750.0296\n",
            "Epoch 961/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 221300384.6305\n",
            "Epoch 962/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 233907111.9212\n",
            "Epoch 963/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 303896079.1330\n",
            "Epoch 964/1030\n",
            "203/203 [==============================] - 0s 146us/step - loss: 256893312.6700\n",
            "Epoch 965/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 268088726.2266\n",
            "Epoch 966/1030\n",
            "203/203 [==============================] - 0s 202us/step - loss: 285167697.6552\n",
            "Epoch 967/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 235639939.6650\n",
            "Epoch 968/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 235103142.4631\n",
            "Epoch 969/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 226504109.5961\n",
            "Epoch 970/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 216636873.8522\n",
            "Epoch 971/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 196248018.7586\n",
            "Epoch 972/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 256119778.9951\n",
            "Epoch 973/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 260426036.3350\n",
            "Epoch 974/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 232447363.5468\n",
            "Epoch 975/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 237494649.8522\n",
            "Epoch 976/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 350852647.0148\n",
            "Epoch 977/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 281759338.2463\n",
            "Epoch 978/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 300419257.7734\n",
            "Epoch 979/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 407567367.5665\n",
            "Epoch 980/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 368425806.1084\n",
            "Epoch 981/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 248686666.2463\n",
            "Epoch 982/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 234450113.0246\n",
            "Epoch 983/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 233729355.0345\n",
            "Epoch 984/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 242472243.5468\n",
            "Epoch 985/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 244630482.5222\n",
            "Epoch 986/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 218037495.3300\n",
            "Epoch 987/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 236095307.0345\n",
            "Epoch 988/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 297959720.5911\n",
            "Epoch 989/1030\n",
            "203/203 [==============================] - 0s 185us/step - loss: 212871002.4828\n",
            "Epoch 990/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 224668152.3547\n",
            "Epoch 991/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 228695771.7438\n",
            "Epoch 992/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 262851626.4039\n",
            "Epoch 993/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 313882688.3153\n",
            "Epoch 994/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 259824495.9212\n",
            "Epoch 995/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 298344726.3054\n",
            "Epoch 996/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 254984721.2611\n",
            "Epoch 997/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 241355808.3153\n",
            "Epoch 998/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 425010477.7143\n",
            "Epoch 999/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 511905259.8227\n",
            "Epoch 1000/1030\n",
            "203/203 [==============================] - 0s 190us/step - loss: 355917968.2365\n",
            "Epoch 1001/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 222980669.5961\n",
            "Epoch 1002/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 273333988.5714\n",
            "Epoch 1003/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 263555049.8719\n",
            "Epoch 1004/1030\n",
            "203/203 [==============================] - 0s 147us/step - loss: 263775288.1182\n",
            "Epoch 1005/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 273163313.3399\n",
            "Epoch 1006/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 353617634.9163\n",
            "Epoch 1007/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 350446929.4975\n",
            "Epoch 1008/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 294326569.6158\n",
            "Epoch 1009/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 269845059.1527\n",
            "Epoch 1010/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 230642937.3005\n",
            "Epoch 1011/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 295324244.1773\n",
            "Epoch 1012/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 306551488.7882\n",
            "Epoch 1013/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 335850536.5123\n",
            "Epoch 1014/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 398636549.0443\n",
            "Epoch 1015/1030\n",
            "203/203 [==============================] - 0s 191us/step - loss: 309381518.3448\n",
            "Epoch 1016/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 251177499.6650\n",
            "Epoch 1017/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 306249303.0936\n",
            "Epoch 1018/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 242033113.8522\n",
            "Epoch 1019/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 253365789.3990\n",
            "Epoch 1020/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 218360617.0640\n",
            "Epoch 1021/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 195930877.7143\n",
            "Epoch 1022/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 266885409.7340\n",
            "Epoch 1023/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 345784925.7143\n",
            "Epoch 1024/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 239332142.9754\n",
            "Epoch 1025/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 266018485.2808\n",
            "Epoch 1026/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 262307679.2118\n",
            "Epoch 1027/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 293091617.4187\n",
            "Epoch 1028/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 224412232.3153\n",
            "Epoch 1029/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 285046965.4384\n",
            "Epoch 1030/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 351414137.3793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5e12c76dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGmLWdIISLpb",
        "colab_type": "code",
        "outputId": "506a0930-4e92-4945-b722-3868f8e2ee0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88/88 [==============================] - 1s 12ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "189046334.54545453"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    }
  ]
}