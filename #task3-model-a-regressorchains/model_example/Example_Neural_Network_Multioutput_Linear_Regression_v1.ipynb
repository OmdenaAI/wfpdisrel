{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example Neural Network - Multioutput Linear Regression-v1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CQXAD8TR7Gj",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFzBrCZiRndp",
        "colab_type": "text"
      },
      "source": [
        "Example of Multi Target Regression using Neural Network Regression\n",
        "\n",
        "      @Sijuade Oguntayo\n",
        "\n",
        "\n",
        "Copied from *@XavierTorres* - \n",
        "\n",
        "In this example we are trying to predict 2 continuous variables with 4 input features. Here we are not doing data cleaning, neither EDA, feature engineering nor fine-tuning model features. It's only a preliminary model overview.\n",
        "\n",
        "- Inputs: \n",
        "    - Land_sqf\n",
        "    - Gross_sqf\n",
        "    - Year_built\n",
        "    - NBH_level\t(Neighborhood level: 10= most upper class; 1: most lower class)\n",
        "    \n",
        "- Outputs: \n",
        "    - SALE_PRICE \t\n",
        "    - YEARLY_RENT\n",
        "\n",
        "We are going to try different approaches to predict each output (possible SALE_PRICE and possible YEARLY_RENT for a new property).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMSAyrl7RioM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed-iOgYySA1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('nyc-rolling-sales-clean-1.csv', sep=';')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqNuQsHuSCq8",
        "colab_type": "code",
        "outputId": "af66c060-7de1-4002-a6a7-a91e85092b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Land_sqf</th>\n",
              "      <th>Gross_sqf</th>\n",
              "      <th>Year_built</th>\n",
              "      <th>NBH_level</th>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <th>YEARLY_RENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2400</td>\n",
              "      <td>1552</td>\n",
              "      <td>1930</td>\n",
              "      <td>2</td>\n",
              "      <td>220485</td>\n",
              "      <td>10900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2742</td>\n",
              "      <td>1207</td>\n",
              "      <td>1925</td>\n",
              "      <td>2</td>\n",
              "      <td>223372</td>\n",
              "      <td>8100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5610</td>\n",
              "      <td>1320</td>\n",
              "      <td>1910</td>\n",
              "      <td>5</td>\n",
              "      <td>362981</td>\n",
              "      <td>19000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1758</td>\n",
              "      <td>1537</td>\n",
              "      <td>1910</td>\n",
              "      <td>5</td>\n",
              "      <td>245135</td>\n",
              "      <td>9700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1317</td>\n",
              "      <td>1339</td>\n",
              "      <td>1920</td>\n",
              "      <td>4</td>\n",
              "      <td>216477</td>\n",
              "      <td>12800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Land_sqf  Gross_sqf  Year_built  NBH_level  SALE_PRICE  YEARLY_RENT\n",
              "0      2400       1552        1930          2      220485        10900\n",
              "1      2742       1207        1925          2      223372         8100\n",
              "2      5610       1320        1910          5      362981        19000\n",
              "3      1758       1537        1910          5      245135         9700\n",
              "4      1317       1339        1920          4      216477        12800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql0tatV_SEVE",
        "colab_type": "code",
        "outputId": "6f0e91ad-3e46-4b16-cc14-d03f81b7b2df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "df.corr()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Land_sqf</th>\n",
              "      <th>Gross_sqf</th>\n",
              "      <th>Year_built</th>\n",
              "      <th>NBH_level</th>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <th>YEARLY_RENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Land_sqf</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.383712</td>\n",
              "      <td>-0.001142</td>\n",
              "      <td>0.030384</td>\n",
              "      <td>0.693293</td>\n",
              "      <td>0.534428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Gross_sqf</th>\n",
              "      <td>0.383712</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.061774</td>\n",
              "      <td>-0.025912</td>\n",
              "      <td>0.782791</td>\n",
              "      <td>0.536878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Year_built</th>\n",
              "      <td>-0.001142</td>\n",
              "      <td>-0.061774</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.030740</td>\n",
              "      <td>-0.004287</td>\n",
              "      <td>0.053032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NBH_level</th>\n",
              "      <td>0.030384</td>\n",
              "      <td>-0.025912</td>\n",
              "      <td>0.030740</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.426108</td>\n",
              "      <td>0.682104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <td>0.693293</td>\n",
              "      <td>0.782791</td>\n",
              "      <td>-0.004287</td>\n",
              "      <td>0.426108</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>YEARLY_RENT</th>\n",
              "      <td>0.534428</td>\n",
              "      <td>0.536878</td>\n",
              "      <td>0.053032</td>\n",
              "      <td>0.682104</td>\n",
              "      <td>0.864152</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Land_sqf  Gross_sqf  ...  SALE_PRICE  YEARLY_RENT\n",
              "Land_sqf     1.000000   0.383712  ...    0.693293     0.534428\n",
              "Gross_sqf    0.383712   1.000000  ...    0.782791     0.536878\n",
              "Year_built  -0.001142  -0.061774  ...   -0.004287     0.053032\n",
              "NBH_level    0.030384  -0.025912  ...    0.426108     0.682104\n",
              "SALE_PRICE   0.693293   0.782791  ...    1.000000     0.864152\n",
              "YEARLY_RENT  0.534428   0.536878  ...    0.864152     1.000000\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7gy7EVISGRT",
        "colab_type": "code",
        "outputId": "485a1973-1d71-42ba-a29a-a76d3913b1a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Land_sqf</th>\n",
              "      <th>Gross_sqf</th>\n",
              "      <th>Year_built</th>\n",
              "      <th>NBH_level</th>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <th>YEARLY_RENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>291.000000</td>\n",
              "      <td>291.000000</td>\n",
              "      <td>291.000000</td>\n",
              "      <td>291.000000</td>\n",
              "      <td>291.000000</td>\n",
              "      <td>291.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2645.676976</td>\n",
              "      <td>1639.773196</td>\n",
              "      <td>1938.378007</td>\n",
              "      <td>5.374570</td>\n",
              "      <td>287247.257732</td>\n",
              "      <td>18346.735395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1353.765596</td>\n",
              "      <td>810.233323</td>\n",
              "      <td>27.953929</td>\n",
              "      <td>2.892423</td>\n",
              "      <td>105082.283757</td>\n",
              "      <td>9435.111314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>353.000000</td>\n",
              "      <td>450.000000</td>\n",
              "      <td>1901.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>78522.000000</td>\n",
              "      <td>4300.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1900.000000</td>\n",
              "      <td>1114.000000</td>\n",
              "      <td>1920.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>216482.500000</td>\n",
              "      <td>10750.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2446.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1930.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>281208.000000</td>\n",
              "      <td>17300.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3000.000000</td>\n",
              "      <td>1960.000000</td>\n",
              "      <td>1950.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>332174.500000</td>\n",
              "      <td>24550.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>14384.000000</td>\n",
              "      <td>5303.000000</td>\n",
              "      <td>2015.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>961748.000000</td>\n",
              "      <td>81400.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Land_sqf    Gross_sqf  ...     SALE_PRICE   YEARLY_RENT\n",
              "count    291.000000   291.000000  ...     291.000000    291.000000\n",
              "mean    2645.676976  1639.773196  ...  287247.257732  18346.735395\n",
              "std     1353.765596   810.233323  ...  105082.283757   9435.111314\n",
              "min      353.000000   450.000000  ...   78522.000000   4300.000000\n",
              "25%     1900.000000  1114.000000  ...  216482.500000  10750.000000\n",
              "50%     2446.000000  1400.000000  ...  281208.000000  17300.000000\n",
              "75%     3000.000000  1960.000000  ...  332174.500000  24550.000000\n",
              "max    14384.000000  5303.000000  ...  961748.000000  81400.000000\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vTFjiG3SGTp",
        "colab_type": "code",
        "outputId": "18541ba0-ecc5-4b00-ff91-b458612e5f0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Land_sqf       int64\n",
              "Gross_sqf      int64\n",
              "Year_built     int64\n",
              "NBH_level      int64\n",
              "SALE_PRICE     int64\n",
              "YEARLY_RENT    int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evEoONQJSGaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.iloc[:,:4] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgPq5VSuSGfg",
        "colab_type": "code",
        "outputId": "72dea830-5086-4be0-dd6c-c7383976e0ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(291, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t-pRoVbSLYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assigning last 4 columns to Y, with the 2 dependant variables or 'outputs' or 'targets'\n",
        "Y = df.iloc[:,4:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3ofxG7qSLdM",
        "colab_type": "code",
        "outputId": "81171f1d-3c28-4e0e-80a5-9882de0be89e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "Y.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <th>YEARLY_RENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>220485</td>\n",
              "      <td>10900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>223372</td>\n",
              "      <td>8100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>362981</td>\n",
              "      <td>19000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>245135</td>\n",
              "      <td>9700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>216477</td>\n",
              "      <td>12800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SALE_PRICE  YEARLY_RENT\n",
              "0      220485        10900\n",
              "1      223372         8100\n",
              "2      362981        19000\n",
              "3      245135         9700\n",
              "4      216477        12800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GctuFnrMSLic",
        "colab_type": "code",
        "outputId": "005ea8e4-20de-455b-f0e5-cfc5cf5f0780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(291, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkCxnczTSLrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split train and test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,\n",
        "                                                    random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ__qQsmSL0D",
        "colab_type": "code",
        "outputId": "7caa850b-ffe5-4b36-8f7b-6c8f73038633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! pip install --upgrade tensorflow"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 42.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 58.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (45.2.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow-addons~=0.7.0, but you'll have tensorflow-addons 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALRwU0aASL3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import r2_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn5LPfF1SoTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Custom Loss funtion - R2\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def coeff_determination(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atVa_FuPSLzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Architecture\n",
        "\n",
        "input_tensor = Input(shape=(4, ))\n",
        "hidden_layer_one = Dense(128)(input_tensor)\n",
        "hidden_layer_two = Dense(256)(hidden_layer_one)\n",
        "dropout_layer = Dropout(rate=0.3)(hidden_layer_two)\n",
        "hidden_layer_three = Dense(256)(dropout_layer)\n",
        "hidden_layer_four = Dense(256)(hidden_layer_three)\n",
        "output = Dense(2)(hidden_layer_four)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUYXkqx8SLyL",
        "colab_type": "code",
        "outputId": "70f7675c-5ebe-4a2a-e259-140a4d2fa222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Model(input_tensor, output)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, Y_train, epochs=1030)"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1030\n",
            "203/203 [==============================] - 3s 17ms/step - loss: 44448002058.0887\n",
            "Epoch 2/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 24027222782.7389\n",
            "Epoch 3/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 5897122407.4089\n",
            "Epoch 4/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 4176799199.2118\n",
            "Epoch 5/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 4031002818.8374\n",
            "Epoch 6/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1884183301.6749\n",
            "Epoch 7/1030\n",
            "203/203 [==============================] - 0s 186us/step - loss: 1725286572.7685\n",
            "Epoch 8/1030\n",
            "203/203 [==============================] - 0s 182us/step - loss: 1481653096.3547\n",
            "Epoch 9/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1418455133.9507\n",
            "Epoch 10/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1333497944.2759\n",
            "Epoch 11/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1296800342.6995\n",
            "Epoch 12/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1217009297.6552\n",
            "Epoch 13/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1252031804.8473\n",
            "Epoch 14/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1233176057.0640\n",
            "Epoch 15/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 1302492110.1872\n",
            "Epoch 16/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1177224983.3300\n",
            "Epoch 17/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 1242585340.8473\n",
            "Epoch 18/1030\n",
            "203/203 [==============================] - 0s 192us/step - loss: 1186293668.5714\n",
            "Epoch 19/1030\n",
            "203/203 [==============================] - 0s 181us/step - loss: 1271540529.8128\n",
            "Epoch 20/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1128773882.9557\n",
            "Epoch 21/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 1153054469.6749\n",
            "Epoch 22/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1301009006.0296\n",
            "Epoch 23/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1257446208.6305\n",
            "Epoch 24/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1276182519.4877\n",
            "Epoch 25/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 1185054663.2512\n",
            "Epoch 26/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1208054190.0296\n",
            "Epoch 27/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1175908128.7882\n",
            "Epoch 28/1030\n",
            "203/203 [==============================] - 0s 187us/step - loss: 1185792827.9015\n",
            "Epoch 29/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1271278916.7291\n",
            "Epoch 30/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1231769801.7734\n",
            "Epoch 31/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1399508386.0493\n",
            "Epoch 32/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1338850040.4335\n",
            "Epoch 33/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1279519551.0542\n",
            "Epoch 34/1030\n",
            "203/203 [==============================] - 0s 181us/step - loss: 1235493942.2266\n",
            "Epoch 35/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1154408398.8177\n",
            "Epoch 36/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1233854995.2315\n",
            "Epoch 37/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 1227526334.4236\n",
            "Epoch 38/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1210362860.1379\n",
            "Epoch 39/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1280578162.1281\n",
            "Epoch 40/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 1215598740.1773\n",
            "Epoch 41/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1230551635.8621\n",
            "Epoch 42/1030\n",
            "203/203 [==============================] - 0s 146us/step - loss: 1298265518.3448\n",
            "Epoch 43/1030\n",
            "203/203 [==============================] - 0s 188us/step - loss: 1173985042.9163\n",
            "Epoch 44/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 1132161157.6749\n",
            "Epoch 45/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 1154146693.3596\n",
            "Epoch 46/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1168046672.0788\n",
            "Epoch 47/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 1217952402.9163\n",
            "Epoch 48/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1181956094.7389\n",
            "Epoch 49/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1163242449.3399\n",
            "Epoch 50/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1189854670.1872\n",
            "Epoch 51/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1250014435.6256\n",
            "Epoch 52/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1191903316.4926\n",
            "Epoch 53/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1205738518.6995\n",
            "Epoch 54/1030\n",
            "203/203 [==============================] - 0s 187us/step - loss: 1159026899.8621\n",
            "Epoch 55/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 1196355000.1182\n",
            "Epoch 56/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 1234705734.3054\n",
            "Epoch 57/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 1250784483.1527\n",
            "Epoch 58/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1205380112.7094\n",
            "Epoch 59/1030\n",
            "203/203 [==============================] - 0s 182us/step - loss: 1147562647.9606\n",
            "Epoch 60/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1195816036.8867\n",
            "Epoch 61/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1157648737.1034\n",
            "Epoch 62/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1157859889.8128\n",
            "Epoch 63/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1232861578.0887\n",
            "Epoch 64/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1198813561.0640\n",
            "Epoch 65/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1251263565.5567\n",
            "Epoch 66/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 1284953850.6404\n",
            "Epoch 67/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1188437901.8719\n",
            "Epoch 68/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 1281926669.8719\n",
            "Epoch 69/1030\n",
            "203/203 [==============================] - 0s 199us/step - loss: 1203597866.8768\n",
            "Epoch 70/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1221826984.3547\n",
            "Epoch 71/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1123383783.4089\n",
            "Epoch 72/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 1206258792.9852\n",
            "Epoch 73/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 1111862530.2069\n",
            "Epoch 74/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 1154814072.7488\n",
            "Epoch 75/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 1329864361.6158\n",
            "Epoch 76/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 1245324088.7488\n",
            "Epoch 77/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1241166099.5468\n",
            "Epoch 78/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1215361417.4581\n",
            "Epoch 79/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1286067587.4680\n",
            "Epoch 80/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1184436704.4729\n",
            "Epoch 81/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 1269738107.5862\n",
            "Epoch 82/1030\n",
            "203/203 [==============================] - 0s 189us/step - loss: 1257196113.9704\n",
            "Epoch 83/1030\n",
            "203/203 [==============================] - 0s 186us/step - loss: 1283483717.0443\n",
            "Epoch 84/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1245608097.7340\n",
            "Epoch 85/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1206544163.3103\n",
            "Epoch 86/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1157468180.1773\n",
            "Epoch 87/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1143686838.2266\n",
            "Epoch 88/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1189137813.4384\n",
            "Epoch 89/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 1141741810.1281\n",
            "Epoch 90/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1216202909.3202\n",
            "Epoch 91/1030\n",
            "203/203 [==============================] - 0s 183us/step - loss: 1164585273.0640\n",
            "Epoch 92/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 1093196372.8079\n",
            "Epoch 93/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1082001175.9606\n",
            "Epoch 94/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1180303566.5025\n",
            "Epoch 95/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1243365019.7438\n",
            "Epoch 96/1030\n",
            "203/203 [==============================] - 0s 140us/step - loss: 1226175244.9261\n",
            "Epoch 97/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 1285541124.7291\n",
            "Epoch 98/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 1376976667.7438\n",
            "Epoch 99/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1145086121.9310\n",
            "Epoch 100/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 1143052850.4433\n",
            "Epoch 101/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1197187069.1626\n",
            "Epoch 102/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 1108721411.1527\n",
            "Epoch 103/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1141148212.3350\n",
            "Epoch 104/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1309737986.5222\n",
            "Epoch 105/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 1309247032.1182\n",
            "Epoch 106/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 1222910943.2118\n",
            "Epoch 107/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1288616453.6749\n",
            "Epoch 108/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1209162653.0049\n",
            "Epoch 109/1030\n",
            "203/203 [==============================] - 0s 186us/step - loss: 1115224617.6158\n",
            "Epoch 110/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 1179904988.0591\n",
            "Epoch 111/1030\n",
            "203/203 [==============================] - 0s 186us/step - loss: 1474453816.7488\n",
            "Epoch 112/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1269232617.3005\n",
            "Epoch 113/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1333808578.2069\n",
            "Epoch 114/1030\n",
            "203/203 [==============================] - 0s 194us/step - loss: 1438356231.8818\n",
            "Epoch 115/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1371687519.8424\n",
            "Epoch 116/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1276871695.1330\n",
            "Epoch 117/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1459830002.7586\n",
            "Epoch 118/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 1283809994.7192\n",
            "Epoch 119/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1128180889.8522\n",
            "Epoch 120/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1102598859.6650\n",
            "Epoch 121/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1161464814.3448\n",
            "Epoch 122/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1518246764.4532\n",
            "Epoch 123/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1419465186.3645\n",
            "Epoch 124/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1408430484.8079\n",
            "Epoch 125/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 1352237806.9754\n",
            "Epoch 126/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1252772305.3399\n",
            "Epoch 127/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1218405002.7192\n",
            "Epoch 128/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1316690669.7143\n",
            "Epoch 129/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1261652877.8719\n",
            "Epoch 130/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1106139822.6601\n",
            "Epoch 131/1030\n",
            "203/203 [==============================] - 0s 185us/step - loss: 1207459163.4286\n",
            "Epoch 132/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1188073340.2167\n",
            "Epoch 133/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1152236723.0739\n",
            "Epoch 134/1030\n",
            "203/203 [==============================] - 0s 194us/step - loss: 1075059235.9409\n",
            "Epoch 135/1030\n",
            "203/203 [==============================] - 0s 223us/step - loss: 1063871588.8867\n",
            "Epoch 136/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 1101799784.6700\n",
            "Epoch 137/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1036585831.4089\n",
            "Epoch 138/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1207694858.7192\n",
            "Epoch 139/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 1298803630.0296\n",
            "Epoch 140/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 1090561129.3005\n",
            "Epoch 141/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1145862778.0099\n",
            "Epoch 142/1030\n",
            "203/203 [==============================] - 0s 198us/step - loss: 1097656266.4039\n",
            "Epoch 143/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 1066746960.7094\n",
            "Epoch 144/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 1298326415.7635\n",
            "Epoch 145/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1356744717.8719\n",
            "Epoch 146/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 1231696760.4335\n",
            "Epoch 147/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1355158838.2266\n",
            "Epoch 148/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1085510508.7685\n",
            "Epoch 149/1030\n",
            "203/203 [==============================] - 0s 189us/step - loss: 1072042941.7931\n",
            "Epoch 150/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1077521392.2365\n",
            "Epoch 151/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 1169595542.3842\n",
            "Epoch 152/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1079810508.2956\n",
            "Epoch 153/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 1101022100.1773\n",
            "Epoch 154/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1048416276.1773\n",
            "Epoch 155/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1148556543.0542\n",
            "Epoch 156/1030\n",
            "203/203 [==============================] - 0s 185us/step - loss: 1183071249.6552\n",
            "Epoch 157/1030\n",
            "203/203 [==============================] - 0s 193us/step - loss: 1222663468.1379\n",
            "Epoch 158/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1058658353.1823\n",
            "Epoch 159/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 1169856030.2660\n",
            "Epoch 160/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1265355328.9458\n",
            "Epoch 161/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1132639962.1675\n",
            "Epoch 162/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1118198071.1724\n",
            "Epoch 163/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1140529956.5714\n",
            "Epoch 164/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1140935169.8916\n",
            "Epoch 165/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1173393532.8473\n",
            "Epoch 166/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1126580411.2709\n",
            "Epoch 167/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 1181455953.6552\n",
            "Epoch 168/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 1083326928.7094\n",
            "Epoch 169/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1200979783.8818\n",
            "Epoch 170/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1068105698.3645\n",
            "Epoch 171/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1130456111.2906\n",
            "Epoch 172/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1057538752.6305\n",
            "Epoch 173/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1179308738.8374\n",
            "Epoch 174/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1159590454.5419\n",
            "Epoch 175/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1168783146.2463\n",
            "Epoch 176/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 1093837513.7734\n",
            "Epoch 177/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 1047277972.8079\n",
            "Epoch 178/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1190776700.5320\n",
            "Epoch 179/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1172834960.7094\n",
            "Epoch 180/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 1141886934.3842\n",
            "Epoch 181/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1138209749.7537\n",
            "Epoch 182/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 1146180034.8374\n",
            "Epoch 183/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 1115916775.7241\n",
            "Epoch 184/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1194976049.1823\n",
            "Epoch 185/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 994612229.6749\n",
            "Epoch 186/1030\n",
            "203/203 [==============================] - 0s 148us/step - loss: 1097586480.2365\n",
            "Epoch 187/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1033737901.0837\n",
            "Epoch 188/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 1073860800.3153\n",
            "Epoch 189/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1151014914.5222\n",
            "Epoch 190/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 1071283543.3300\n",
            "Epoch 191/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1165726118.4631\n",
            "Epoch 192/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1082139730.6010\n",
            "Epoch 193/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1063566368.4729\n",
            "Epoch 194/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 1190064723.2315\n",
            "Epoch 195/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 1013601465.0640\n",
            "Epoch 196/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 1050530746.6404\n",
            "Epoch 197/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 1185843229.0049\n",
            "Epoch 198/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1156054640.8670\n",
            "Epoch 199/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1214251955.0739\n",
            "Epoch 200/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 1084911938.8374\n",
            "Epoch 201/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1123424136.8276\n",
            "Epoch 202/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 1303841967.9212\n",
            "Epoch 203/1030\n",
            "203/203 [==============================] - 0s 201us/step - loss: 1077826780.0591\n",
            "Epoch 204/1030\n",
            "203/203 [==============================] - 0s 209us/step - loss: 1277756525.3990\n",
            "Epoch 205/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 1133221968.7094\n",
            "Epoch 206/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1042073351.8818\n",
            "Epoch 207/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1002143231.0542\n",
            "Epoch 208/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 1048654846.4236\n",
            "Epoch 209/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1043518510.9754\n",
            "Epoch 210/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 1084917139.5468\n",
            "Epoch 211/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1053949609.7734\n",
            "Epoch 212/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 1157964014.9754\n",
            "Epoch 213/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1179551966.5813\n",
            "Epoch 214/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1028391568.3941\n",
            "Epoch 215/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1142250685.1626\n",
            "Epoch 216/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 1168926559.2118\n",
            "Epoch 217/1030\n",
            "203/203 [==============================] - 0s 148us/step - loss: 983555787.6650\n",
            "Epoch 218/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1037963758.6601\n",
            "Epoch 219/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 1044308272.8670\n",
            "Epoch 220/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1019524723.0739\n",
            "Epoch 221/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1169149765.6749\n",
            "Epoch 222/1030\n",
            "203/203 [==============================] - 0s 183us/step - loss: 1173201278.1084\n",
            "Epoch 223/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 1092570470.1478\n",
            "Epoch 224/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 986305812.6502\n",
            "Epoch 225/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 1123749454.1872\n",
            "Epoch 226/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 1038710338.8374\n",
            "Epoch 227/1030\n",
            "203/203 [==============================] - 0s 192us/step - loss: 963419075.4680\n",
            "Epoch 228/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1049884780.4532\n",
            "Epoch 229/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 991093561.6946\n",
            "Epoch 230/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 959825824.7882\n",
            "Epoch 231/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 1082194891.9803\n",
            "Epoch 232/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 965532363.0345\n",
            "Epoch 233/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 987733876.0197\n",
            "Epoch 234/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 966145444.2562\n",
            "Epoch 235/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1017530676.3350\n",
            "Epoch 236/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 1017487091.7044\n",
            "Epoch 237/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 941955728.3941\n",
            "Epoch 238/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1051629606.6207\n",
            "Epoch 239/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 919652360.8276\n",
            "Epoch 240/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1076393220.4138\n",
            "Epoch 241/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 1012372171.6650\n",
            "Epoch 242/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1161167031.4877\n",
            "Epoch 243/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 936705773.8719\n",
            "Epoch 244/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 869497464.4335\n",
            "Epoch 245/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 985268812.9261\n",
            "Epoch 246/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 960887127.6453\n",
            "Epoch 247/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 838036419.7833\n",
            "Epoch 248/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 1001370266.4828\n",
            "Epoch 249/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 949774299.4286\n",
            "Epoch 250/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 1043724834.0493\n",
            "Epoch 251/1030\n",
            "203/203 [==============================] - 0s 181us/step - loss: 911508486.6207\n",
            "Epoch 252/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1025537442.6798\n",
            "Epoch 253/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 853211701.9113\n",
            "Epoch 254/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 864860193.7340\n",
            "Epoch 255/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 969773385.4581\n",
            "Epoch 256/1030\n",
            "203/203 [==============================] - 0s 187us/step - loss: 870575789.3990\n",
            "Epoch 257/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 810862948.5714\n",
            "Epoch 258/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 773834928.8670\n",
            "Epoch 259/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 728918348.2956\n",
            "Epoch 260/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 865864406.6995\n",
            "Epoch 261/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 986913329.1823\n",
            "Epoch 262/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 1065526159.7635\n",
            "Epoch 263/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 980561110.3842\n",
            "Epoch 264/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 875359270.4631\n",
            "Epoch 265/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 985843840.1576\n",
            "Epoch 266/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 857091116.7685\n",
            "Epoch 267/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 736716209.1823\n",
            "Epoch 268/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 748668160.0000\n",
            "Epoch 269/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 750694708.4926\n",
            "Epoch 270/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 696876613.8325\n",
            "Epoch 271/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 728840817.3399\n",
            "Epoch 272/1030\n",
            "203/203 [==============================] - 0s 187us/step - loss: 668416711.0936\n",
            "Epoch 273/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 699381684.6502\n",
            "Epoch 274/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 753551875.4680\n",
            "Epoch 275/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 720427385.6946\n",
            "Epoch 276/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 601310513.6552\n",
            "Epoch 277/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 769198885.8325\n",
            "Epoch 278/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 992179339.9803\n",
            "Epoch 279/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 586698994.4433\n",
            "Epoch 280/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 620357577.7734\n",
            "Epoch 281/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 723833972.3350\n",
            "Epoch 282/1030\n",
            "203/203 [==============================] - 0s 181us/step - loss: 738684276.0197\n",
            "Epoch 283/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 572298930.7586\n",
            "Epoch 284/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 613559326.8966\n",
            "Epoch 285/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 565518661.5172\n",
            "Epoch 286/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 559842087.8818\n",
            "Epoch 287/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 954127293.1626\n",
            "Epoch 288/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 763467105.5764\n",
            "Epoch 289/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 604421886.8966\n",
            "Epoch 290/1030\n",
            "203/203 [==============================] - 0s 194us/step - loss: 688975221.7537\n",
            "Epoch 291/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 690048182.2266\n",
            "Epoch 292/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 567348618.4039\n",
            "Epoch 293/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 593999987.0739\n",
            "Epoch 294/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 483649232.5517\n",
            "Epoch 295/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 593214385.0246\n",
            "Epoch 296/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 481563945.4581\n",
            "Epoch 297/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 489172679.5665\n",
            "Epoch 298/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 419635875.7833\n",
            "Epoch 299/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 368542429.3202\n",
            "Epoch 300/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 454237909.5961\n",
            "Epoch 301/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 349427571.7833\n",
            "Epoch 302/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 440037013.4384\n",
            "Epoch 303/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 482392205.8719\n",
            "Epoch 304/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 511972691.8621\n",
            "Epoch 305/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 442668690.1281\n",
            "Epoch 306/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 486531390.1084\n",
            "Epoch 307/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 426491513.6946\n",
            "Epoch 308/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 422435187.0739\n",
            "Epoch 309/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 443422568.1970\n",
            "Epoch 310/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 629160846.8177\n",
            "Epoch 311/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 705517810.4433\n",
            "Epoch 312/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 339845077.3596\n",
            "Epoch 313/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 487106305.8916\n",
            "Epoch 314/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 468680945.4975\n",
            "Epoch 315/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 424797799.4089\n",
            "Epoch 316/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 624073695.8424\n",
            "Epoch 317/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 514311685.9113\n",
            "Epoch 318/1030\n",
            "203/203 [==============================] - 0s 182us/step - loss: 719642118.8571\n",
            "Epoch 319/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 543329225.1429\n",
            "Epoch 320/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 489500306.9163\n",
            "Epoch 321/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 532813594.7980\n",
            "Epoch 322/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 403905053.9507\n",
            "Epoch 323/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 335459778.6798\n",
            "Epoch 324/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 282869577.0640\n",
            "Epoch 325/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 324402808.7488\n",
            "Epoch 326/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 414087219.7044\n",
            "Epoch 327/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 274875480.3547\n",
            "Epoch 328/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 295980125.9507\n",
            "Epoch 329/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 383831771.7438\n",
            "Epoch 330/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 284379211.3498\n",
            "Epoch 331/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 411426840.5911\n",
            "Epoch 332/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 456944966.4631\n",
            "Epoch 333/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 337207495.5665\n",
            "Epoch 334/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 352896389.0443\n",
            "Epoch 335/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 424090030.8177\n",
            "Epoch 336/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 321061066.5616\n",
            "Epoch 337/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 664066982.8571\n",
            "Epoch 338/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 630663197.9507\n",
            "Epoch 339/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 489072841.4581\n",
            "Epoch 340/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 1358263376.3941\n",
            "Epoch 341/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 790889846.3842\n",
            "Epoch 342/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 378081840.2365\n",
            "Epoch 343/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 386894509.7143\n",
            "Epoch 344/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 289094300.5320\n",
            "Epoch 345/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 313199200.0000\n",
            "Epoch 346/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 349621469.8719\n",
            "Epoch 347/1030\n",
            "203/203 [==============================] - 0s 187us/step - loss: 300904607.6847\n",
            "Epoch 348/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 288202756.2562\n",
            "Epoch 349/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 258798957.7537\n",
            "Epoch 350/1030\n",
            "203/203 [==============================] - 0s 187us/step - loss: 328650659.6256\n",
            "Epoch 351/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 332207063.0148\n",
            "Epoch 352/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 284794147.9409\n",
            "Epoch 353/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 280615441.2611\n",
            "Epoch 354/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 286152416.8670\n",
            "Epoch 355/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 210208609.7340\n",
            "Epoch 356/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 291201839.1330\n",
            "Epoch 357/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 277433997.2414\n",
            "Epoch 358/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 283332363.6650\n",
            "Epoch 359/1030\n",
            "203/203 [==============================] - 0s 183us/step - loss: 357068900.0197\n",
            "Epoch 360/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 402282587.5862\n",
            "Epoch 361/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 288359898.4039\n",
            "Epoch 362/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 323681661.9507\n",
            "Epoch 363/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 285850345.4581\n",
            "Epoch 364/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 299259470.8177\n",
            "Epoch 365/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 227702515.7044\n",
            "Epoch 366/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 214713164.0591\n",
            "Epoch 367/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 276435968.6305\n",
            "Epoch 368/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 310991067.5862\n",
            "Epoch 369/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 237038335.6059\n",
            "Epoch 370/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 220796733.3990\n",
            "Epoch 371/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 256385388.1379\n",
            "Epoch 372/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 319208041.7734\n",
            "Epoch 373/1030\n",
            "203/203 [==============================] - 0s 147us/step - loss: 316058254.7389\n",
            "Epoch 374/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 278936630.3842\n",
            "Epoch 375/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 334777089.6552\n",
            "Epoch 376/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 288889121.6552\n",
            "Epoch 377/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 276731924.4138\n",
            "Epoch 378/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 275380594.6010\n",
            "Epoch 379/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 318731031.5665\n",
            "Epoch 380/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 284008263.7241\n",
            "Epoch 381/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 241512418.0493\n",
            "Epoch 382/1030\n",
            "203/203 [==============================] - 0s 185us/step - loss: 249302253.3990\n",
            "Epoch 383/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 322258211.4680\n",
            "Epoch 384/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 262749660.2167\n",
            "Epoch 385/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 289146176.1576\n",
            "Epoch 386/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 284450808.5911\n",
            "Epoch 387/1030\n",
            "203/203 [==============================] - 0s 184us/step - loss: 309416289.6158\n",
            "Epoch 388/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 320709620.8867\n",
            "Epoch 389/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 282603108.8867\n",
            "Epoch 390/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 279606098.0493\n",
            "Epoch 391/1030\n",
            "203/203 [==============================] - 0s 148us/step - loss: 207753091.0739\n",
            "Epoch 392/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 467184486.3054\n",
            "Epoch 393/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 461854291.5468\n",
            "Epoch 394/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 261397367.6453\n",
            "Epoch 395/1030\n",
            "203/203 [==============================] - 0s 140us/step - loss: 286469908.9655\n",
            "Epoch 396/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 298345676.6108\n",
            "Epoch 397/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 296426988.1379\n",
            "Epoch 398/1030\n",
            "203/203 [==============================] - 0s 185us/step - loss: 202271893.4384\n",
            "Epoch 399/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 293668589.3990\n",
            "Epoch 400/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 336249845.9113\n",
            "Epoch 401/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 274760992.3153\n",
            "Epoch 402/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 228003631.7635\n",
            "Epoch 403/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 319748547.9409\n",
            "Epoch 404/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 439953085.0049\n",
            "Epoch 405/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 491902799.7635\n",
            "Epoch 406/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 479171979.9803\n",
            "Epoch 407/1030\n",
            "203/203 [==============================] - 0s 181us/step - loss: 392518704.3153\n",
            "Epoch 408/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 328000809.2217\n",
            "Epoch 409/1030\n",
            "203/203 [==============================] - 0s 211us/step - loss: 267202910.1084\n",
            "Epoch 410/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 269916655.2906\n",
            "Epoch 411/1030\n",
            "203/203 [==============================] - 0s 144us/step - loss: 361396130.5222\n",
            "Epoch 412/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 343924913.6552\n",
            "Epoch 413/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 323068798.7389\n",
            "Epoch 414/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 363714961.4975\n",
            "Epoch 415/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 271089735.0936\n",
            "Epoch 416/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 295361632.0000\n",
            "Epoch 417/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 308742840.2759\n",
            "Epoch 418/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 271862839.2512\n",
            "Epoch 419/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 288386692.7291\n",
            "Epoch 420/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 320863588.5714\n",
            "Epoch 421/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 341213766.6207\n",
            "Epoch 422/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 675493969.3399\n",
            "Epoch 423/1030\n",
            "203/203 [==============================] - 0s 143us/step - loss: 738840350.8966\n",
            "Epoch 424/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 929887655.7241\n",
            "Epoch 425/1030\n",
            "203/203 [==============================] - 0s 222us/step - loss: 667050085.8325\n",
            "Epoch 426/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 1032138842.9557\n",
            "Epoch 427/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 569339708.5320\n",
            "Epoch 428/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 508998410.0887\n",
            "Epoch 429/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 369630909.4778\n",
            "Epoch 430/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 404754393.8522\n",
            "Epoch 431/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 352931568.5517\n",
            "Epoch 432/1030\n",
            "203/203 [==============================] - 0s 144us/step - loss: 367712154.9557\n",
            "Epoch 433/1030\n",
            "203/203 [==============================] - 0s 182us/step - loss: 340066221.8719\n",
            "Epoch 434/1030\n",
            "203/203 [==============================] - 0s 181us/step - loss: 273067229.3990\n",
            "Epoch 435/1030\n",
            "203/203 [==============================] - 0s 193us/step - loss: 260773853.3202\n",
            "Epoch 436/1030\n",
            "203/203 [==============================] - 0s 206us/step - loss: 245857985.8128\n",
            "Epoch 437/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 259607008.7094\n",
            "Epoch 438/1030\n",
            "203/203 [==============================] - 0s 185us/step - loss: 297805242.7192\n",
            "Epoch 439/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 287867591.2512\n",
            "Epoch 440/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 331837426.4433\n",
            "Epoch 441/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 378517152.7882\n",
            "Epoch 442/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 248181265.4975\n",
            "Epoch 443/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 357962713.8522\n",
            "Epoch 444/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 276199605.7537\n",
            "Epoch 445/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 343273082.9557\n",
            "Epoch 446/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 392815024.2365\n",
            "Epoch 447/1030\n",
            "203/203 [==============================] - 0s 147us/step - loss: 353099886.9754\n",
            "Epoch 448/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 329923125.7537\n",
            "Epoch 449/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 279514929.8916\n",
            "Epoch 450/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 243609653.3596\n",
            "Epoch 451/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 264611111.7241\n",
            "Epoch 452/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 320326724.4138\n",
            "Epoch 453/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 331725044.4138\n",
            "Epoch 454/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 275032770.6010\n",
            "Epoch 455/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 325623434.7192\n",
            "Epoch 456/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 365797019.2709\n",
            "Epoch 457/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 279574051.3103\n",
            "Epoch 458/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 315633167.9212\n",
            "Epoch 459/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 240403678.1084\n",
            "Epoch 460/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 385484537.0640\n",
            "Epoch 461/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 338250588.6897\n",
            "Epoch 462/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 632256536.2759\n",
            "Epoch 463/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 433457972.6502\n",
            "Epoch 464/1030\n",
            "203/203 [==============================] - 0s 146us/step - loss: 394974101.7537\n",
            "Epoch 465/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 303020934.4631\n",
            "Epoch 466/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 282241586.0493\n",
            "Epoch 467/1030\n",
            "203/203 [==============================] - 0s 147us/step - loss: 317144459.9803\n",
            "Epoch 468/1030\n",
            "203/203 [==============================] - 0s 186us/step - loss: 277104035.9015\n",
            "Epoch 469/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 242886136.1182\n",
            "Epoch 470/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 288377676.3744\n",
            "Epoch 471/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 267467658.0887\n",
            "Epoch 472/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 374778501.6749\n",
            "Epoch 473/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 341728912.9458\n",
            "Epoch 474/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 415840245.5961\n",
            "Epoch 475/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 310925354.4039\n",
            "Epoch 476/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 250992138.4828\n",
            "Epoch 477/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 270532145.5764\n",
            "Epoch 478/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 244649289.9310\n",
            "Epoch 479/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 278277581.7143\n",
            "Epoch 480/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 304345163.3498\n",
            "Epoch 481/1030\n",
            "203/203 [==============================] - 0s 148us/step - loss: 344524210.2857\n",
            "Epoch 482/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 277652060.4532\n",
            "Epoch 483/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 248800030.8966\n",
            "Epoch 484/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 261026197.5961\n",
            "Epoch 485/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 266522651.8227\n",
            "Epoch 486/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 227838280.0394\n",
            "Epoch 487/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 289421467.9803\n",
            "Epoch 488/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 223635149.0837\n",
            "Epoch 489/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 223065173.7537\n",
            "Epoch 490/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 243196048.1576\n",
            "Epoch 491/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 278709070.1084\n",
            "Epoch 492/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 274306033.5764\n",
            "Epoch 493/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 269654417.1823\n",
            "Epoch 494/1030\n",
            "203/203 [==============================] - 0s 186us/step - loss: 297188984.7488\n",
            "Epoch 495/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 387036460.2167\n",
            "Epoch 496/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 284376432.8670\n",
            "Epoch 497/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 225489738.4039\n",
            "Epoch 498/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 267303198.9754\n",
            "Epoch 499/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 230580797.3990\n",
            "Epoch 500/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 266487477.9113\n",
            "Epoch 501/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 242403483.2709\n",
            "Epoch 502/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 295306428.2167\n",
            "Epoch 503/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 217459152.2365\n",
            "Epoch 504/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 233016443.3498\n",
            "Epoch 505/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 268156717.3990\n",
            "Epoch 506/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 292158012.3350\n",
            "Epoch 507/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 435935629.5567\n",
            "Epoch 508/1030\n",
            "203/203 [==============================] - 0s 147us/step - loss: 278959127.0148\n",
            "Epoch 509/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 271556618.3251\n",
            "Epoch 510/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 274745309.8719\n",
            "Epoch 511/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 254652026.7192\n",
            "Epoch 512/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 281419374.0296\n",
            "Epoch 513/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 290276910.0296\n",
            "Epoch 514/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 396240664.5911\n",
            "Epoch 515/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 328164817.6552\n",
            "Epoch 516/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 259826128.8670\n",
            "Epoch 517/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 272030742.9360\n",
            "Epoch 518/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 234471343.2118\n",
            "Epoch 519/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 249397367.6453\n",
            "Epoch 520/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 243579721.1429\n",
            "Epoch 521/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 245483382.6995\n",
            "Epoch 522/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 255245733.6749\n",
            "Epoch 523/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 262947030.3842\n",
            "Epoch 524/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 238709795.3103\n",
            "Epoch 525/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 270412772.5714\n",
            "Epoch 526/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 244878540.4532\n",
            "Epoch 527/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 253418448.5517\n",
            "Epoch 528/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 290955678.1084\n",
            "Epoch 529/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 331780449.4187\n",
            "Epoch 530/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 261862824.0394\n",
            "Epoch 531/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 223413433.2217\n",
            "Epoch 532/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 288352997.5172\n",
            "Epoch 533/1030\n",
            "203/203 [==============================] - 0s 185us/step - loss: 291259199.3695\n",
            "Epoch 534/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 347882655.2906\n",
            "Epoch 535/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 293191038.5813\n",
            "Epoch 536/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 316651495.5665\n",
            "Epoch 537/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 303654475.0345\n",
            "Epoch 538/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 302838090.7980\n",
            "Epoch 539/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 402569861.4384\n",
            "Epoch 540/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 315446291.5468\n",
            "Epoch 541/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 334864635.5862\n",
            "Epoch 542/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 259088204.8079\n",
            "Epoch 543/1030\n",
            "203/203 [==============================] - 0s 184us/step - loss: 251524426.9951\n",
            "Epoch 544/1030\n",
            "203/203 [==============================] - 0s 186us/step - loss: 345349065.9310\n",
            "Epoch 545/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 259957114.0099\n",
            "Epoch 546/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 263986495.7241\n",
            "Epoch 547/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 229111124.7291\n",
            "Epoch 548/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 270355882.3251\n",
            "Epoch 549/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 235770284.9261\n",
            "Epoch 550/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 276099707.7438\n",
            "Epoch 551/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 227010882.3645\n",
            "Epoch 552/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 313975787.5074\n",
            "Epoch 553/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 302183177.7734\n",
            "Epoch 554/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 257826703.2906\n",
            "Epoch 555/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 245414213.2808\n",
            "Epoch 556/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 236657431.6453\n",
            "Epoch 557/1030\n",
            "203/203 [==============================] - 0s 188us/step - loss: 265652244.1773\n",
            "Epoch 558/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 331100523.5074\n",
            "Epoch 559/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 264087457.3005\n",
            "Epoch 560/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 398531702.6995\n",
            "Epoch 561/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 316990433.7340\n",
            "Epoch 562/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 283637293.4778\n",
            "Epoch 563/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 264959808.7094\n",
            "Epoch 564/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 285631127.9606\n",
            "Epoch 565/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 421086862.9754\n",
            "Epoch 566/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 298795228.5320\n",
            "Epoch 567/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 322274581.9113\n",
            "Epoch 568/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 280333383.2512\n",
            "Epoch 569/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 200926888.7882\n",
            "Epoch 570/1030\n",
            "203/203 [==============================] - 0s 182us/step - loss: 289620984.0394\n",
            "Epoch 571/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 287742595.6256\n",
            "Epoch 572/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 268052415.0542\n",
            "Epoch 573/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 285218740.3350\n",
            "Epoch 574/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 232812762.0099\n",
            "Epoch 575/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 233329275.2709\n",
            "Epoch 576/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 236755574.1478\n",
            "Epoch 577/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 261747035.8227\n",
            "Epoch 578/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 229301736.8276\n",
            "Epoch 579/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 192245468.5320\n",
            "Epoch 580/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 298850937.0640\n",
            "Epoch 581/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 571105977.3793\n",
            "Epoch 582/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 386524386.0493\n",
            "Epoch 583/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 328060081.3399\n",
            "Epoch 584/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 265508163.3103\n",
            "Epoch 585/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 365221687.9606\n",
            "Epoch 586/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 306181073.8128\n",
            "Epoch 587/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 410213360.5517\n",
            "Epoch 588/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 233678127.7635\n",
            "Epoch 589/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 248926192.6305\n",
            "Epoch 590/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 249395414.0690\n",
            "Epoch 591/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 243846638.7389\n",
            "Epoch 592/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 234713222.6995\n",
            "Epoch 593/1030\n",
            "203/203 [==============================] - 0s 143us/step - loss: 262974669.3990\n",
            "Epoch 594/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 283627266.8374\n",
            "Epoch 595/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 234485218.5222\n",
            "Epoch 596/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 255599863.3300\n",
            "Epoch 597/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 182198931.2315\n",
            "Epoch 598/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 256208520.3547\n",
            "Epoch 599/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 262293025.2611\n",
            "Epoch 600/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 275824368.7094\n",
            "Epoch 601/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 276351908.2562\n",
            "Epoch 602/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 245536126.1084\n",
            "Epoch 603/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 243632932.0985\n",
            "Epoch 604/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 286851051.4286\n",
            "Epoch 605/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 263048800.0394\n",
            "Epoch 606/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 233953982.5813\n",
            "Epoch 607/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 247653979.1133\n",
            "Epoch 608/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 213454595.4680\n",
            "Epoch 609/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 249838298.0099\n",
            "Epoch 610/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 322258444.2167\n",
            "Epoch 611/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 232762390.3054\n",
            "Epoch 612/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 274426554.2463\n",
            "Epoch 613/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 229321099.6650\n",
            "Epoch 614/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 226895351.8227\n",
            "Epoch 615/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 232543769.2217\n",
            "Epoch 616/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 284871354.6404\n",
            "Epoch 617/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 247865722.9557\n",
            "Epoch 618/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 291388639.5271\n",
            "Epoch 619/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 308869497.1429\n",
            "Epoch 620/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 265377895.1724\n",
            "Epoch 621/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 284741006.8966\n",
            "Epoch 622/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 274619166.7389\n",
            "Epoch 623/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 434035225.2217\n",
            "Epoch 624/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 375725337.6946\n",
            "Epoch 625/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 304063528.6700\n",
            "Epoch 626/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 378687308.6108\n",
            "Epoch 627/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 304375874.9557\n",
            "Epoch 628/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 295205594.7980\n",
            "Epoch 629/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 250664188.2167\n",
            "Epoch 630/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 299586119.8818\n",
            "Epoch 631/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 309556862.8966\n",
            "Epoch 632/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 252748767.3695\n",
            "Epoch 633/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 285300881.6552\n",
            "Epoch 634/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 383881039.7635\n",
            "Epoch 635/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 293444625.3399\n",
            "Epoch 636/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 292872161.7340\n",
            "Epoch 637/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 278979381.9113\n",
            "Epoch 638/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 266934461.9507\n",
            "Epoch 639/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 309581561.5369\n",
            "Epoch 640/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 220768230.1478\n",
            "Epoch 641/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 260484394.8768\n",
            "Epoch 642/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 302896221.5172\n",
            "Epoch 643/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 269158237.4778\n",
            "Epoch 644/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 308919516.5911\n",
            "Epoch 645/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 223472421.0443\n",
            "Epoch 646/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 303579222.5419\n",
            "Epoch 647/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 222795403.7438\n",
            "Epoch 648/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 280980986.0887\n",
            "Epoch 649/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 253839858.7586\n",
            "Epoch 650/1030\n",
            "203/203 [==============================] - 0s 148us/step - loss: 387407158.3842\n",
            "Epoch 651/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 260555226.0099\n",
            "Epoch 652/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 237033713.1823\n",
            "Epoch 653/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 410309000.3941\n",
            "Epoch 654/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 515951998.5813\n",
            "Epoch 655/1030\n",
            "203/203 [==============================] - 0s 181us/step - loss: 400907114.3251\n",
            "Epoch 656/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 330028523.9409\n",
            "Epoch 657/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 280410096.2365\n",
            "Epoch 658/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 319454803.5468\n",
            "Epoch 659/1030\n",
            "203/203 [==============================] - 0s 141us/step - loss: 245294170.9557\n",
            "Epoch 660/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 250878597.9113\n",
            "Epoch 661/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 352565913.2217\n",
            "Epoch 662/1030\n",
            "203/203 [==============================] - 0s 183us/step - loss: 267119849.2217\n",
            "Epoch 663/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 226628609.2611\n",
            "Epoch 664/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 267411209.6158\n",
            "Epoch 665/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 297923004.2167\n",
            "Epoch 666/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 300834560.1576\n",
            "Epoch 667/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 386778295.6453\n",
            "Epoch 668/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 286547752.1970\n",
            "Epoch 669/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 228761681.1034\n",
            "Epoch 670/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 236063121.2611\n",
            "Epoch 671/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 225632430.6601\n",
            "Epoch 672/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 242475971.2709\n",
            "Epoch 673/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 213418893.3202\n",
            "Epoch 674/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 268452215.1724\n",
            "Epoch 675/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 253180759.2512\n",
            "Epoch 676/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 243683357.8719\n",
            "Epoch 677/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 282228175.7635\n",
            "Epoch 678/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 290339378.0493\n",
            "Epoch 679/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 218112779.8227\n",
            "Epoch 680/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 227550674.6010\n",
            "Epoch 681/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 300020227.9409\n",
            "Epoch 682/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 223801316.7291\n",
            "Epoch 683/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 253687786.7192\n",
            "Epoch 684/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 251705407.3695\n",
            "Epoch 685/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 255488929.3399\n",
            "Epoch 686/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 224107703.0148\n",
            "Epoch 687/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 276234678.3842\n",
            "Epoch 688/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 245755444.7291\n",
            "Epoch 689/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 219339084.1379\n",
            "Epoch 690/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 208810088.1970\n",
            "Epoch 691/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 234041911.7241\n",
            "Epoch 692/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 267043996.3744\n",
            "Epoch 693/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 236575163.1133\n",
            "Epoch 694/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 227085521.6552\n",
            "Epoch 695/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 229479279.7635\n",
            "Epoch 696/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 268727640.9064\n",
            "Epoch 697/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 483921035.5862\n",
            "Epoch 698/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 315747409.8128\n",
            "Epoch 699/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 283774100.5714\n",
            "Epoch 700/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 328296587.3498\n",
            "Epoch 701/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 248952616.3547\n",
            "Epoch 702/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 485716203.0345\n",
            "Epoch 703/1030\n",
            "203/203 [==============================] - 0s 182us/step - loss: 363556887.1724\n",
            "Epoch 704/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 338899327.2118\n",
            "Epoch 705/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 390744066.3645\n",
            "Epoch 706/1030\n",
            "203/203 [==============================] - 0s 144us/step - loss: 316538838.0690\n",
            "Epoch 707/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 369291299.7833\n",
            "Epoch 708/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 463698675.9409\n",
            "Epoch 709/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 324328828.2167\n",
            "Epoch 710/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 222108604.8473\n",
            "Epoch 711/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 287394954.7980\n",
            "Epoch 712/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 310457522.7586\n",
            "Epoch 713/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 262735545.3793\n",
            "Epoch 714/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 256795326.3448\n",
            "Epoch 715/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 226218332.4926\n",
            "Epoch 716/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 264095990.8571\n",
            "Epoch 717/1030\n",
            "203/203 [==============================] - 0s 141us/step - loss: 309243313.5764\n",
            "Epoch 718/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 266475274.4828\n",
            "Epoch 719/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 217313395.7044\n",
            "Epoch 720/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 224678798.9754\n",
            "Epoch 721/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 263505799.8818\n",
            "Epoch 722/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 224361783.5665\n",
            "Epoch 723/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 263033358.6601\n",
            "Epoch 724/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 219203195.9015\n",
            "Epoch 725/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 275159467.9803\n",
            "Epoch 726/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 262078011.9015\n",
            "Epoch 727/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 291189216.0000\n",
            "Epoch 728/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 262321525.9113\n",
            "Epoch 729/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 248898312.9852\n",
            "Epoch 730/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 257238075.2709\n",
            "Epoch 731/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 260686282.7192\n",
            "Epoch 732/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 392197300.3350\n",
            "Epoch 733/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 291800413.0049\n",
            "Epoch 734/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 267169489.3005\n",
            "Epoch 735/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 264465495.0936\n",
            "Epoch 736/1030\n",
            "203/203 [==============================] - 0s 212us/step - loss: 292465471.6453\n",
            "Epoch 737/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 223467862.1478\n",
            "Epoch 738/1030\n",
            "203/203 [==============================] - 0s 192us/step - loss: 223824180.9655\n",
            "Epoch 739/1030\n",
            "203/203 [==============================] - 0s 184us/step - loss: 271016608.1576\n",
            "Epoch 740/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 259259678.4236\n",
            "Epoch 741/1030\n",
            "203/203 [==============================] - 0s 190us/step - loss: 291179519.5271\n",
            "Epoch 742/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 282465186.6798\n",
            "Epoch 743/1030\n",
            "203/203 [==============================] - 0s 186us/step - loss: 220094079.4089\n",
            "Epoch 744/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 244728669.3202\n",
            "Epoch 745/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 233738346.4039\n",
            "Epoch 746/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 231017433.3793\n",
            "Epoch 747/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 328150454.2266\n",
            "Epoch 748/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 629270809.8522\n",
            "Epoch 749/1030\n",
            "203/203 [==============================] - 0s 185us/step - loss: 421815348.8867\n",
            "Epoch 750/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 266934074.4039\n",
            "Epoch 751/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 305895920.8276\n",
            "Epoch 752/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 306303902.1084\n",
            "Epoch 753/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 284293071.7635\n",
            "Epoch 754/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 244993931.3498\n",
            "Epoch 755/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 251533282.8374\n",
            "Epoch 756/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 259241744.0788\n",
            "Epoch 757/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 216900710.0690\n",
            "Epoch 758/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 249149084.3744\n",
            "Epoch 759/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 221898181.8325\n",
            "Epoch 760/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 253715919.9212\n",
            "Epoch 761/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 223235031.5665\n",
            "Epoch 762/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 255304835.7044\n",
            "Epoch 763/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 259306605.4778\n",
            "Epoch 764/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 327272248.3547\n",
            "Epoch 765/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 259473321.9310\n",
            "Epoch 766/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 318808096.3153\n",
            "Epoch 767/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 302493509.3596\n",
            "Epoch 768/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 289759242.2463\n",
            "Epoch 769/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 266715429.6749\n",
            "Epoch 770/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 316846344.5123\n",
            "Epoch 771/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 414752981.7537\n",
            "Epoch 772/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 407429223.4089\n",
            "Epoch 773/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 275569522.6798\n",
            "Epoch 774/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 236363877.2020\n",
            "Epoch 775/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 272177391.6059\n",
            "Epoch 776/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 263700557.7931\n",
            "Epoch 777/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 368659893.9113\n",
            "Epoch 778/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 466420278.2266\n",
            "Epoch 779/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 333732537.6946\n",
            "Epoch 780/1030\n",
            "203/203 [==============================] - 0s 189us/step - loss: 255913484.1379\n",
            "Epoch 781/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 278971676.3744\n",
            "Epoch 782/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 282493604.5714\n",
            "Epoch 783/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 492379920.3153\n",
            "Epoch 784/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 447121080.1182\n",
            "Epoch 785/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 347794746.0099\n",
            "Epoch 786/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 328580476.5320\n",
            "Epoch 787/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 321168079.7635\n",
            "Epoch 788/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 263642055.2512\n",
            "Epoch 789/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 222988243.8621\n",
            "Epoch 790/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 277662831.2906\n",
            "Epoch 791/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 221239041.8916\n",
            "Epoch 792/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 255437329.2611\n",
            "Epoch 793/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 263898603.5074\n",
            "Epoch 794/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 299302248.1970\n",
            "Epoch 795/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 342187457.5764\n",
            "Epoch 796/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 270397447.4877\n",
            "Epoch 797/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 290099662.0296\n",
            "Epoch 798/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 309062361.2217\n",
            "Epoch 799/1030\n",
            "203/203 [==============================] - 0s 181us/step - loss: 249521861.9901\n",
            "Epoch 800/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 340856832.7882\n",
            "Epoch 801/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 242280753.5764\n",
            "Epoch 802/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 223124325.2020\n",
            "Epoch 803/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 239212102.7783\n",
            "Epoch 804/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 268549306.1675\n",
            "Epoch 805/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 290845264.3941\n",
            "Epoch 806/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 282284202.3645\n",
            "Epoch 807/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 244300049.3399\n",
            "Epoch 808/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 286750786.2069\n",
            "Epoch 809/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 239364346.0099\n",
            "Epoch 810/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 293981073.3399\n",
            "Epoch 811/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 282359570.1281\n",
            "Epoch 812/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 308909646.3448\n",
            "Epoch 813/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 373065226.9557\n",
            "Epoch 814/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 391624151.6453\n",
            "Epoch 815/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 301104213.7537\n",
            "Epoch 816/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 244169060.2562\n",
            "Epoch 817/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 243354073.5369\n",
            "Epoch 818/1030\n",
            "203/203 [==============================] - 0s 190us/step - loss: 205936450.5222\n",
            "Epoch 819/1030\n",
            "203/203 [==============================] - 0s 197us/step - loss: 274081791.6059\n",
            "Epoch 820/1030\n",
            "203/203 [==============================] - 0s 187us/step - loss: 315825216.6305\n",
            "Epoch 821/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 309827948.2956\n",
            "Epoch 822/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 388273957.2020\n",
            "Epoch 823/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 360957415.4089\n",
            "Epoch 824/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 345668421.8325\n",
            "Epoch 825/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 254210791.5665\n",
            "Epoch 826/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 238421404.8473\n",
            "Epoch 827/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 289347553.4975\n",
            "Epoch 828/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 225262755.5468\n",
            "Epoch 829/1030\n",
            "203/203 [==============================] - 0s 148us/step - loss: 261159591.4877\n",
            "Epoch 830/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 235036591.7241\n",
            "Epoch 831/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 250814800.0788\n",
            "Epoch 832/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 291994682.0099\n",
            "Epoch 833/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 254126359.5271\n",
            "Epoch 834/1030\n",
            "203/203 [==============================] - 0s 188us/step - loss: 241382980.3350\n",
            "Epoch 835/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 187429519.6059\n",
            "Epoch 836/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 245646618.6404\n",
            "Epoch 837/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 259865890.8374\n",
            "Epoch 838/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 226535398.7783\n",
            "Epoch 839/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 290472709.5961\n",
            "Epoch 840/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 321331882.5616\n",
            "Epoch 841/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 419670952.8276\n",
            "Epoch 842/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 434196311.8030\n",
            "Epoch 843/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 354487339.5862\n",
            "Epoch 844/1030\n",
            "203/203 [==============================] - 0s 182us/step - loss: 274526469.8325\n",
            "Epoch 845/1030\n",
            "203/203 [==============================] - 0s 188us/step - loss: 287304356.0985\n",
            "Epoch 846/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 218492573.1626\n",
            "Epoch 847/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 274792771.1527\n",
            "Epoch 848/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 272577193.3005\n",
            "Epoch 849/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 194098239.8424\n",
            "Epoch 850/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 265257072.7094\n",
            "Epoch 851/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 258745222.8571\n",
            "Epoch 852/1030\n",
            "203/203 [==============================] - 0s 177us/step - loss: 209745323.3498\n",
            "Epoch 853/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 499719218.1281\n",
            "Epoch 854/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 419492632.0394\n",
            "Epoch 855/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 295863101.7143\n",
            "Epoch 856/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 286192970.0887\n",
            "Epoch 857/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 401840948.9655\n",
            "Epoch 858/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 316007845.3596\n",
            "Epoch 859/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 288572067.6256\n",
            "Epoch 860/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 248847561.4581\n",
            "Epoch 861/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 209782484.1773\n",
            "Epoch 862/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 217963534.9754\n",
            "Epoch 863/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 230853981.4778\n",
            "Epoch 864/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 286573605.5172\n",
            "Epoch 865/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 249031424.9458\n",
            "Epoch 866/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 255917674.1675\n",
            "Epoch 867/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 220476325.9901\n",
            "Epoch 868/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 250243044.5714\n",
            "Epoch 869/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 307626417.8128\n",
            "Epoch 870/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 231002834.9163\n",
            "Epoch 871/1030\n",
            "203/203 [==============================] - 0s 187us/step - loss: 278242625.4975\n",
            "Epoch 872/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 249044918.5419\n",
            "Epoch 873/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 388365742.8177\n",
            "Epoch 874/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 225648293.4384\n",
            "Epoch 875/1030\n",
            "203/203 [==============================] - 0s 202us/step - loss: 254614633.5369\n",
            "Epoch 876/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 253231296.0788\n",
            "Epoch 877/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 355781558.2266\n",
            "Epoch 878/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 224301492.4926\n",
            "Epoch 879/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 268369746.2069\n",
            "Epoch 880/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 238358211.8621\n",
            "Epoch 881/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 245514984.4335\n",
            "Epoch 882/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 228462751.5271\n",
            "Epoch 883/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 230087248.4729\n",
            "Epoch 884/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 329625931.6650\n",
            "Epoch 885/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 222142352.0197\n",
            "Epoch 886/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 273187680.4729\n",
            "Epoch 887/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 274700206.9754\n",
            "Epoch 888/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 369648960.4729\n",
            "Epoch 889/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 374354184.3547\n",
            "Epoch 890/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 246293875.7044\n",
            "Epoch 891/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 212483585.8128\n",
            "Epoch 892/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 229121630.5025\n",
            "Epoch 893/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 295508736.6305\n",
            "Epoch 894/1030\n",
            "203/203 [==============================] - 0s 178us/step - loss: 845330919.7241\n",
            "Epoch 895/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 466105845.7537\n",
            "Epoch 896/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 380355403.9803\n",
            "Epoch 897/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 257211296.4335\n",
            "Epoch 898/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 263256739.1527\n",
            "Epoch 899/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 304022453.5172\n",
            "Epoch 900/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 305686227.8621\n",
            "Epoch 901/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 327791477.7537\n",
            "Epoch 902/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 256913062.3842\n",
            "Epoch 903/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 228643819.1133\n",
            "Epoch 904/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 242269984.9458\n",
            "Epoch 905/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 253166621.8719\n",
            "Epoch 906/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 244441043.5468\n",
            "Epoch 907/1030\n",
            "203/203 [==============================] - 0s 184us/step - loss: 282748263.7241\n",
            "Epoch 908/1030\n",
            "203/203 [==============================] - 0s 183us/step - loss: 282264096.7094\n",
            "Epoch 909/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 370051203.5468\n",
            "Epoch 910/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 260875782.7783\n",
            "Epoch 911/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 318585022.1872\n",
            "Epoch 912/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 371616094.5813\n",
            "Epoch 913/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 333407457.5764\n",
            "Epoch 914/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 279608606.7783\n",
            "Epoch 915/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 228937978.0099\n",
            "Epoch 916/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 394236532.4926\n",
            "Epoch 917/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 392397960.1182\n",
            "Epoch 918/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 262342674.1281\n",
            "Epoch 919/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 234503889.9704\n",
            "Epoch 920/1030\n",
            "203/203 [==============================] - 0s 182us/step - loss: 218433820.8473\n",
            "Epoch 921/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 229143294.1084\n",
            "Epoch 922/1030\n",
            "203/203 [==============================] - 0s 184us/step - loss: 257052912.0000\n",
            "Epoch 923/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 246410875.8227\n",
            "Epoch 924/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 281168331.5074\n",
            "Epoch 925/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 237388548.9655\n",
            "Epoch 926/1030\n",
            "203/203 [==============================] - 0s 171us/step - loss: 190503528.9064\n",
            "Epoch 927/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 282381477.9901\n",
            "Epoch 928/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 276807135.4483\n",
            "Epoch 929/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 276338507.6650\n",
            "Epoch 930/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 202049504.2365\n",
            "Epoch 931/1030\n",
            "203/203 [==============================] - 0s 167us/step - loss: 248199283.4680\n",
            "Epoch 932/1030\n",
            "203/203 [==============================] - 0s 181us/step - loss: 195373086.5813\n",
            "Epoch 933/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 257666513.4187\n",
            "Epoch 934/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 197275844.4926\n",
            "Epoch 935/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 219670342.6995\n",
            "Epoch 936/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 219494139.4286\n",
            "Epoch 937/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 235963611.9015\n",
            "Epoch 938/1030\n",
            "203/203 [==============================] - 0s 149us/step - loss: 242842674.2069\n",
            "Epoch 939/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 234736370.2463\n",
            "Epoch 940/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 234379476.3350\n",
            "Epoch 941/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 280731662.3448\n",
            "Epoch 942/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 407164493.3990\n",
            "Epoch 943/1030\n",
            "203/203 [==============================] - 0s 180us/step - loss: 316687577.3005\n",
            "Epoch 944/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 226588361.2217\n",
            "Epoch 945/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 269681460.6502\n",
            "Epoch 946/1030\n",
            "203/203 [==============================] - 0s 148us/step - loss: 235294447.9212\n",
            "Epoch 947/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 209195884.1379\n",
            "Epoch 948/1030\n",
            "203/203 [==============================] - 0s 188us/step - loss: 240289694.1084\n",
            "Epoch 949/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 191006697.1429\n",
            "Epoch 950/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 231821778.5222\n",
            "Epoch 951/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 204778918.3054\n",
            "Epoch 952/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 285806655.6847\n",
            "Epoch 953/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 213850858.1675\n",
            "Epoch 954/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 230496279.1724\n",
            "Epoch 955/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 253431492.3350\n",
            "Epoch 956/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 231798050.5222\n",
            "Epoch 957/1030\n",
            "203/203 [==============================] - 0s 183us/step - loss: 200606408.8276\n",
            "Epoch 958/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 232009937.1823\n",
            "Epoch 959/1030\n",
            "203/203 [==============================] - 0s 175us/step - loss: 283370172.5320\n",
            "Epoch 960/1030\n",
            "203/203 [==============================] - 0s 174us/step - loss: 274037356.2167\n",
            "Epoch 961/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 287412031.4483\n",
            "Epoch 962/1030\n",
            "203/203 [==============================] - 0s 179us/step - loss: 257265193.7734\n",
            "Epoch 963/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 261791298.7586\n",
            "Epoch 964/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 250056403.2315\n",
            "Epoch 965/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 221197753.3005\n",
            "Epoch 966/1030\n",
            "203/203 [==============================] - 0s 145us/step - loss: 300800298.5616\n",
            "Epoch 967/1030\n",
            "203/203 [==============================] - 0s 147us/step - loss: 310929985.7340\n",
            "Epoch 968/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 287070475.2709\n",
            "Epoch 969/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 226722028.6502\n",
            "Epoch 970/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 253823037.4778\n",
            "Epoch 971/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 229338950.6995\n",
            "Epoch 972/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 274606835.0739\n",
            "Epoch 973/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 324445463.8818\n",
            "Epoch 974/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 208230746.6798\n",
            "Epoch 975/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 206621608.3547\n",
            "Epoch 976/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 242014918.9360\n",
            "Epoch 977/1030\n",
            "203/203 [==============================] - 0s 164us/step - loss: 227617528.2759\n",
            "Epoch 978/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 271524437.1232\n",
            "Epoch 979/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 259733219.4680\n",
            "Epoch 980/1030\n",
            "203/203 [==============================] - 0s 185us/step - loss: 273117097.9310\n",
            "Epoch 981/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 292487073.0246\n",
            "Epoch 982/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 318869508.8867\n",
            "Epoch 983/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 330545929.8522\n",
            "Epoch 984/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 242131410.6798\n",
            "Epoch 985/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 255561580.5320\n",
            "Epoch 986/1030\n",
            "203/203 [==============================] - 0s 161us/step - loss: 327371584.0000\n",
            "Epoch 987/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 264496242.7980\n",
            "Epoch 988/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 509826662.2660\n",
            "Epoch 989/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 390464338.2857\n",
            "Epoch 990/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 293325826.2069\n",
            "Epoch 991/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 247625116.9655\n",
            "Epoch 992/1030\n",
            "203/203 [==============================] - 0s 165us/step - loss: 250231800.1182\n",
            "Epoch 993/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 351445784.8276\n",
            "Epoch 994/1030\n",
            "203/203 [==============================] - 0s 168us/step - loss: 206818051.9409\n",
            "Epoch 995/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 197536564.9655\n",
            "Epoch 996/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 249327666.5222\n",
            "Epoch 997/1030\n",
            "203/203 [==============================] - 0s 143us/step - loss: 268922891.3498\n",
            "Epoch 998/1030\n",
            "203/203 [==============================] - 0s 166us/step - loss: 262295474.3645\n",
            "Epoch 999/1030\n",
            "203/203 [==============================] - 0s 170us/step - loss: 221212729.0640\n",
            "Epoch 1000/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 211284565.9113\n",
            "Epoch 1001/1030\n",
            "203/203 [==============================] - 0s 158us/step - loss: 279164095.8424\n",
            "Epoch 1002/1030\n",
            "203/203 [==============================] - 0s 172us/step - loss: 219044099.1527\n",
            "Epoch 1003/1030\n",
            "203/203 [==============================] - 0s 148us/step - loss: 222518677.2808\n",
            "Epoch 1004/1030\n",
            "203/203 [==============================] - 0s 201us/step - loss: 233714597.5961\n",
            "Epoch 1005/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 296175629.8719\n",
            "Epoch 1006/1030\n",
            "203/203 [==============================] - 0s 173us/step - loss: 291851325.2808\n",
            "Epoch 1007/1030\n",
            "203/203 [==============================] - 0s 160us/step - loss: 290118842.6404\n",
            "Epoch 1008/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 527484597.2808\n",
            "Epoch 1009/1030\n",
            "203/203 [==============================] - 0s 163us/step - loss: 392282003.3892\n",
            "Epoch 1010/1030\n",
            "203/203 [==============================] - 0s 152us/step - loss: 429572007.4089\n",
            "Epoch 1011/1030\n",
            "203/203 [==============================] - 0s 148us/step - loss: 448748614.6207\n",
            "Epoch 1012/1030\n",
            "203/203 [==============================] - 0s 176us/step - loss: 307855818.5616\n",
            "Epoch 1013/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 251038130.3645\n",
            "Epoch 1014/1030\n",
            "203/203 [==============================] - 0s 159us/step - loss: 234498052.2562\n",
            "Epoch 1015/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 318016580.8867\n",
            "Epoch 1016/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 372057857.1823\n",
            "Epoch 1017/1030\n",
            "203/203 [==============================] - 0s 169us/step - loss: 287288314.6404\n",
            "Epoch 1018/1030\n",
            "203/203 [==============================] - 0s 182us/step - loss: 213929008.3941\n",
            "Epoch 1019/1030\n",
            "203/203 [==============================] - 0s 155us/step - loss: 233669199.2906\n",
            "Epoch 1020/1030\n",
            "203/203 [==============================] - 0s 151us/step - loss: 214104336.1576\n",
            "Epoch 1021/1030\n",
            "203/203 [==============================] - 0s 162us/step - loss: 233849321.9310\n",
            "Epoch 1022/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 253685118.5025\n",
            "Epoch 1023/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 242943330.5222\n",
            "Epoch 1024/1030\n",
            "203/203 [==============================] - 0s 143us/step - loss: 235377901.4778\n",
            "Epoch 1025/1030\n",
            "203/203 [==============================] - 0s 156us/step - loss: 260053754.8374\n",
            "Epoch 1026/1030\n",
            "203/203 [==============================] - 0s 154us/step - loss: 213320720.7094\n",
            "Epoch 1027/1030\n",
            "203/203 [==============================] - 0s 157us/step - loss: 235286120.8276\n",
            "Epoch 1028/1030\n",
            "203/203 [==============================] - 0s 150us/step - loss: 272997169.4975\n",
            "Epoch 1029/1030\n",
            "203/203 [==============================] - 0s 153us/step - loss: 267291017.7734\n",
            "Epoch 1030/1030\n",
            "203/203 [==============================] - 0s 146us/step - loss: 258572068.3350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5e11baa940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGmLWdIISLpb",
        "colab_type": "code",
        "outputId": "22546fd0-8143-47f9-d9f3-3cbe6b4cea60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88/88 [==============================] - 1s 15ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "146739072.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBLlmOFQ3Tlj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "211fcdc3-c2a3-4a25-d88a-e1fcac38d365"
      },
      "source": [
        "r2_score(Y_test, model.predict(X_test), multioutput='uniform_average')"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9177452784596352"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    }
  ]
}