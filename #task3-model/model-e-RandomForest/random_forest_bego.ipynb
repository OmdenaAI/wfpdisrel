{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import math\n",
    "import joblib \n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# income level to numerical\n",
    "# MinMaxScaler()\n",
    "# Feature importance https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html\n",
    "# https://www.kaggle.com/residentmario/automated-feature-selection-with-sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "\n",
    "# import data\n",
    "df = pd.read_csv('../../#task4-eda/datasets/OUTPUT_WBI_exposer_cyclones_v5.csv', sep=';')\n",
    "df.columns = [e.lower().replace(' ','_') for e in df.columns]\n",
    "\n",
    "# separate X and y\n",
    "X = df.drop(['total_affected'], axis=1)\n",
    "y = df['total_affected']\n",
    "\n",
    "# types of features\n",
    "num_features = list(df.loc[:, df.dtypes != object].columns)\n",
    "cat_features = list(df.loc[:, df.dtypes == object].columns)\n",
    "\n",
    "# fixing types of features\n",
    "num_features.append('income_level_final')\n",
    "cat_features.remove('income_level_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom transformers\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    # selects a slice of the dataset based on feature_names\n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[set(X.columns).intersection(self.feature_names)]\n",
    "\n",
    "\n",
    "class TransformerCat(BaseEstimator, TransformerMixin):\n",
    "    # transforms categorical features\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.features_to_keep = []\n",
    "        self.features_to_drop = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.features_to_keep = ['basin']\n",
    "        self.features_to_drop= ['sid', 'name', 'iso', 'sub_basin', 'nature', 'iso_time',\n",
    "       'coords']\n",
    "        return self\n",
    "    \n",
    "    def _get_income_level(self, income):\n",
    "        order = {'High': 4, 'High_Middle': 3, 'Low_Middle': 2, 'Low': 1 }\n",
    "        return order[income]\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.drop(self.features_to_drop, axis = 1)\n",
    "        return X\n",
    "    \n",
    "class TransformerNum(BaseEstimator, TransformerMixin):\n",
    "    # transforms numerical features\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.features_to_keep = []\n",
    "        self.features_to_drop = []\n",
    "        self.features_to_log = []\n",
    "    \n",
    "    def _get_income_level(self, income):\n",
    "        order = {'High': 4, 'High_Middle': 3, 'Low_Middle': 2, 'Low': 1 }\n",
    "        return order[income]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # do not drop ['total_hrs', 'wind_calc_mean', 'storm_spd_mean', 'v_land_kn', \\\n",
    "        # 'population_density_(people_per_sq._km_of_land_area)', 'rural_population_(%_of_total_population)', \\\n",
    "        # 'income_level_final', 'pop_max_34', 'pop_max_50', 'pop_max_64'] based on Arnab's Analysis\n",
    "        \n",
    "        \n",
    "        self.features_to_keep = [['96kn_assets', 'cpi', 'pop_max_50', \n",
    "                                 'population_density_(people_per_sq._km_of_land_area)', \n",
    "                                 'life_expectancy_at_birth,_total_(years)', \n",
    "                                 'v_land_kn', 'pres_cal_max', 'wind_cal_max', 'wind_calc_mean', \n",
    "                                 'total_hrs', 'storm_spd_mean', 'pop_max_64', 'total_affected', \n",
    "                                 'gdp_per_capita_(constant_2010_us$)', 'rural_population_(%_of_total_population)', \n",
    "                                 'storm_spd_max', 'pop_max_34', 'pres_calc_mean', 'income_level_final']]\n",
    "    \n",
    "        self.features_to_drop = ['usa_sshs', 'year', 'day_hrs', 'night_hrs', '34kn_assets', '64kn_assets', \n",
    "                                'total_damage_(000$)', 'total_deaths', 'air_transport,_freight_(million_ton-km)', \n",
    "                                'arable_land_(hectares_per_person)', 'cereal_yield_(kg_per_hectare)', \n",
    "                                'food_production_index_(2004-2006_=_100)', 'gdp_growth_(annual_%)', \n",
    "                                'net_flows_from_un_agencies_us$', 'mobile_cellular_subscriptions_(per_100_people)', \n",
    "                                'adjusted_savings:_education_expenditure_(%_of_gni)',\n",
    "                                'storm_dr_max', 'storm_dr_mean', \n",
    "                                'storm_spd_min', 'storm_dr_min', 'pres_cal_min', 'wind_cal_min']\n",
    "        \n",
    "        self.features_to_log = ['population_density_(people_per_sq._km_of_land_area)',\n",
    "                               'pop_max_50', 'pop_max_64']\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _get_log(self, f):\n",
    "        try:\n",
    "            return math.log(f)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        X['income_level_final'] = X['income_level_final'].apply(self._get_income_level)\n",
    "        X = X.drop(self.features_to_drop, axis = 1)\n",
    "        for f in self.features_to_log:\n",
    "            X[f] = X[f].apply(self._get_log)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess pipeline\n",
    "\n",
    "preprocess_pipeline = FeatureUnion(transformer_list =[\n",
    "        (\"num_pipeline\", Pipeline([\n",
    "            ('selector', DataFrameSelector(num_features)),\n",
    "            ('num_transformer', TransformerNum()),\n",
    "            ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "            ('scaler', StandardScaler()),\n",
    "            # ('uniform_distribution', QuantileTransformer(random_state=42)) # decreasing performance\n",
    "            # ('pca', PCA(0.99)) # decreasing performance\n",
    "             ])),\n",
    "        (\"cat_pipeline\", Pipeline([\n",
    "            ('selector', DataFrameSelector(cat_features)),\n",
    "            ('cat_transformer', TransformerCat()),\n",
    "            ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse = False)), # TODO: not encode income_level_final\n",
    "            # (\"scaler\", StandardScaler()) # decreasing performance\n",
    "            ]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    # evaluates different models using cross validation on the whole dataset\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.mean_scores = dict()\n",
    "        \n",
    "    def evaluate_models(self, models):\n",
    "        for model_name, model in models.items():\n",
    "            print(f'***{model_name}***')\n",
    "            self._evaluate_model(model_name, model)\n",
    "            print('\\n')\n",
    "            \n",
    "    def _display_scores(self, scores):\n",
    "            print(\"Scores:\", scores)\n",
    "            print(\"Mean:\", scores.mean())\n",
    "            print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "    def _evaluate_model(self, model_name, model):\n",
    "        transformed_X = preprocess_pipeline.fit_transform(self.X)\n",
    "        scores = cross_val_score(model, transformed_X, self.y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "        tree_rmse_scores = np.sqrt(-scores)\n",
    "        self._display_scores(tree_rmse_scores)\n",
    "        self.mean_scores[model_name] = tree_rmse_scores.mean()\n",
    "        \n",
    "    def save_models(self, models):\n",
    "        for model_name, model in models.items():\n",
    "            transformed_X = preprocess_pipeline.fit_transform(self.X)\n",
    "            model.fit(transformed_X, self.y)\n",
    "            joblib.dump(model, f'./models/{datetime.datetime.now().date()}_bego_{model_name}_{int(self.mean_scores[model_name])}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# grid search random_forest\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': list(range(10,200,30)), 'max_features': list(range(2, 10, 2)), 'min_samples_leaf':[2,4,6]},\n",
    "    {'bootstrap': [False], 'n_estimators': list(range(10,200,30)), 'max_features': list(range(2, 10, 2)), 'min_samples_leaf':[2,4,6]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "grid_search_forest = GridSearchCV(forest_reg, \n",
    "                                  param_grid, \n",
    "                                  cv=6, \n",
    "                                  scoring='neg_mean_squared_error', \n",
    "                                  return_train_score=True, \n",
    "                                  iid = True, \n",
    "                                  verbose=1) # higher verbose desactivated for Github visualization purposes\n",
    "transformed_X = preprocess_pipeline.fit_transform(X)\n",
    "grid_search_forest.fit(transformed_X, y)\n",
    "\n",
    "print(grid_search_forest.score(transformed_X, y))\n",
    "print(grid_search_forest.best_params_)\"\"\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***forest_1***\n",
      "Scores: [1370135.30357728 1467502.85375128 1545967.82330743 2033419.86963731\n",
      " 1681181.03382911 3364613.62252863 1424137.54499329 1499352.50608047\n",
      " 2344327.8899283  2884502.59315723]\n",
      "Mean: 1961514.1040790346\n",
      "Standard deviation: 657245.8516561881\n",
      "\n",
      "\n",
      "***forest_2***\n",
      "Scores: [1041653.16647244 1186679.68707165 1395298.67407935 2029581.5359116\n",
      " 1383421.08741117 3632794.22043755 1313248.38016145 1296766.38022253\n",
      " 2228743.93373959 2115604.62084664]\n",
      "Mean: 1762379.1686353975\n",
      "Standard deviation: 738908.7087909199\n",
      "\n",
      "\n",
      "***forest_3***\n",
      "Scores: [1400176.31317098 1214628.35309767 1411254.04912044 2008843.04925284\n",
      " 1358379.94912634 3368863.11161206 1344335.77271017 1249713.77227627\n",
      " 1996216.52470632 2636167.89938798]\n",
      "Mean: 1798857.879446108\n",
      "Standard deviation: 678360.3001354883\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### evaluate models \n",
    "\n",
    "models = {'forest_1': RandomForestRegressor(n_estimators=3, random_state=42),\n",
    "          'forest_2':RandomForestRegressor(max_depth=5, n_estimators=10, max_features=9, random_state=42),\n",
    "          'forest_3':RandomForestRegressor(max_depth=10, n_estimators=20, random_state=42),\n",
    "          #'forest_grid_search': grid_search_forest.best_estimator_\n",
    "         }\n",
    "\n",
    "random_forest_evaluator = ModelEvaluator(X, y)\n",
    "\n",
    "random_forest_evaluator.evaluate_models(models)\n",
    "\n",
    "random_forest_evaluator.save_models(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
