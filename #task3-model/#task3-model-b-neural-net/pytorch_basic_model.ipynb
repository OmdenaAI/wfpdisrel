{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#from utils import *\n",
    "from kaggle import api\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
    "from fastai2.tabular.all import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from dtreeviz.trees import *\n",
    "from IPython.display import Image, display_svg, SVG\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/dev/course-v3/nbs/dl1/omdena1/model/nyc_data/nyc-rolling-sales-clean-1.csv', sep = ';', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Land_sqf</th>\n",
       "      <th>Gross_sqf</th>\n",
       "      <th>Year_built</th>\n",
       "      <th>NBH_level</th>\n",
       "      <th>SALE_PRICE</th>\n",
       "      <th>YEARLY_RENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2400</td>\n",
       "      <td>1552</td>\n",
       "      <td>1930</td>\n",
       "      <td>2</td>\n",
       "      <td>220485</td>\n",
       "      <td>10900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2742</td>\n",
       "      <td>1207</td>\n",
       "      <td>1925</td>\n",
       "      <td>2</td>\n",
       "      <td>223372</td>\n",
       "      <td>8100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5610</td>\n",
       "      <td>1320</td>\n",
       "      <td>1910</td>\n",
       "      <td>5</td>\n",
       "      <td>362981</td>\n",
       "      <td>19000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1758</td>\n",
       "      <td>1537</td>\n",
       "      <td>1910</td>\n",
       "      <td>5</td>\n",
       "      <td>245135</td>\n",
       "      <td>9700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1317</td>\n",
       "      <td>1339</td>\n",
       "      <td>1920</td>\n",
       "      <td>4</td>\n",
       "      <td>216477</td>\n",
       "      <td>12800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>4000</td>\n",
       "      <td>1562</td>\n",
       "      <td>1945</td>\n",
       "      <td>6</td>\n",
       "      <td>354597</td>\n",
       "      <td>21100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>2000</td>\n",
       "      <td>1548</td>\n",
       "      <td>1930</td>\n",
       "      <td>6</td>\n",
       "      <td>277394</td>\n",
       "      <td>20200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>1200</td>\n",
       "      <td>450</td>\n",
       "      <td>1987</td>\n",
       "      <td>6</td>\n",
       "      <td>154983</td>\n",
       "      <td>9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>3411</td>\n",
       "      <td>1960</td>\n",
       "      <td>1920</td>\n",
       "      <td>5</td>\n",
       "      <td>311596</td>\n",
       "      <td>23400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>4100</td>\n",
       "      <td>2340</td>\n",
       "      <td>1970</td>\n",
       "      <td>7</td>\n",
       "      <td>408346</td>\n",
       "      <td>25900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>291 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Land_sqf  Gross_sqf  Year_built  NBH_level  SALE_PRICE  YEARLY_RENT\n",
       "0        2400       1552        1930          2      220485        10900\n",
       "1        2742       1207        1925          2      223372         8100\n",
       "2        5610       1320        1910          5      362981        19000\n",
       "3        1758       1537        1910          5      245135         9700\n",
       "4        1317       1339        1920          4      216477        12800\n",
       "..        ...        ...         ...        ...         ...          ...\n",
       "286      4000       1562        1945          6      354597        21100\n",
       "287      2000       1548        1930          6      277394        20200\n",
       "288      1200        450        1987          6      154983         9500\n",
       "289      3411       1960        1920          5      311596        23400\n",
       "290      4100       2340        1970          7      408346        25900\n",
       "\n",
       "[291 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['Land_sqf', 'Gross_sqf', 'Year_built', 'NBH_level']]\n",
    "Y = df[['SALE_PRICE', 'YEARLY_RENT']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, shuffle=True)\n",
    "\n",
    "type(X_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert dataframe to train/test tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([232, 4]) torch.Size([232, 2])\n",
      "torch.Size([59, 4]) torch.Size([59, 2])\n"
     ]
    }
   ],
   "source": [
    "#Convert all into tensors\n",
    "\n",
    "x_train_tensor = torch.Tensor(X_train.values)\n",
    "y_train_tensor = torch.Tensor(y_train.values)\n",
    "x_test_tensor = torch.Tensor(X_test.values)\n",
    "y_test_tensor = torch.Tensor(y_test.values)\n",
    "\n",
    "# X_train = torch.from_numpy(X_train.to_numpy())\n",
    "# y_train = torch.squeeze(torch.from_numpy(y_train.to_numpy()).float())\n",
    "\n",
    "# X_test = torch.from_numpy(X_test.to_numpy()).float()\n",
    "# y_test = torch.squeeze(torch.from_numpy(y_test.to_numpy()).float())\n",
    "\n",
    "print(x_train_tensor.shape, y_train_tensor.shape)\n",
    "print(x_test_tensor.shape, y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep with Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x, self.y = x, y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(x_train_tensor, y_train_tensor)\n",
    "valid_ds = Dataset(x_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, 32)\n",
    "valid_dl = DataLoader(valid_ds, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and optimiser (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(4, 3), nn.ReLU(), nn.Linear(3,2))\n",
    "    return model, optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for 1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Training. Epoch # 0- Loss = 138125535870976.0\n",
      "In Training. Epoch # 0- Loss = 135089119821824.0\n",
      "In Training. Epoch # 0- Loss = 132815295348736.0\n",
      "In Training. Epoch # 0- Loss = 129747329744896.0\n",
      "In Training. Epoch # 0- Loss = 126684657352704.0\n",
      "In Training. Epoch # 0- Loss = 124643448979456.0\n",
      "In Training. Epoch # 0- Loss = 121949581737984.0\n",
      "In Training. Epoch # 0- Loss = 119356914663424.0\n",
      "In Training. Epoch # 1- Loss = 117634146238464.0\n",
      "In Training. Epoch # 1- Loss = 115027730235392.0\n",
      "In Training. Epoch # 1- Loss = 113121486176256.0\n",
      "In Training. Epoch # 1- Loss = 110478462287872.0\n",
      "In Training. Epoch # 1- Loss = 107835849441280.0\n",
      "In Training. Epoch # 1- Loss = 106132693581824.0\n",
      "In Training. Epoch # 1- Loss = 103823502737408.0\n",
      "In Training. Epoch # 1- Loss = 101604497817600.0\n",
      "In Training. Epoch # 2- Loss = 100184616861696.0\n",
      "In Training. Epoch # 2- Loss = 97945949044736.0\n",
      "In Training. Epoch # 2- Loss = 96350427414528.0\n",
      "In Training. Epoch # 2- Loss = 94071628496896.0\n",
      "In Training. Epoch # 2- Loss = 91789289586688.0\n",
      "In Training. Epoch # 2- Loss = 90371287678976.0\n",
      "In Training. Epoch # 2- Loss = 88390804439040.0\n",
      "In Training. Epoch # 2- Loss = 86490868613120.0\n",
      "In Training. Epoch # 3- Loss = 85325263142912.0\n",
      "In Training. Epoch # 3- Loss = 83401210068992.0\n",
      "In Training. Epoch # 3- Loss = 82068193148928.0\n",
      "In Training. Epoch # 3- Loss = 80101727272960.0\n",
      "In Training. Epoch # 3- Loss = 78128567287808.0\n",
      "In Training. Epoch # 3- Loss = 76950882222080.0\n",
      "In Training. Epoch # 3- Loss = 75251341852672.0\n",
      "In Training. Epoch # 3- Loss = 73623926734848.0\n",
      "In Training. Epoch # 4- Loss = 72671467405312.0\n",
      "In Training. Epoch # 4- Loss = 71016688648192.0\n",
      "In Training. Epoch # 4- Loss = 69905261002752.0\n",
      "In Training. Epoch # 4- Loss = 68206798569472.0\n",
      "In Training. Epoch # 4- Loss = 66499112861696.0\n",
      "In Training. Epoch # 4- Loss = 65523765215232.0\n",
      "In Training. Epoch # 4- Loss = 64064449413120.0\n",
      "In Training. Epoch # 4- Loss = 62669830750208.0\n",
      "In Training. Epoch # 5- Loss = 61895767752704.0\n",
      "In Training. Epoch # 5- Loss = 60471528914944.0\n",
      "In Training. Epoch # 5- Loss = 59547007844352.0\n",
      "In Training. Epoch # 5- Loss = 58078619762688.0\n",
      "In Training. Epoch # 5- Loss = 56599028695040.0\n",
      "In Training. Epoch # 5- Loss = 55793839767552.0\n",
      "In Training. Epoch # 5- Loss = 54539981946880.0\n",
      "In Training. Epoch # 5- Loss = 53344286539776.0\n",
      "In Training. Epoch # 6- Loss = 52719280717824.0\n",
      "In Training. Epoch # 6- Loss = 51492518100992.0\n",
      "In Training. Epoch # 6- Loss = 50725509922816.0\n",
      "In Training. Epoch # 6- Loss = 49454761639936.0\n",
      "In Training. Epoch # 6- Loss = 48171279450112.0\n",
      "In Training. Epoch # 6- Loss = 47509024014336.0\n",
      "In Training. Epoch # 6- Loss = 46430941085696.0\n",
      "In Training. Epoch # 6- Loss = 45405291151360.0\n",
      "In Training. Epoch # 7- Loss = 44904562556928.0\n",
      "In Training. Epoch # 7- Loss = 43847035912192.0\n",
      "In Training. Epoch # 7- Loss = 43212630654976.0\n",
      "In Training. Epoch # 7- Loss = 42111781044224.0\n",
      "In Training. Epoch # 7- Loss = 40997023121408.0\n",
      "In Training. Epoch # 7- Loss = 40454674448384.0\n",
      "In Training. Epoch # 7- Loss = 39527053787136.0\n",
      "In Training. Epoch # 7- Loss = 38646791012352.0\n",
      "In Training. Epoch # 8- Loss = 38249452011520.0\n",
      "In Training. Epoch # 8- Loss = 37337018925056.0\n",
      "In Training. Epoch # 8- Loss = 36814144405504.0\n",
      "In Training. Epoch # 8- Loss = 35859441123328.0\n",
      "In Training. Epoch # 8- Loss = 34889955016704.0\n",
      "In Training. Epoch # 8- Loss = 34448045244416.0\n",
      "In Training. Epoch # 8- Loss = 33649244241920.0\n",
      "In Training. Epoch # 8- Loss = 32893336289280.0\n",
      "In Training. Epoch # 9- Loss = 32581791776768.0\n",
      "In Training. Epoch # 9- Loss = 31793818370048.0\n",
      "In Training. Epoch # 9- Loss = 31364619436032.0\n",
      "In Training. Epoch # 9- Loss = 30535724302336.0\n",
      "In Training. Epoch # 9- Loss = 29691423490048.0\n",
      "In Training. Epoch # 9- Loss = 29333500461056.0\n",
      "In Training. Epoch # 9- Loss = 28645051596800.0\n",
      "In Training. Epoch # 9- Loss = 27995538456576.0\n",
      "In Training. Epoch # 10- Loss = 27754978344960.0\n",
      "In Training. Epoch # 10- Loss = 27073835958272.0\n",
      "In Training. Epoch # 10- Loss = 26723219406848.0\n",
      "In Training. Epoch # 10- Loss = 26002705088512.0\n",
      "In Training. Epoch # 10- Loss = 25266367758336.0\n",
      "In Training. Epoch # 10- Loss = 24978544132096.0\n",
      "In Training. Epoch # 10- Loss = 24384670531584.0\n",
      "In Training. Epoch # 10- Loss = 23826224119808.0\n",
      "In Training. Epoch # 11- Loss = 23644197617664.0\n",
      "In Training. Epoch # 11- Loss = 23054801436672.0\n",
      "In Training. Epoch # 11- Loss = 22770001903616.0\n",
      "In Training. Epoch # 11- Loss = 22142940872704.0\n",
      "In Training. Epoch # 11- Loss = 21499796783104.0\n",
      "In Training. Epoch # 11- Loss = 21270353674240.0\n",
      "In Training. Epoch # 11- Loss = 20757574844416.0\n",
      "In Training. Epoch # 11- Loss = 20277106835456.0\n",
      "In Training. Epoch # 12- Loss = 20143157542912.0\n",
      "In Training. Epoch # 12- Loss = 19632610082816.0\n",
      "In Training. Epoch # 12- Loss = 19402844012544.0\n",
      "In Training. Epoch # 12- Loss = 18856428961792.0\n",
      "In Training. Epoch # 12- Loss = 18293807120384.0\n",
      "In Training. Epoch # 12- Loss = 18112871137280.0\n",
      "In Training. Epoch # 12- Loss = 17669661130752.0\n",
      "In Training. Epoch # 12- Loss = 17255997898752.0\n",
      "In Training. Epoch # 13- Loss = 17161361817600.0\n",
      "In Training. Epoch # 13- Loss = 16718617378816.0\n",
      "In Training. Epoch # 13- Loss = 16534783131648.0\n",
      "In Training. Epoch # 13- Loss = 16058030227456.0\n",
      "In Training. Epoch # 13- Loss = 15565047463936.0\n",
      "In Training. Epoch # 13- Loss = 15424301301760.0\n",
      "In Training. Epoch # 13- Loss = 15040810844160.0\n",
      "In Training. Epoch # 13- Loss = 14684398813184.0\n",
      "In Training. Epoch # 14- Loss = 14621739057152.0\n",
      "In Training. Epoch # 14- Loss = 14237345775616.0\n",
      "In Training. Epoch # 14- Loss = 14091761483776.0\n",
      "In Training. Epoch # 14- Loss = 13675238785024.0\n",
      "In Training. Epoch # 14- Loss = 13242543898624.0\n",
      "In Training. Epoch # 14- Loss = 13134995652608.0\n",
      "In Training. Epoch # 14- Loss = 12802798387200.0\n",
      "In Training. Epoch # 14- Loss = 12495478587392.0\n",
      "In Training. Epoch # 15- Loss = 12458663084032.0\n",
      "In Training. Epoch # 15- Loss = 12124523855872.0\n",
      "In Training. Epoch # 15- Loss = 12010718756864.0\n",
      "In Training. Epoch # 15- Loss = 11646326013952.0\n",
      "In Training. Epoch # 15- Loss = 11265886912512.0\n",
      "In Training. Epoch # 15- Loss = 11185658265600.0\n",
      "In Training. Epoch # 15- Loss = 10897545232384.0\n",
      "In Training. Epoch # 15- Loss = 10632345681920.0\n",
      "In Training. Epoch # 16- Loss = 10616259477504.0\n",
      "In Training. Epoch # 16- Loss = 10325435875328.0\n",
      "In Training. Epoch # 16- Loss = 10237959471104.0\n",
      "In Training. Epoch # 16- Loss = 9918733090816.0\n",
      "In Training. Epoch # 16- Loss = 9583633367040.0\n",
      "In Training. Epoch # 16- Loss = 9525796012032.0\n",
      "In Training. Epoch # 16- Loss = 9275597389824.0\n",
      "In Training. Epoch # 16- Loss = 9046554836992.0\n",
      "In Training. Epoch # 17- Loss = 9046940712960.0\n",
      "In Training. Epoch # 17- Loss = 8793485737984.0\n",
      "In Training. Epoch # 17- Loss = 8727752605696.0\n",
      "In Training. Epoch # 17- Loss = 8447704694784.0\n",
      "In Training. Epoch # 17- Loss = 8151991582720.0\n",
      "In Training. Epoch # 17- Loss = 8112412033024.0\n",
      "In Training. Epoch # 17- Loss = 7894846668800.0\n",
      "In Training. Epoch # 17- Loss = 7696861888512.0\n",
      "In Training. Epoch # 18- Loss = 7710187192320.0\n",
      "In Training. Epoch # 18- Loss = 7488996900864.0\n",
      "In Training. Epoch # 18- Loss = 7441156669440.0\n",
      "In Training. Epoch # 18- Loss = 7195131904000.0\n",
      "In Training. Epoch # 18- Loss = 6933676818432.0\n",
      "In Training. Epoch # 18- Loss = 6908902113280.0\n",
      "In Training. Epoch # 18- Loss = 6719445925888.0\n",
      "In Training. Epoch # 18- Loss = 6548155269120.0\n",
      "In Training. Epoch # 19- Loss = 6571496046592.0\n",
      "In Training. Epoch # 19- Loss = 6378192633856.0\n",
      "In Training. Epoch # 19- Loss = 6345014640640.0\n",
      "In Training. Epoch # 19- Loss = 6128572825600.0\n",
      "In Training. Epoch # 19- Loss = 5896953921536.0\n",
      "In Training. Epoch # 19- Loss = 5884098904064.0\n",
      "In Training. Epoch # 19- Loss = 5718872686592.0\n",
      "In Training. Epoch # 19- Loss = 5570542174208.0\n",
      "In Training. Epoch # 20- Loss = 5601485651968.0\n",
      "In Training. Epoch # 20- Loss = 5432307351552.0\n",
      "In Training. Epoch # 20- Loss = 5411084173312.0\n",
      "In Training. Epoch # 20- Loss = 5220396433408.0\n",
      "In Training. Epoch # 20- Loss = 5014796894208.0\n",
      "In Training. Epoch # 20- Loss = 5011459801088.0\n",
      "In Training. Epoch # 20- Loss = 4867139567616.0\n",
      "In Training. Epoch # 20- Loss = 4738571042816.0\n",
      "In Training. Epoch # 21- Loss = 4775135412224.0\n",
      "In Training. Epoch # 21- Loss = 4626851561472.0\n",
      "In Training. Epoch # 21- Loss = 4615319846912.0\n",
      "In Training. Epoch # 21- Loss = 4447083167744.0\n",
      "In Training. Epoch # 21- Loss = 4264204697600.0\n",
      "In Training. Epoch # 21- Loss = 4268387205120.0\n",
      "In Training. Epoch # 21- Loss = 4142122139648.0\n",
      "In Training. Epoch # 21- Loss = 4030576984064.0\n",
      "In Training. Epoch # 22- Loss = 4071139049472.0\n",
      "In Training. Epoch # 22- Loss = 3940970921984.0\n",
      "In Training. Epoch # 22- Loss = 3937239826432.0\n",
      "In Training. Epoch # 22- Loss = 3788601819136.0\n",
      "In Training. Epoch # 22- Loss = 3625593339904.0\n",
      "In Training. Epoch # 22- Loss = 3635640008704.0\n",
      "In Training. Epoch # 22- Loss = 3524981161984.0\n",
      "In Training. Epoch # 22- Loss = 3428112400384.0\n",
      "In Training. Epoch # 23- Loss = 3471352266752.0\n",
      "In Training. Epoch # 23- Loss = 3356909109248.0\n",
      "In Training. Epoch # 23- Loss = 3359404720128.0\n",
      "In Training. Epoch # 23- Loss = 3227898347520.0\n",
      "In Training. Epoch # 23- Loss = 3082290987008.0\n",
      "In Training. Epoch # 23- Loss = 3096834473984.0\n",
      "In Training. Epoch # 23- Loss = 2999679451136.0\n",
      "In Training. Epoch # 23- Loss = 2915474341888.0\n",
      "In Training. Epoch # 24- Loss = 2960323772416.0\n",
      "In Training. Epoch # 24- Loss = 2859546443776.0\n",
      "In Training. Epoch # 24- Loss = 2866960400384.0\n",
      "In Training. Epoch # 24- Loss = 2750451023872.0\n",
      "In Training. Epoch # 24- Loss = 2620106211328.0\n",
      "In Training. Epoch # 24- Loss = 2638019821568.0\n",
      "In Training. Epoch # 24- Loss = 2552562450432.0\n",
      "In Training. Epoch # 24- Loss = 2479293988864.0\n",
      "In Training. Epoch # 25- Loss = 2524894986240.0\n",
      "In Training. Epoch # 25- Loss = 2436008771584.0\n",
      "In Training. Epoch # 25- Loss = 2447256322048.0\n",
      "In Training. Epoch # 25- Loss = 2343895302144.0\n",
      "In Training. Epoch # 25- Loss = 2226956795904.0\n",
      "In Training. Epoch # 25- Loss = 2247316996096.0\n",
      "In Training. Epoch # 25- Loss = 2172003155968.0\n",
      "In Training. Epoch # 25- Loss = 2108190228480.0\n",
      "In Training. Epoch # 26- Loss = 2153860169728.0\n",
      "In Training. Epoch # 26- Loss = 2075334541312.0\n",
      "In Training. Epoch # 26- Loss = 2089519808512.0\n",
      "In Training. Epoch # 26- Loss = 1997703610368.0\n",
      "In Training. Epoch # 26- Loss = 1892559093760.0\n",
      "In Training. Epoch # 26- Loss = 1914612350976.0\n",
      "In Training. Epoch # 26- Loss = 1848104845312.0\n",
      "In Training. Epoch # 26- Loss = 1792473956352.0\n",
      "In Training. Epoch # 27- Loss = 1837675970560.0\n",
      "In Training. Epoch # 27- Loss = 1768189198336.0\n",
      "In Training. Epoch # 27- Loss = 1784574640128.0\n",
      "In Training. Epoch # 27- Loss = 1702910623744.0\n",
      "In Training. Epoch # 27- Loss = 1608158543872.0\n",
      "In Training. Epoch # 27- Loss = 1631292882944.0\n",
      "In Training. Epoch # 27- Loss = 1572440244224.0\n",
      "In Training. Epoch # 27- Loss = 1523897335808.0\n",
      "In Training. Epoch # 28- Loss = 1568215138304.0\n",
      "In Training. Epoch # 28- Loss = 1506625454080.0\n",
      "In Training. Epoch # 28- Loss = 1524606042112.0\n",
      "In Training. Epoch # 28- Loss = 1451883626496.0\n",
      "In Training. Epoch # 28- Loss = 1366303571968.0\n",
      "In Training. Epoch # 28- Loss = 1390026293248.0\n",
      "In Training. Epoch # 28- Loss = 1337835913216.0\n",
      "In Training. Epoch # 28- Loss = 1295440150528.0\n",
      "In Training. Epoch # 29- Loss = 1338556153856.0\n",
      "In Training. Epoch # 29- Loss = 1283875536896.0\n",
      "In Training. Epoch # 29- Loss = 1302957785088.0\n",
      "In Training. Epoch # 29- Loss = 1238123282432.0\n",
      "In Training. Epoch # 29- Loss = 1160650817536.0\n",
      "In Training. Epoch # 29- Loss = 1184567656448.0\n",
      "In Training. Epoch # 29- Loss = 1138183241728.0\n",
      "In Training. Epoch # 29- Loss = 1101124730880.0\n",
      "In Training. Epoch # 30- Loss = 1142804054016.0\n",
      "In Training. Epoch # 30- Loss = 1094177128448.0\n",
      "In Training. Epoch # 30- Loss = 1113960742912.0\n",
      "In Training. Epoch # 30- Loss = 1056095731712.0\n",
      "In Training. Epoch # 30- Loss = 985801818112.0\n",
      "In Training. Epoch # 30- Loss = 1009601085440.0\n",
      "In Training. Epoch # 30- Loss = 968283062272.0\n",
      "In Training. Epoch # 30- Loss = 935864631296.0\n",
      "In Training. Epoch # 31- Loss = 975938519040.0\n",
      "In Training. Epoch # 31- Loss = 932623745024.0\n",
      "In Training. Epoch # 31- Loss = 952786092032.0\n",
      "In Training. Epoch # 31- Loss = 901089001472.0\n",
      "In Training. Epoch # 31- Loss = 837160992768.0\n",
      "In Training. Epoch # 31- Loss = 860599025664.0\n",
      "In Training. Epoch # 31- Loss = 823708221440.0\n",
      "In Training. Epoch # 31- Loss = 795328512000.0\n",
      "In Training. Epoch # 32- Loss = 833682997248.0\n",
      "In Training. Epoch # 32- Loss = 795036614656.0\n",
      "In Training. Epoch # 32- Loss = 815319941120.0\n",
      "In Training. Epoch # 32- Loss = 769090322432.0\n",
      "In Training. Epoch # 32- Loss = 710817153024.0\n",
      "In Training. Epoch # 32- Loss = 733706518528.0\n",
      "In Training. Epoch # 32- Loss = 700689940480.0\n",
      "In Training. Epoch # 32- Loss = 675830104064.0\n",
      "In Training. Epoch # 33- Loss = 712395587584.0\n",
      "In Training. Epoch # 33- Loss = 677858639872.0\n",
      "In Training. Epoch # 33- Loss = 698058670080.0\n",
      "In Training. Epoch # 33- Loss = 656683368448.0\n",
      "In Training. Epoch # 33- Loss = 603441070080.0\n",
      "In Training. Epoch # 33- Loss = 625641127936.0\n",
      "In Training. Epoch # 33- Loss = 596020101120.0\n",
      "In Training. Epoch # 33- Loss = 574232264704.0\n",
      "In Training. Epoch # 34- Loss = 608974340096.0\n",
      "In Training. Epoch # 34- Loss = 578060091392.0\n",
      "In Training. Epoch # 34- Loss = 598017966080.0\n",
      "In Training. Epoch # 34- Loss = 560959455232.0\n",
      "In Training. Epoch # 34- Loss = 512200146944.0\n",
      "In Training. Epoch # 34- Loss = 533608038400.0\n",
      "In Training. Epoch # 34- Loss = 506967195648.0\n",
      "In Training. Epoch # 34- Loss = 487864205312.0\n",
      "In Training. Epoch # 35- Loss = 520776876032.0\n",
      "In Training. Epoch # 35- Loss = 493061505024.0\n",
      "In Training. Epoch # 35- Loss = 512654606336.0\n",
      "In Training. Epoch # 35- Loss = 479441190912.0\n",
      "In Training. Epoch # 35- Loss = 434683019264.0\n",
      "In Training. Epoch # 35- Loss = 455227146240.0\n",
      "In Training. Epoch # 35- Loss = 431206170624.0\n",
      "In Training. Epoch # 35- Loss = 414452973568.0\n",
      "In Training. Epoch # 36- Loss = 445552590848.0\n",
      "In Training. Epoch # 36- Loss = 420666376192.0\n",
      "In Training. Epoch # 36- Loss = 439802822656.0\n",
      "In Training. Epoch # 36- Loss = 410019725312.0\n",
      "In Training. Epoch # 36- Loss = 368838246400.0\n",
      "In Training. Epoch # 36- Loss = 388472209408.0\n",
      "In Training. Epoch # 36- Loss = 366757740544.0\n",
      "In Training. Epoch # 36- Loss = 352064176128.0\n",
      "In Training. Epoch # 37- Loss = 381384261632.0\n",
      "In Training. Epoch # 37- Loss = 359004340224.0\n",
      "In Training. Epoch # 37- Loss = 377617088512.0\n",
      "In Training. Epoch # 37- Loss = 350899077120.0\n",
      "In Training. Epoch # 37- Loss = 312919654400.0\n",
      "In Training. Epoch # 37- Loss = 331617468416.0\n",
      "In Training. Epoch # 37- Loss = 311937007616.0\n",
      "In Training. Epoch # 37- Loss = 299051483136.0\n",
      "In Training. Epoch # 38- Loss = 326638829568.0\n",
      "In Training. Epoch # 38- Loss = 306482675712.0\n",
      "In Training. Epoch # 38- Loss = 324525096960.0\n",
      "In Training. Epoch # 38- Loss = 300549963776.0\n",
      "In Training. Epoch # 38- Loss = 265441591296.0\n",
      "In Training. Epoch # 38- Loss = 283193606144.0\n",
      "In Training. Epoch # 38- Loss = 265309601792.0\n",
      "In Training. Epoch # 38- Loss = 254013620224.0\n",
      "In Training. Epoch # 39- Loss = 279924932608.0\n",
      "In Training. Epoch # 39- Loss = 261745098752.0\n",
      "In Training. Epoch # 39- Loss = 279187062784.0\n",
      "In Training. Epoch # 39- Loss = 257670217728.0\n",
      "In Training. Epoch # 39- Loss = 225139834880.0\n",
      "In Training. Epoch # 39- Loss = 241949310976.0\n",
      "In Training. Epoch # 39- Loss = 225654505472.0\n",
      "In Training. Epoch # 39- Loss = 215758209024.0\n",
      "In Training. Epoch # 40- Loss = 240057204736.0\n",
      "In Training. Epoch # 40- Loss = 223636602880.0\n",
      "In Training. Epoch # 40- Loss = 240461447168.0\n",
      "In Training. Epoch # 40- Loss = 221151100928.0\n",
      "In Training. Epoch # 40- Loss = 190938890240.0\n",
      "In Training. Epoch # 40- Loss = 206819196928.0\n",
      "In Training. Epoch # 40- Loss = 191932596224.0\n",
      "In Training. Epoch # 40- Loss = 183270768640.0\n",
      "In Training. Epoch # 41- Loss = 206026063872.0\n",
      "In Training. Epoch # 41- Loss = 191173902336.0\n",
      "In Training. Epoch # 41- Loss = 207375482880.0\n",
      "In Training. Epoch # 41- Loss = 190048468992.0\n",
      "In Training. Epoch # 41- Loss = 161923710976.0\n",
      "In Training. Epoch # 41- Loss = 176895983616.0\n",
      "In Training. Epoch # 41- Loss = 163259219968.0\n",
      "In Training. Epoch # 41- Loss = 155687829504.0\n",
      "In Training. Epoch # 42- Loss = 176971022336.0\n",
      "In Training. Epoch # 42- Loss = 163519447040.0\n",
      "In Training. Epoch # 42- Loss = 179100106752.0\n",
      "In Training. Epoch # 42- Loss = 163558326272.0\n",
      "In Training. Epoch # 42- Loss = 137315811328.0\n",
      "In Training. Epoch # 42- Loss = 151407083520.0\n",
      "In Training. Epoch # 42- Loss = 138881318912.0\n",
      "In Training. Epoch # 42- Loss = 132274790400.0\n",
      "In Training. Epoch # 43- Loss = 152159010816.0\n",
      "In Training. Epoch # 43- Loss = 139960041472.0\n",
      "In Training. Epoch # 43- Loss = 154928840704.0\n",
      "In Training. Epoch # 43- Loss = 140996198400.0\n",
      "In Training. Epoch # 43- Loss = 116453015552.0\n",
      "In Training. Epoch # 43- Loss = 129694711808.0\n",
      "In Training. Epoch # 43- Loss = 118158163968.0\n",
      "In Training. Epoch # 43- Loss = 112406659072.0\n",
      "In Training. Epoch # 44- Loss = 130965463040.0\n",
      "In Training. Epoch # 44- Loss = 119888461824.0\n",
      "In Training. Epoch # 44- Loss = 134259441664.0\n",
      "In Training. Epoch # 44- Loss = 121779085312.0\n",
      "In Training. Epoch # 44- Loss = 98772099072.0\n",
      "In Training. Epoch # 44- Loss = 111198552064.0\n",
      "In Training. Epoch # 44- Loss = 100544192512.0\n",
      "In Training. Epoch # 44- Loss = 95551651840.0\n",
      "In Training. Epoch # 45- Loss = 112858005504.0\n",
      "In Training. Epoch # 45- Loss = 102787473408.0\n",
      "In Training. Epoch # 45- Loss = 116578557952.0\n",
      "In Training. Epoch # 45- Loss = 105410609152.0\n",
      "In Training. Epoch # 45- Loss = 83793977344.0\n",
      "In Training. Epoch # 45- Loss = 95441616896.0\n",
      "In Training. Epoch # 45- Loss = 85575180288.0\n",
      "In Training. Epoch # 45- Loss = 81257422848.0\n",
      "In Training. Epoch # 46- Loss = 97383047168.0\n",
      "In Training. Epoch # 46- Loss = 88216666112.0\n",
      "In Training. Epoch # 46- Loss = 101448613888.0\n",
      "In Training. Epoch # 46- Loss = 91468095488.0\n",
      "In Training. Epoch # 46- Loss = 71111311360.0\n",
      "In Training. Epoch # 46- Loss = 82017607680.0\n",
      "In Training. Epoch # 46- Loss = 72856059904.0\n",
      "In Training. Epoch # 46- Loss = 69139185664.0\n",
      "In Training. Epoch # 47- Loss = 84153966592.0\n",
      "In Training. Epoch # 47- Loss = 75800993792.0\n",
      "In Training. Epoch # 47- Loss = 88496504832.0\n",
      "In Training. Epoch # 47- Loss = 79591596032.0\n",
      "In Training. Epoch # 47- Loss = 60377649152.0\n",
      "In Training. Epoch # 47- Loss = 70580609024.0\n",
      "In Training. Epoch # 47- Loss = 62050578432.0\n",
      "In Training. Epoch # 47- Loss = 58869633024.0\n",
      "In Training. Epoch # 48- Loss = 72841224192.0\n",
      "In Training. Epoch # 48- Loss = 65221033984.0\n",
      "In Training. Epoch # 48- Loss = 77404102656.0\n",
      "In Training. Epoch # 48- Loss = 69474623488.0\n",
      "In Training. Epoch # 48- Loss = 51298459648.0\n",
      "In Training. Epoch # 48- Loss = 60835995648.0\n",
      "In Training. Epoch # 48- Loss = 52872593408.0\n",
      "In Training. Epoch # 48- Loss = 50170408960.0\n",
      "In Training. Epoch # 49- Loss = 63163953152.0\n",
      "In Training. Epoch # 49- Loss = 56204767232.0\n",
      "In Training. Epoch # 49- Loss = 67900133376.0\n",
      "In Training. Epoch # 49- Loss = 60856156160.0\n",
      "In Training. Epoch # 49- Loss = 43623350272.0\n",
      "In Training. Epoch # 49- Loss = 52532883456.0\n",
      "In Training. Epoch # 49- Loss = 45078634496.0\n",
      "In Training. Epoch # 49- Loss = 42804789248.0\n",
      "In Training. Epoch # 50- Loss = 54882672640.0\n",
      "In Training. Epoch # 50- Loss = 48520519680.0\n",
      "In Training. Epoch # 50- Loss = 59753222144.0\n",
      "In Training. Epoch # 50- Loss = 53513945088.0\n",
      "In Training. Epoch # 50- Loss = 37139456000.0\n",
      "In Training. Epoch # 50- Loss = 45457604608.0\n",
      "In Training. Epoch # 50- Loss = 38461497344.0\n",
      "In Training. Epoch # 50- Loss = 36571463680.0\n",
      "In Training. Epoch # 51- Loss = 47793225728.0\n",
      "In Training. Epoch # 51- Loss = 41970995200.0\n",
      "In Training. Epoch # 51- Loss = 52766015488.0\n",
      "In Training. Epoch # 51- Loss = 47258714112.0\n",
      "In Training. Epoch # 51- Loss = 31665897472.0\n",
      "In Training. Epoch # 51- Loss = 39428206592.0\n",
      "In Training. Epoch # 51- Loss = 32844916736.0\n",
      "In Training. Epoch # 51- Loss = 31299303424.0\n",
      "In Training. Epoch # 52- Loss = 41721532416.0\n",
      "In Training. Epoch # 52- Loss = 36388163584.0\n",
      "In Training. Epoch # 52- Loss = 46770135040.0\n",
      "In Training. Epoch # 52- Loss = 41929285632.0\n",
      "In Training. Epoch # 52- Loss = 27048937472.0\n",
      "In Training. Epoch # 52- Loss = 34289725440.0\n",
      "In Training. Epoch # 52- Loss = 28078897152.0\n",
      "In Training. Epoch # 52- Loss = 26842804224.0\n",
      "In Training. Epoch # 53- Loss = 36519141376.0\n",
      "In Training. Epoch # 53- Loss = 31628904448.0\n",
      "In Training. Epoch # 53- Loss = 41621934080.0\n",
      "In Training. Epoch # 53- Loss = 37388394496.0\n",
      "In Training. Epoch # 53- Loss = 23157981184.0\n",
      "In Training. Epoch # 53- Loss = 29910177792.0\n",
      "In Training. Epoch # 53- Loss = 24035840000.0\n",
      "In Training. Epoch # 53- Loss = 23078295552.0\n",
      "In Training. Epoch # 54- Loss = 32059449344.0\n",
      "In Training. Epoch # 54- Loss = 27571326976.0\n",
      "In Training. Epoch # 54- Loss = 37198811136.0\n",
      "In Training. Epoch # 54- Loss = 33519155200.0\n",
      "In Training. Epoch # 54- Loss = 19882076160.0\n",
      "In Training. Epoch # 54- Loss = 26177175552.0\n",
      "In Training. Epoch # 54- Loss = 20607188992.0\n",
      "In Training. Epoch # 54- Loss = 19900665856.0\n",
      "In Training. Epoch # 55- Loss = 28234455040.0\n",
      "In Training. Epoch # 55- Loss = 24111599616.0\n",
      "In Training. Epoch # 55- Loss = 33396129792.0\n",
      "In Training. Epoch # 55- Loss = 30222008320.0\n",
      "In Training. Epoch # 55- Loss = 17126996992.0\n",
      "In Training. Epoch # 55- Loss = 22994982912.0\n",
      "In Training. Epoch # 55- Loss = 17700622336.0\n",
      "In Training. Epoch # 55- Loss = 17220605952.0\n",
      "In Training. Epoch # 56- Loss = 24952018944.0\n",
      "In Training. Epoch # 56- Loss = 21161306112.0\n",
      "In Training. Epoch # 56- Loss = 30124550144.0\n",
      "In Training. Epoch # 56- Loss = 27412195328.0\n",
      "In Training. Epoch # 56- Loss = 14812737536.0\n",
      "In Training. Epoch # 56- Loss = 20282068992.0\n",
      "In Training. Epoch # 56- Loss = 15237593088.0\n",
      "In Training. Epoch # 56- Loss = 14962227200.0\n",
      "In Training. Epoch # 57- Loss = 22133536768.0\n",
      "In Training. Epoch # 57- Loss = 18645114880.0\n",
      "In Training. Epoch # 57- Loss = 27307794432.0\n",
      "In Training. Epoch # 57- Loss = 25017503744.0\n",
      "In Training. Epoch # 57- Loss = 12871375872.0\n",
      "In Training. Epoch # 57- Loss = 17968990208.0\n",
      "In Training. Epoch # 57- Loss = 13151305728.0\n",
      "In Training. Epoch # 57- Loss = 13061070848.0\n",
      "In Training. Epoch # 58- Loss = 19711903744.0\n",
      "In Training. Epoch # 58- Loss = 16498856960.0\n",
      "In Training. Epoch # 58- Loss = 24880695296.0\n",
      "In Training. Epoch # 58- Loss = 22976446464.0\n",
      "In Training. Epoch # 58- Loss = 11245270016.0\n",
      "In Training. Epoch # 58- Loss = 15996594176.0\n",
      "In Training. Epoch # 58- Loss = 11384958976.0\n",
      "In Training. Epoch # 58- Loss = 11462391808.0\n",
      "In Training. Epoch # 59- Loss = 17629868032.0\n",
      "In Training. Epoch # 59- Loss = 14667883520.0\n",
      "In Training. Epoch # 59- Loss = 22787608576.0\n",
      "In Training. Epoch # 59- Loss = 21236654080.0\n",
      "In Training. Epoch # 59- Loss = 9885516800.0\n",
      "In Training. Epoch # 59- Loss = 14314505216.0\n",
      "In Training. Epoch # 59- Loss = 9890253824.0\n",
      "In Training. Epoch # 59- Loss = 10119712768.0\n",
      "In Training. Epoch # 60- Loss = 15838531584.0\n",
      "In Training. Epoch # 60- Loss = 13105633280.0\n",
      "In Training. Epoch # 60- Loss = 20980957184.0\n",
      "In Training. Epoch # 60- Loss = 19753525248.0\n",
      "In Training. Epoch # 60- Loss = 8750633984.0\n",
      "In Training. Epoch # 60- Loss = 12879802368.0\n",
      "In Training. Epoch # 60- Loss = 8626122752.0\n",
      "In Training. Epoch # 60- Loss = 8993569792.0\n",
      "In Training. Epoch # 61- Loss = 14296151040.0\n",
      "In Training. Epoch # 61- Loss = 11772438528.0\n",
      "In Training. Epoch # 61- Loss = 19420090368.0\n",
      "In Training. Epoch # 61- Loss = 18489073664.0\n",
      "In Training. Epoch # 61- Loss = 7805450752.0\n",
      "In Training. Epoch # 61- Loss = 11655929856.0\n",
      "In Training. Epoch # 61- Loss = 7557653504.0\n",
      "In Training. Epoch # 61- Loss = 8050481152.0\n",
      "In Training. Epoch # 62- Loss = 12967068672.0\n",
      "In Training. Epoch # 62- Loss = 10634509312.0\n",
      "In Training. Epoch # 62- Loss = 18070239232.0\n",
      "In Training. Epoch # 62- Loss = 17410936832.0\n",
      "In Training. Epoch # 62- Loss = 7020156416.0\n",
      "In Training. Epoch # 62- Loss = 10611750912.0\n",
      "In Training. Epoch # 62- Loss = 6655177728.0\n",
      "In Training. Epoch # 62- Loss = 7262037504.0\n",
      "In Training. Epoch # 63- Loss = 11820824576.0\n",
      "In Training. Epoch # 63- Loss = 9663055872.0\n",
      "In Training. Epoch # 63- Loss = 16901670912.0\n",
      "In Training. Epoch # 63- Loss = 16491560960.0\n",
      "In Training. Epoch # 63- Loss = 6369494528.0\n",
      "In Training. Epoch # 63- Loss = 9720732672.0\n",
      "In Training. Epoch # 63- Loss = 5893474304.0\n",
      "In Training. Epoch # 63- Loss = 6604143104.0\n",
      "In Training. Epoch # 64- Loss = 10831385600.0\n",
      "In Training. Epoch # 64- Loss = 8833544192.0\n",
      "In Training. Epoch # 64- Loss = 15888946176.0\n",
      "In Training. Epoch # 64- Loss = 15707470848.0\n",
      "In Training. Epoch # 64- Loss = 5832079872.0\n",
      "In Training. Epoch # 64- Loss = 8960276480.0\n",
      "In Training. Epoch # 64- Loss = 5251112960.0\n",
      "In Training. Epoch # 64- Loss = 6056366592.0\n",
      "In Training. Epoch # 65- Loss = 9976498176.0\n",
      "In Training. Epoch # 65- Loss = 8125071872.0\n",
      "In Training. Epoch # 65- Loss = 15010275328.0\n",
      "In Training. Epoch # 65- Loss = 15038661632.0\n",
      "In Training. Epoch # 65- Loss = 5389804544.0\n",
      "In Training. Epoch # 65- Loss = 8311122432.0\n",
      "In Training. Epoch # 65- Loss = 4709885952.0\n",
      "In Training. Epoch # 65- Loss = 5601392128.0\n",
      "In Training. Epoch # 66- Loss = 9237134336.0\n",
      "In Training. Epoch # 66- Loss = 7519828992.0\n",
      "In Training. Epoch # 66- Loss = 14247020544.0\n",
      "In Training. Epoch # 66- Loss = 14468111360.0\n",
      "In Training. Epoch # 66- Loss = 5027356160.0\n",
      "In Training. Epoch # 66- Loss = 7756870144.0\n",
      "In Training. Epoch # 66- Loss = 4254330624.0\n",
      "In Training. Epoch # 66- Loss = 5224554496.0\n",
      "In Training. Epoch # 67- Loss = 8597022720.0\n",
      "In Training. Epoch # 67- Loss = 7002637312.0\n",
      "In Training. Epoch # 67- Loss = 13583203328.0\n",
      "In Training. Epoch # 67- Loss = 13981302784.0\n",
      "In Training. Epoch # 67- Loss = 4731784704.0\n",
      "In Training. Epoch # 67- Loss = 7283538944.0\n",
      "In Training. Epoch # 67- Loss = 3871314432.0\n",
      "In Training. Epoch # 67- Loss = 4913430016.0\n",
      "In Training. Epoch # 68- Loss = 8042234880.0\n",
      "In Training. Epoch # 68- Loss = 6560560128.0\n",
      "In Training. Epoch # 68- Loss = 13005129728.0\n",
      "In Training. Epoch # 68- Loss = 13565876224.0\n",
      "In Training. Epoch # 68- Loss = 4492149248.0\n",
      "In Training. Epoch # 68- Loss = 6879218176.0\n",
      "In Training. Epoch # 68- Loss = 3549684480.0\n",
      "In Training. Epoch # 68- Loss = 4657505792.0\n",
      "In Training. Epoch # 69- Loss = 7560850944.0\n",
      "In Training. Epoch # 69- Loss = 6182570496.0\n",
      "In Training. Epoch # 69- Loss = 12501051392.0\n",
      "In Training. Epoch # 69- Loss = 13211298816.0\n",
      "In Training. Epoch # 69- Loss = 4299210752.0\n",
      "In Training. Epoch # 69- Loss = 6533755392.0\n",
      "In Training. Epoch # 69- Loss = 3279975424.0\n",
      "In Training. Epoch # 69- Loss = 4447887360.0\n",
      "In Training. Epoch # 70- Loss = 7142663680.0\n",
      "In Training. Epoch # 70- Loss = 5859273728.0\n",
      "In Training. Epoch # 70- Loss = 12060900352.0\n",
      "In Training. Epoch # 70- Loss = 12908600320.0\n",
      "In Training. Epoch # 70- Loss = 4145174528.0\n",
      "In Training. Epoch # 70- Loss = 6238501888.0\n",
      "In Training. Epoch # 70- Loss = 3054155008.0\n",
      "In Training. Epoch # 70- Loss = 4277056256.0\n",
      "In Training. Epoch # 71- Loss = 6778925568.0\n",
      "In Training. Epoch # 71- Loss = 5582653440.0\n",
      "In Training. Epoch # 71- Loss = 11676022784.0\n",
      "In Training. Epoch # 71- Loss = 12650133504.0\n",
      "In Training. Epoch # 71- Loss = 4023472128.0\n",
      "In Training. Epoch # 71- Loss = 5986085376.0\n",
      "In Training. Epoch # 71- Loss = 2865408512.0\n",
      "In Training. Epoch # 71- Loss = 4138660608.0\n",
      "In Training. Epoch # 72- Loss = 6462144512.0\n",
      "In Training. Epoch # 72- Loss = 5345885696.0\n",
      "In Training. Epoch # 72- Loss = 11338993664.0\n",
      "In Training. Epoch # 72- Loss = 12429388800.0\n",
      "In Training. Epoch # 72- Loss = 3928570112.0\n",
      "In Training. Epoch # 72- Loss = 5770221056.0\n",
      "In Training. Epoch # 72- Loss = 2707956224.0\n",
      "In Training. Epoch # 72- Loss = 4027336192.0\n",
      "In Training. Epoch # 73- Loss = 6185889280.0\n",
      "In Training. Epoch # 73- Loss = 5143141888.0\n",
      "In Training. Epoch # 73- Loss = 11043419136.0\n",
      "In Training. Epoch # 73- Loss = 12240808960.0\n",
      "In Training. Epoch # 73- Loss = 3855812096.0\n",
      "In Training. Epoch # 73- Loss = 5585549824.0\n",
      "In Training. Epoch # 73- Loss = 2576896768.0\n",
      "In Training. Epoch # 73- Loss = 3938558464.0\n",
      "In Training. Epoch # 74- Loss = 5944645120.0\n",
      "In Training. Epoch # 74- Loss = 4969458688.0\n",
      "In Training. Epoch # 74- Loss = 10783810560.0\n",
      "In Training. Epoch # 74- Loss = 12079666176.0\n",
      "In Training. Epoch # 74- Loss = 3801288960.0\n",
      "In Training. Epoch # 74- Loss = 5427509248.0\n",
      "In Training. Epoch # 74- Loss = 2468078848.0\n",
      "In Training. Epoch # 74- Loss = 3868514560.0\n",
      "In Training. Epoch # 75- Loss = 5733677568.0\n",
      "In Training. Epoch # 75- Loss = 4820600320.0\n",
      "In Training. Epoch # 75- Loss = 10555440128.0\n",
      "In Training. Epoch # 75- Loss = 11941934080.0\n",
      "In Training. Epoch # 75- Loss = 3761717504.0\n",
      "In Training. Epoch # 75- Loss = 5292204544.0\n",
      "In Training. Epoch # 75- Loss = 2377984512.0\n",
      "In Training. Epoch # 75- Loss = 3813993216.0\n",
      "In Training. Epoch # 76- Loss = 5548918784.0\n",
      "In Training. Epoch # 76- Loss = 4692956160.0\n",
      "In Training. Epoch # 76- Loss = 10354232320.0\n",
      "In Training. Epoch # 76- Loss = 11824169984.0\n",
      "In Training. Epoch # 76- Loss = 3734347264.0\n",
      "In Training. Epoch # 76- Loss = 5176314368.0\n",
      "In Training. Epoch # 76- Loss = 2303634432.0\n",
      "In Training. Epoch # 76- Loss = 3772292864.0\n",
      "In Training. Epoch # 77- Loss = 5386870784.0\n",
      "In Training. Epoch # 77- Loss = 4583441408.0\n",
      "In Training. Epoch # 77- Loss = 10176671744.0\n",
      "In Training. Epoch # 77- Loss = 11723448320.0\n",
      "In Training. Epoch # 77- Loss = 3716877568.0\n",
      "In Training. Epoch # 77- Loss = 5077008384.0\n",
      "In Training. Epoch # 77- Loss = 2242506752.0\n",
      "In Training. Epoch # 77- Loss = 3741144320.0\n",
      "In Training. Epoch # 78- Loss = 5244526080.0\n",
      "In Training. Epoch # 78- Loss = 4489428992.0\n",
      "In Training. Epoch # 78- Loss = 10019731456.0\n",
      "In Training. Epoch # 78- Loss = 11637271552.0\n",
      "In Training. Epoch # 78- Loss = 3707386624.0\n",
      "In Training. Epoch # 78- Loss = 4991871488.0\n",
      "In Training. Epoch # 78- Loss = 2192470016.0\n",
      "In Training. Epoch # 78- Loss = 3718643456.0\n",
      "In Training. Epoch # 79- Loss = 5119296000.0\n",
      "In Training. Epoch # 79- Loss = 4408675328.0\n",
      "In Training. Epoch # 79- Loss = 9880791040.0\n",
      "In Training. Epoch # 79- Loss = 11563510784.0\n",
      "In Training. Epoch # 79- Loss = 3704273920.0\n",
      "In Training. Epoch # 79- Loss = 4918844928.0\n",
      "In Training. Epoch # 79- Loss = 2151720960.0\n",
      "In Training. Epoch # 79- Loss = 3703195392.0\n",
      "In Training. Epoch # 80- Loss = 5008948224.0\n",
      "In Training. Epoch # 80- Loss = 4339265024.0\n",
      "In Training. Epoch # 80- Loss = 9757588480.0\n",
      "In Training. Epoch # 80- Loss = 11500353536.0\n",
      "In Training. Epoch # 80- Loss = 3706207488.0\n",
      "In Training. Epoch # 80- Loss = 4856169984.0\n",
      "In Training. Epoch # 80- Loss = 2118737024.0\n",
      "In Training. Epoch # 80- Loss = 3693465856.0\n",
      "In Training. Epoch # 81- Loss = 4911561728.0\n",
      "In Training. Epoch # 81- Loss = 4279564032.0\n",
      "In Training. Epoch # 81- Loss = 9648162816.0\n",
      "In Training. Epoch # 81- Loss = 11446250496.0\n",
      "In Training. Epoch # 81- Loss = 3712084224.0\n",
      "In Training. Epoch # 81- Loss = 4802347520.0\n",
      "In Training. Epoch # 81- Loss = 2092232064.0\n",
      "In Training. Epoch # 81- Loss = 3688340480.0\n",
      "In Training. Epoch # 82- Loss = 4825474560.0\n",
      "In Training. Epoch # 82- Loss = 4228176128.0\n",
      "In Training. Epoch # 82- Loss = 9550820352.0\n",
      "In Training. Epoch # 82- Loss = 11399881728.0\n",
      "In Training. Epoch # 82- Loss = 3720991232.0\n",
      "In Training. Epoch # 82- Loss = 4756098560.0\n",
      "In Training. Epoch # 82- Loss = 2071121792.0\n",
      "In Training. Epoch # 82- Loss = 3686891776.0\n",
      "In Training. Epoch # 83- Loss = 4749251584.0\n",
      "In Training. Epoch # 83- Loss = 4183908864.0\n",
      "In Training. Epoch # 83- Loss = 9464085504.0\n",
      "In Training. Epoch # 83- Loss = 11360124928.0\n",
      "In Training. Epoch # 83- Loss = 3732176640.0\n",
      "In Training. Epoch # 83- Loss = 4716330496.0\n",
      "In Training. Epoch # 83- Loss = 2054492416.0\n",
      "In Training. Epoch # 83- Loss = 3688347904.0\n",
      "In Training. Epoch # 84- Loss = 4681656320.0\n",
      "In Training. Epoch # 84- Loss = 4145744640.0\n",
      "In Training. Epoch # 84- Loss = 9386679296.0\n",
      "In Training. Epoch # 84- Loss = 11326013440.0\n",
      "In Training. Epoch # 84- Loss = 3745024512.0\n",
      "In Training. Epoch # 84- Loss = 4682110464.0\n",
      "In Training. Epoch # 84- Loss = 2041574272.0\n",
      "In Training. Epoch # 84- Loss = 3692070144.0\n",
      "In Training. Epoch # 85- Loss = 4621613568.0\n",
      "In Training. Epoch # 85- Loss = 4112813568.0\n",
      "In Training. Epoch # 85- Loss = 9317493760.0\n",
      "In Training. Epoch # 85- Loss = 11296733184.0\n",
      "In Training. Epoch # 85- Loss = 3759030528.0\n",
      "In Training. Epoch # 85- Loss = 4652642304.0\n",
      "In Training. Epoch # 85- Loss = 2031720320.0\n",
      "In Training. Epoch # 85- Loss = 3697529600.0\n",
      "In Training. Epoch # 86- Loss = 4568195584.0\n",
      "In Training. Epoch # 86- Loss = 4084372992.0\n",
      "In Training. Epoch # 86- Loss = 9255561216.0\n",
      "In Training. Epoch # 86- Loss = 11271584768.0\n",
      "In Training. Epoch # 86- Loss = 3773785600.0\n",
      "In Training. Epoch # 86- Loss = 4627245568.0\n",
      "In Training. Epoch # 86- Loss = 2024386816.0\n",
      "In Training. Epoch # 86- Loss = 3704291328.0\n",
      "In Training. Epoch # 87- Loss = 4520593920.0\n",
      "In Training. Epoch # 87- Loss = 4059785216.0\n",
      "In Training. Epoch # 87- Loss = 9200036864.0\n",
      "In Training. Epoch # 87- Loss = 11249970176.0\n",
      "In Training. Epoch # 87- Loss = 3788959488.0\n",
      "In Training. Epoch # 87- Loss = 4605340160.0\n",
      "In Training. Epoch # 87- Loss = 2019117952.0\n",
      "In Training. Epoch # 87- Loss = 3711998976.0\n",
      "In Training. Epoch # 88- Loss = 4478108672.0\n",
      "In Training. Epoch # 88- Loss = 4038507008.0\n",
      "In Training. Epoch # 88- Loss = 9150183424.0\n",
      "In Training. Epoch # 88- Loss = 11231377408.0\n",
      "In Training. Epoch # 88- Loss = 3804289792.0\n",
      "In Training. Epoch # 88- Loss = 4586426880.0\n",
      "In Training. Epoch # 88- Loss = 2015531264.0\n",
      "In Training. Epoch # 88- Loss = 3720361216.0\n",
      "In Training. Epoch # 89- Loss = 4440129536.0\n",
      "In Training. Epoch # 89- Loss = 4020072960.0\n",
      "In Training. Epoch # 89- Loss = 9105355776.0\n",
      "In Training. Epoch # 89- Loss = 11215376384.0\n",
      "In Training. Epoch # 89- Loss = 3819566592.0\n",
      "In Training. Epoch # 89- Loss = 4570082816.0\n",
      "In Training. Epoch # 89- Loss = 2013307648.0\n",
      "In Training. Epoch # 89- Loss = 3729141504.0\n",
      "In Training. Epoch # 90- Loss = 4406127616.0\n",
      "In Training. Epoch # 90- Loss = 4004085760.0\n",
      "In Training. Epoch # 90- Loss = 9064995840.0\n",
      "In Training. Epoch # 90- Loss = 11201593344.0\n",
      "In Training. Epoch # 90- Loss = 3834625280.0\n",
      "In Training. Epoch # 90- Loss = 4555945472.0\n",
      "In Training. Epoch # 90- Loss = 2012180480.0\n",
      "In Training. Epoch # 90- Loss = 3738149888.0\n",
      "In Training. Epoch # 91- Loss = 4375641600.0\n",
      "In Training. Epoch # 91- Loss = 3990204160.0\n",
      "In Training. Epoch # 91- Loss = 9028606976.0\n",
      "In Training. Epoch # 91- Loss = 11189711872.0\n",
      "In Training. Epoch # 91- Loss = 3849339904.0\n",
      "In Training. Epoch # 91- Loss = 4543703040.0\n",
      "In Training. Epoch # 91- Loss = 2011927680.0\n",
      "In Training. Epoch # 91- Loss = 3747234304.0\n",
      "In Training. Epoch # 92- Loss = 4348265984.0\n",
      "In Training. Epoch # 92- Loss = 3978136064.0\n",
      "In Training. Epoch # 92- Loss = 8995753984.0\n",
      "In Training. Epoch # 92- Loss = 11179458560.0\n",
      "In Training. Epoch # 92- Loss = 3863616256.0\n",
      "In Training. Epoch # 92- Loss = 4533090816.0\n",
      "In Training. Epoch # 92- Loss = 2012364672.0\n",
      "In Training. Epoch # 92- Loss = 3756273408.0\n",
      "In Training. Epoch # 93- Loss = 4323651072.0\n",
      "In Training. Epoch # 93- Loss = 3967631872.0\n",
      "In Training. Epoch # 93- Loss = 8966060032.0\n",
      "In Training. Epoch # 93- Loss = 11170606080.0\n",
      "In Training. Epoch # 93- Loss = 3877382400.0\n",
      "In Training. Epoch # 93- Loss = 4523880960.0\n",
      "In Training. Epoch # 93- Loss = 2013338752.0\n",
      "In Training. Epoch # 93- Loss = 3765173504.0\n",
      "In Training. Epoch # 94- Loss = 4301485056.0\n",
      "In Training. Epoch # 94- Loss = 3958476544.0\n",
      "In Training. Epoch # 94- Loss = 8939186176.0\n",
      "In Training. Epoch # 94- Loss = 11162952704.0\n",
      "In Training. Epoch # 94- Loss = 3890591744.0\n",
      "In Training. Epoch # 94- Loss = 4515877888.0\n",
      "In Training. Epoch # 94- Loss = 2014723456.0\n",
      "In Training. Epoch # 94- Loss = 3773862400.0\n",
      "In Training. Epoch # 95- Loss = 4281498880.0\n",
      "In Training. Epoch # 95- Loss = 3950485760.0\n",
      "In Training. Epoch # 95- Loss = 8914836480.0\n",
      "In Training. Epoch # 95- Loss = 11156329472.0\n",
      "In Training. Epoch # 95- Loss = 3903212032.0\n",
      "In Training. Epoch # 95- Loss = 4508915712.0\n",
      "In Training. Epoch # 95- Loss = 2016415104.0\n",
      "In Training. Epoch # 95- Loss = 3782286080.0\n",
      "In Training. Epoch # 96- Loss = 4263452672.0\n",
      "In Training. Epoch # 96- Loss = 3943501568.0\n",
      "In Training. Epoch # 96- Loss = 8892748800.0\n",
      "In Training. Epoch # 96- Loss = 11150593024.0\n",
      "In Training. Epoch # 96- Loss = 3915223296.0\n",
      "In Training. Epoch # 96- Loss = 4502850560.0\n",
      "In Training. Epoch # 96- Loss = 2018328320.0\n",
      "In Training. Epoch # 96- Loss = 3790403584.0\n",
      "In Training. Epoch # 97- Loss = 4247140864.0\n",
      "In Training. Epoch # 97- Loss = 3937389312.0\n",
      "In Training. Epoch # 97- Loss = 8872694784.0\n",
      "In Training. Epoch # 97- Loss = 11145617408.0\n",
      "In Training. Epoch # 97- Loss = 3926619648.0\n",
      "In Training. Epoch # 97- Loss = 4497560064.0\n",
      "In Training. Epoch # 97- Loss = 2020393856.0\n",
      "In Training. Epoch # 97- Loss = 3798188800.0\n",
      "In Training. Epoch # 98- Loss = 4232375296.0\n",
      "In Training. Epoch # 98- Loss = 3932030976.0\n",
      "In Training. Epoch # 98- Loss = 8854464512.0\n",
      "In Training. Epoch # 98- Loss = 11141299200.0\n",
      "In Training. Epoch # 98- Loss = 3937401344.0\n",
      "In Training. Epoch # 98- Loss = 4492939264.0\n",
      "In Training. Epoch # 98- Loss = 2022555264.0\n",
      "In Training. Epoch # 98- Loss = 3805622272.0\n",
      "In Training. Epoch # 99- Loss = 4218997760.0\n",
      "In Training. Epoch # 99- Loss = 3927327744.0\n",
      "In Training. Epoch # 99- Loss = 8837879808.0\n",
      "In Training. Epoch # 99- Loss = 11137543168.0\n",
      "In Training. Epoch # 99- Loss = 3947575296.0\n",
      "In Training. Epoch # 99- Loss = 4488897536.0\n",
      "In Training. Epoch # 99- Loss = 2024766848.0\n",
      "In Training. Epoch # 99- Loss = 3812694272.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        loss = loss_func(model(xb), yb)\n",
    "        print(f'In Training. Epoch # {epoch}- Loss = {loss}')\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.0918e+09, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(x_test_tensor)\n",
    "loss = loss_func(pred, y_test_tensor)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
